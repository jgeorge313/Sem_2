{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a329b3a4",
   "metadata": {},
   "source": [
    "## HW 7 Joby George\n",
    "## DS GA 1003 Due 5/6/22\n",
    "\n",
    "# Problems 1 - 3:\n",
    "\n",
    "Make updates to the nodes.py and ridge_regression.py functions, running the script via terminal, paste the screenshot of the resulting test to prove you have fixed the scripts.\n",
    "\n",
    "Done below.\n",
    "\n",
    "![prob_1_3.png](prob_1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd40e7",
   "metadata": {},
   "source": [
    "## Output of ridge_regression.py\n",
    "\n",
    "The graph generated by running *ridge_regression.py* is produced below:\n",
    "\n",
    "![output_p3](output_p3.png)\n",
    "\n",
    "The average square errors reported for the two lambdas were: .2 after 1950 epochs using  $\\lambda$ = 0, and .056 after 450 epochs using $\\lambda$ = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c75622",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1  L2NormPenaltyNode\n",
    "\n",
    "class L2NormPenaltyNode(object):\n",
    "    \"\"\" Node computing l2_reg * ||w||^2 for scalars l2_reg and vector w\"\"\"\n",
    "    def __init__(self, l2_reg, w, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        l2_reg: a numpy scalar array (e.g. np.array(.01)) (not a node)\n",
    "        w: a node for which w.out is a numpy vector\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.out = self.l2_reg = np.array(l2_reg)\n",
    "        self.w = w\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = self.l2_reg * np.dot(self.w.out, self.w.out)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return(self.out)\n",
    "\n",
    "    def backward(self):\n",
    "        self.w.d_out = 2*self.l2_reg*self.d_out*self.w.out\n",
    "        pass\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.w]\n",
    "\n",
    "## Problem 2 SumNode \n",
    "class SumNode(object):\n",
    " \"\"\" Node computing a + b, for numpy arrays a and b\"\"\"\n",
    "    def __init__(self, a, b, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "        b: node for which b.out is a numpy array of the same shape as a\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = self.a.out + self.b.out\n",
    "        self.d_out =  np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "    def backward(self):\n",
    "        self.a.d_out += self.d_out\n",
    "        self.b.d_out += self.d_out\n",
    "        return self.d_out\n",
    "    def get_predecessors(self):\n",
    "        return([self.a, self.b])\n",
    "\n",
    "## Problem 3 Graph\n",
    "class RidgeRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" Ridge regression with computation graph \"\"\"\n",
    "    def __init__(self, l2_reg=1, step_size=.005,  max_num_epochs = 5000):\n",
    "        self.max_num_epochs = max_num_epochs\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Build computation graph\n",
    "        self.x = nodes.ValueNode(node_name=\"x\") # to hold a vector input\n",
    "        self.y = nodes.ValueNode(node_name=\"y\") # to hold a scalar response\n",
    "        self.w = nodes.ValueNode(node_name=\"w\") # to hold the parameter vector\n",
    "        self.b = nodes.ValueNode(node_name=\"b\") # to hold the bias parameter (scalar)\n",
    "        self.prediction = nodes.VectorScalarAffineNode(x=self.x, w=self.w, b=self.b,\n",
    "                                                 node_name=\"prediction\")\n",
    "        # Build computation graph\n",
    "        residual = nodes.SquaredL2DistanceNode(a=self.prediction, b=self.y,  node_name=\"square loss\") \n",
    "        regularization =  nodes.L2NormPenaltyNode(l2_reg, self.w,  node_name='L2NormPenalty')\n",
    "        \n",
    "        self.objective = nodes.SumNode(residual, regularization, node_name = 'loss')\n",
    "\n",
    "        self.graph = graph.ComputationGraphFunction(inputs = [self.x],\n",
    "                                                    outcomes= [self.y],\n",
    "                                                    parameters=[self.w,self.b],\n",
    "                                                    prediction=self.prediction,\n",
    "                                                    objective = self.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28bec1",
   "metadata": {},
   "source": [
    "# Problem 4:\n",
    "\n",
    "Show that $\\frac{\\partial J}{\\partial W_{ij}}=\\frac{\\partial J}{\\partial y_{i}}x_{j}$,\n",
    "where $x=\\left(x_{1},\\ldots,x_{d}\\right)^{T}$. {[}Hint: Although\n",
    "not necessary, you might find it helpful to use the notation $\\delta_{ij}=\\begin{cases}\n",
    "1 & i=j\\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}$. So, for examples, $\\partial_{x_{j}}\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)=2x_{i}\\delta_{ij}=2x_{j}$\n",
    "\n",
    "\n",
    "## Problem 4 answer:\n",
    "\n",
    "We know:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W_{ij}}=\\sum_{i=1}^{m}\\frac{\\partial J}{\\partial y_{r}}\\frac{\\partial y_{r}}{\\partial W_{ij}}$$\n",
    "\n",
    "\n",
    "$$ y_i = W_{ij}@x + b_i $$\n",
    "\n",
    "Therefore:\n",
    "$$ \\frac{{\\partial y_i}}{\\partial W_{ij}} = x_j $$ \n",
    "\n",
    "Where $x_j$ is the jth element of x. \n",
    "Re-expressing this we get: \n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W_{ij}}=(\\frac{\\partial J}{\\partial y_{i}}x_j)_{ij} $$\n",
    "\n",
    "Where $x_j$ is the jth entry of x and $(\\frac{\\partial J}{\\partial y_{i}}x_j)_{ij}$ is the i jth entry in the m x d matrix. \n",
    "\n",
    "### Q.E.D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bab613",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "\n",
    "\n",
    "Give a vectorized expression for $\\frac{\\partial J}{\\partial W}$ in terms of the column vectors $\\frac{\\partial J}{\\partial y}$ and x. \n",
    "\n",
    "\n",
    "## Problem 5 Answer\n",
    "\n",
    "\n",
    "Looking at an example of the matrix from problem four, we see that the first entries by row would be:\n",
    "\n",
    "$$ [\\frac{\\partial J}{\\partial W_{11}}]_{11} = \\frac{\\partial J}{\\partial y_1}*x_1 $$\n",
    "and continuing we see: \n",
    "$$ [\\frac{\\partial J}{\\partial W_{12}}]_{12} = \\frac{\\partial J}{\\partial y_1}*x_2 $$\n",
    "\n",
    "This means the first row of our matrix is the first entry of  $\\frac{\\partial J}{\\partial y_1}$ $\\otimes$ x.\n",
    "\n",
    "To get the ith row of  $\\frac{\\partial J}{\\partial W_i}$ all we have to do is take $y_i$ $\\otimes$ x.\n",
    "\n",
    "Rather than keeping this in a non-vectorized format, we can simplify  $\\frac{\\partial J}{\\partial W}$ to be:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial y} \\otimes x_j  $$\n",
    "\n",
    "### Q.E.D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9c22a",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "\n",
    "\n",
    "In the usual way, define $\\frac{\\partial J}{\\partial x}\\in\\ R^{d}$,\n",
    "whose $i$'th entry is $\\frac{\\partial J}{\\partial x_{i}}$. Show\n",
    "that \n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x}=W^{T}\\left(\\frac{\\partial J}{\\partial y}\\right) $$\n",
    "\n",
    "Note, if $x$ is just data, technically we won't need this derivative.\n",
    "However, in a multilayer perceptron, $x$ may actually be the output of a previous hidden layer, in which case we will need to propagate the derivative through $x$ as well.\n",
    "\n",
    "## Problem 6 answer:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x_i} = \\frac{\\partial J}{\\partial y} *  \\frac{\\partial y}{\\partial x_i}$$\n",
    "\n",
    "The rationale behind taking the entire $\\frac{\\partial J}{\\partial y}$   when we change any given $x_i$ in our data, we observe that it would impact all m elements in W at column i, due to the nature of matrix vector multiplication.\n",
    "\n",
    "We observe that $\\frac{\\partial y}{\\partial x_i}$ is the ith column of W, meaning:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = W_i^T \\frac{\\partial J}{\\partial y}$$\n",
    "\n",
    "For the whole vector $\\frac{\\partial y}{\\partial x}$ we can solve by taking:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = W^T \\frac{\\partial J}{\\partial y}$$\n",
    "\n",
    "\n",
    "### Q.E.D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fe63c",
   "metadata": {},
   "source": [
    "# Problem 7 \n",
    "\n",
    "Show that $\\frac{\\partial J}{\\partial b}=\\frac{\\partial J}{\\partial y}$,\n",
    "where $\\frac{\\partial J}{\\partial b}$ is defined in the usual way.\n",
    "\n",
    "## Problem 7 answer\n",
    "\n",
    "By chain rule: \n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y} *\\frac{\\partial y}{\\partial b} $$\n",
    "\n",
    "since Y = Wx + b $\\frac{\\partial y}{\\partial b}$ = 1 therefore:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y} $$ \n",
    "\n",
    "### Q.E.D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6f32a",
   "metadata": {},
   "source": [
    "# Problem 8 \n",
    "\n",
    "Show that $\\frac{\\partial J}{\\partial A}=\\frac{\\partial J}{\\partial S}\\odot\\sigma'(A)$, where we're using $\\odot$ to represent the \\textbf{Hadamard product}.\n",
    "\n",
    "If $A$ and $B$ are arrays of the same shape, then their Hadamardproduct $A\\odot B$ is an array with the same shape as $A$ and $B$, and for which $\\left(A\\odot B\\right)_{i}=A_{i}B_{i}$. That is, it's just the array formed by multiplying corresponding elements of $A$ and $B$. Conveniently, in numpy if A and B are arrays of the same shape, then A*B is their Hadamard product.\n",
    "\n",
    "\n",
    "## Problem 8 Answer\n",
    "\n",
    "We know that the derivative of $\\sigma$(a) is $\\sigma'$(a). Using the chain rule we can express:\n",
    "$$\\frac{\\partial J}{\\partial A} = \\frac{\\partial J}{\\partial S}*\\frac{\\partial S}{\\partial A}$$\n",
    "\n",
    "The derivative of S with respect to A is exactly $\\sigma'$(a)\n",
    "\n",
    "What does $\\sigma'$(a) represent? Since $\\sigma$ is an element wise transformation, each element in matrix A is multiplied by a scalar at the same index in S. $\\sigma'$ also completes this same element operation, meaning what we have is \n",
    "\n",
    "Therefore what we have is:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial A}=\\frac{\\partial J}{\\partial S}\\odot\\sigma'(A)$$\n",
    "\n",
    "where $\\odot$ is the Hadamard product\n",
    "\n",
    "### Q.E.D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a9206",
   "metadata": {},
   "source": [
    "# Problem 9 - 11 \n",
    "\n",
    "\n",
    "Complete the class AffineNode in nodes.py. Be sure to propagate the gradient with respect to x as well, since when we stack these layers, x will itself be the output of another node that depends on our optimization parameters. If your code is correct, you should be able to pass test AffineNode in mlp regression.t.py. Please attach a screenshot that shows the test results for this question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Complete the class TanhNode in nodes.py. As you’ll recall, d tanh(x) = 1−tanh2 x. Note dx\n",
    "that in the forward pass, we’ll already have computed tanh of the input and stored it in self.out. So make sure to use self.out and not recalculate it in the backward pass. If your code is correct, you should be able to pass test TanhNode in mlp regression.t.py. Please attach a screenshot that shows the test results for this question.\n",
    "\n",
    "\n",
    "Implement an MLP by completing the skeleton code in mlp regression.py and making use of the nodes above. Your code should pass the tests provided in mlp regression.t.py. Note that to break the symmetry of the problem, we initialize our weights to small random values, rather than all zeros, as we often do for convex optimization problems. Run the MLP for the two settings given in the main() function and report the average training error. Note that with an MLP, we can take the original scalar as input, in the hopes that it will learn nonlinear features on its own, using the hidden layers. In practice, it is quite challenging to get such a neural network to fit as well as one where we provide features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c103e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 9\n",
    "class AffineNode(object):\n",
    "    \"\"\"Node implementing affine transformation (W,x,b)-->Wx+b, where W is a matrix,\n",
    "    and x and b are vectors\n",
    "        Parameters:\n",
    "        W: node for which W.out is a numpy array of shape (m,d)\n",
    "        x: node for which x.out is a numpy array of shape (d)\n",
    "        b: node for which b.out is a numpy array of shape (m) (i.e. vector of length m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, W, x, b, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.W = W\n",
    "        self.x = x\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = self.W.out @ self.x.out + self.b.out\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.x.d_out = np.dot(self.W.out.T, self.d_out)\n",
    "        self.b.d_out = self.d_out\n",
    "        self.W.d_out = np.outer(self.d_out, self.x.out)\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return([self.W, self.x, self.b])\n",
    "\n",
    "# Problem 10\n",
    "\n",
    "\n",
    "class TanhNode(object):\n",
    "    \"\"\"Node tanh(a), where tanh is applied elementwise to the array a\n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = np.tanh(self.a.out)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.a.d_out = self.d_out*(1-self.out**2)\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return([self.a])\n",
    "\n",
    "    pass\n",
    "\n",
    "# Problem 11\n",
    "\n",
    "\n",
    "class MLPRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" MLP regression with computation graph \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden_units=10, step_size=.005, init_param_scale=0.01, max_num_epochs=5000):\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.init_param_scale = init_param_scale\n",
    "        self.max_num_epochs = max_num_epochs\n",
    "        self.step_size = step_size\n",
    "        # Build computation graph\n",
    "        \"\"\"\"\n",
    "        first set of logic, use Affine node to get L in the hw, then apply tan_h to it\n",
    "        \"\"\"\n",
    "        self.x = nodes.ValueNode(node_name=\"x\")  # to hold a vector input\n",
    "        # to hold the parameter vector\n",
    "        self.W1 = nodes.ValueNode(node_name=\"W1\")\n",
    "        # to hold the bias parameter (scalar)\n",
    "        self.b1 = nodes.ValueNode(node_name=\"b1\")\n",
    "        self.affine = nodes.AffineNode(\n",
    "            x=self.x, W=self.W1, b=self.b1, node_name='affine')\n",
    "        self.tan_h = nodes.TanhNode(a=self.affine, node_name='tan_h')\n",
    "\n",
    "        \"\"\"\n",
    "        second set of logic, use the VectorScalarAffineNode to get the ultimate\n",
    "        output and then calculate the L2 loss\n",
    "        \"\"\"\n",
    "        self.w2 = nodes.ValueNode(\n",
    "            node_name=\"w2\")  # to hold the parameter vector\n",
    "        # to hold the bias parameter (scalar)\n",
    "        self.b2 = nodes.ValueNode(node_name=\"b2\")\n",
    "        self.prediction = nodes.VectorScalarAffineNode(x=self.tan_h, w=self.w2, b=self.b2,\n",
    "                                                       node_name=\"prediction\")\n",
    "        self.y = nodes.ValueNode(node_name=\"y\")  # to hold a scalar response\n",
    "        self.objective = nodes.SquaredL2DistanceNode(\n",
    "            a=self.prediction, b=self.y,  node_name=\"square loss\")\n",
    "\n",
    "        self.graph = graph.ComputationGraphFunction(inputs=[self.x],\n",
    "                                                    outcomes=[self.y],\n",
    "                                                    parameters=[\n",
    "                                                        self.W1, self.b1, self.w2, self.b2],\n",
    "                                                    prediction=self.prediction,\n",
    "                                                    objective=self.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e78a07",
   "metadata": {},
   "source": [
    "## Screenshot mlp_regression.t.py for 9 and 10\n",
    "\n",
    "![Problem9-10](Problem9-10.png)\n",
    "\n",
    "## Screenshot of ml_regression.py\n",
    "\n",
    "![Problem11](Problem11.png)\n",
    "Average training error was: .2385 for the first parameter and 0.0428 for the second. (Trained on 4950 and 450 epochs, respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a5bb4",
   "metadata": {},
   "source": [
    "# Problem 12 -14 \n",
    "\n",
    "Implement a Softmax node. We provided skeleton code for class SoftmaxNode in nodes.py. If your code is correct, you should be able to pass test SoftmaxNode in multiclass.t.py. Please attach a screenshot that shows the test results for this question.\n",
    "\n",
    "Implement a negative log-likelihood loss node for multiclass classification. We provided skeleton code for class NLLNode in nodes.py. The test code for this question is combined with the test code for the next question.\n",
    "\n",
    " ImplementaMLPformulticlassclassificationbycompletingtheskeletoncodeinmulticlass.py. Your code should pass the tests in test multiclass provided in multiclass.t.py. Please attach\n",
    "a screenshot that shows the test results for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 12\n",
    "class SoftmaxNode(object):\n",
    "    \"\"\" Softmax node\n",
    "        Parameters:\n",
    "        z: node for which z.out is a numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, z, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.z = z\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = np.exp(self.z.out) / np.sum(np.exp(self.z.out))\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        #d f(z) / d(z)  = p(1-p)\n",
    "        temp = []\n",
    "        for prob in self.out:\n",
    "            temp.append(prob * (1 - prob))\n",
    "        self.diag = temp\n",
    "        self.temp2 = -1 * np.outer(self.out, self.out)\n",
    "        np.fill_diagonal(self.temp2, np.array(self.diag))\n",
    "\n",
    "        dz = self.d_out.T @ self.temp2\n",
    "        self.z.d_out += dz\n",
    "\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        #print(self.z.d_out)\n",
    "        return ([self.z])\n",
    "\n",
    "\n",
    "#Problem 13\n",
    "class NLLNode(object):\n",
    "    \"\"\" Node computing NLL loss between 2 arrays.\n",
    "        Parameters:\n",
    "        y_hat: a node that contains a vector, for a single\n",
    "        x's probability prediction\n",
    "        y_true: a node that's out is a single value, corresponding\n",
    "        to the true class value of x_i. Used as an index\n",
    "\n",
    "        Interestingly enough, maybe b/c we're doing SGD\n",
    "        the shape of Y_HAT is R^k and Y_true R \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, y_hat, y_true, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.y_hat = y_hat\n",
    "        self.y_true = y_true\n",
    "\n",
    "    def forward(self):\n",
    "        self.a = self.y_hat.out[self.y_true.out]\n",
    "        self.out = np.mean(-np.log(self.a))\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        #inspiration\n",
    "        # https://stats.stackexchange.com/questions/309427/softmax-with-log-likelihood-cost\n",
    "\n",
    "        #initialize an array that is of same shape as y_hat.out\n",
    "        #we'll use this for y_hat.d_out\n",
    "\n",
    "        temp_mat = np.zeros_like(self.y_hat.out)\n",
    "\n",
    "        #now the derivative of log(y_hat_true.out[y])\n",
    "        #this means we take the recipricol of y_hat_true[y] and multiply\n",
    "        #it by -1\n",
    "\n",
    "        dz = -1 * (self.a**-1)\n",
    "        #both of the dz are equal to each other, included both for\n",
    "        #transparency\n",
    "        dz = -1 * (self.y_hat.out[self.y_true.out]**-1)\n",
    "\n",
    "        #now the only entry in our array that has a value, is when\n",
    "        #class = y, so we update our array at index y_i by setting\n",
    "        #it to dz\n",
    "        temp_mat[self.y_true.out] = dz\n",
    "        self.y_hat.d_out = self.d_out * temp_mat\n",
    "\n",
    "        #don't need y_true.d_out b/c it's an index\n",
    "        return (self.d_out)\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return ([self.y_hat, self.y_true])\n",
    "\n",
    "\n",
    "#Problem 14\n",
    "class MulticlassClassifier(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" Multiclass prediction \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_hidden_units=10,\n",
    "                 step_size=.005,\n",
    "                 init_param_scale=0.01,\n",
    "                 max_num_epochs=1000,\n",
    "                 num_class=3):\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.init_param_scale = init_param_scale\n",
    "        self.max_num_epochs = max_num_epochs\n",
    "        self.step_size = step_size\n",
    "        self.num_class = num_class\n",
    "\n",
    "        # Build computation graph\n",
    "        self.x = nodes.ValueNode(node_name=\"x\")  # inputs\n",
    "        self.y = nodes.ValueNode(node_name=\"y\")  # vector of class labels\n",
    "        self.W1 = nodes.ValueNode(\n",
    "            node_name=\"W1\")  # to hold the parameter vector\n",
    "        self.b1 = nodes.ValueNode(\n",
    "            node_name=\"b1\")  # to hold the bias parameter (scalar)\n",
    "        self.W2 = nodes.ValueNode(\n",
    "            node_name=\"W2\")  # to hold the parameter vector\n",
    "        self.b2 = nodes.ValueNode(\n",
    "            node_name=\"b2\")  # to hold the bias parameter (scalar)\n",
    "        self.affine = nodes.AffineNode(x=self.x,\n",
    "                                       W=self.W1,\n",
    "                                       b=self.b1,\n",
    "                                       node_name='affine')\n",
    "        self.tan_h = nodes.TanhNode(a=self.affine, node_name='tan_h')\n",
    "        \"\"\"\n",
    "        second set of logic, use the VectorScalarAffineNode to get the ultimate\n",
    "        output and then calculate the L2 loss\n",
    "        \"\"\"\n",
    "        self.Z = nodes.AffineNode(x=self.tan_h,\n",
    "                                  W=self.W2,\n",
    "                                  b=self.b2,\n",
    "                                  node_name='Z')\n",
    "        self.prediction = nodes.SoftmaxNode(z=self.Z, node_name=\"prediction\")\n",
    "        self.objective = nodes.NLLNode(y_hat=self.prediction,\n",
    "                                       y_true=self.y,\n",
    "                                       node_name='NLL')\n",
    "        self.graph = graph.ComputationGraphFunction(\n",
    "            inputs=[self.x],\n",
    "            outcomes=[self.y],\n",
    "            parameters=[self.W1, self.b1, self.W2, self.b2],\n",
    "            prediction=self.prediction,\n",
    "            objective=self.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02740c6",
   "metadata": {},
   "source": [
    "## Screenshots for Problem 12 and 13\n",
    "\n",
    "![p12_13](p12_13.png)\n",
    "\n",
    "## Screenshot for Problem 14\n",
    "![p14](p14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330598a",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
