{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f048ad77",
   "metadata": {},
   "source": [
    "## HW 5 SGD for Multiclass Linear SVM\n",
    "## Joby George (jg6615)\n",
    "## Due 4/8/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c9e17",
   "metadata": {},
   "source": [
    "## Bayesian Modeling\n",
    "\n",
    "This section analyzes logistic regression in the Bayesian setting, where we introduce a prior $p(w)$ on  $w\\in\\mathbb{R}^{d}$. Consider a binary classification setting with input\n",
    "space $\\mathcal{X}=\\mathbb{R}^{d}$, outcome space $\\mathcal{Y}_{\\pm}=\\left\\{ -1,1\\right\\} $,\n",
    "and a dataset $\\mathcal{D}=\\left((x_{(1)},y_{(1)}),\\ldots,(x_{(n)},y_{(n)})\\right)$.\n",
    "\n",
    "# Problem 1 Prompt\n",
    "\n",
    "Give an expression for the posterior density p(w|D) in terms of negative log-likelihood function $NLL_D(w)$ and the prior density p(w) (up to a proportionality constant is fine).\n",
    "\n",
    "## Problem 1 answer\n",
    "\n",
    "\n",
    "By Bayes rule we can express the posterior density as:\n",
    "\n",
    "$$p(w|D) = \\frac{P(w \\cap D)}{P(D)} $$\n",
    "\n",
    "By chain rule we can re-express the numerator, giving us:\n",
    "\n",
    "\n",
    "$$p(w|D) = \\frac{P(D|w)*P(w)}{P(D)} $$\n",
    "\n",
    "We observe:\n",
    "\n",
    "1. Taking the Negative Log Likelihood and multiplying it by negative 1 gives us (positive) log likelihood\n",
    "2. Taking the exponential of -NLL is the equivalent of likelihood, thus we can re-express P(D|w), the likelihood, as the $e^{-NLL_D(w)}$ \n",
    "3. We can re-express the proportionality constant, the denominator $\\frac{1}{P(D)}$ as some K since it does not rely on w, giving us:\n",
    "\n",
    "$$p(w|D) = K*e^{-NLL_D(w)}P(w) $$\n",
    "\n",
    "\n",
    "### Q.E.D \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529889d5",
   "metadata": {},
   "source": [
    "# Problem 2 Prompt \n",
    "\n",
    "Suppose we take a prior on $w$ of the form $w\\sim\\mathcal{N}(0,\\Sigma)$, that is in the Gaussian family. Is this a conjugate prior to the likelihood given by logistic regression?\n",
    "\n",
    "(While not stated in the problem, since w is a vector, $\\Sigma$ is a **covariance matrix** from a multivariate Gaussian distribution)\n",
    "\n",
    "## Problem 2 Answer\n",
    "\n",
    "We can start with Equation Three from Part 1 to understand whether the Gaussian Prior on our weight vector is a conjugate prior, replacing $\\frac{1}{P(D)}$ with a constant K we see:\n",
    "\n",
    "$$p(w|D) = K*e^{-NLL_D(w)}*P(w))$$\n",
    "\n",
    "From the lecture, we know that the Gaussian is a conjugate prior if our likelihood is also a Gaussian, however with logistic regression, our likelihood is a Bernoulli distribution.\n",
    "\n",
    "Since our posterior distribution is therefore a Gaussian distribution multiplied by a Bernoulli  our posterior would not follow a Gaussian distribution, **meaning that our Gaussian prior is not a conjugate prior**. \n",
    "\n",
    "The non Gaussian Conjugate prior for logistic regression is eloquently shown in the following figure from Figure 3.1 (page 39, Chapter 3) of the text [Gaussian Processes for Machine Learning](http://gaussianprocess.org/gpml/chapters/RW3.pdf), depicted below:\n",
    "\n",
    "### Posterior Distribution of a Gaussian Multiplied by a Bernoulli likelihood\n",
    "![Posterior Distribution of a Gaussian Multiplied by a Bernoulli likelihood](Posterior.png)\n",
    "\n",
    "### Q.E.D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49012099",
   "metadata": {},
   "source": [
    "# Problem 3 Prompt\n",
    "\n",
    "Show that there exist a covariance matrix $\\Sigma$ such that MAP (maximum a posteriori) estimate for $w$\n",
    "after observing data $\\mathcal{D}$ is the same as the minimizer of the regularized\n",
    "logistic regression function defined in Regularized Logistic Regression paragraph above, and give its value. \n",
    "\n",
    "Hint: Consider minimizing the negative log posterior of $w$. Also, remember you can drop any terms from the objective function that don't depend on $w$. You may freely use results of previous problems.\n",
    "\n",
    "## Problem 3 Answer\n",
    "\n",
    "Using the Hint and equation 5 from Question 1, we can state:\n",
    "\n",
    "$$-log(P(w|D)) = -(log(K) + -NLL + log(P(w)) $$\n",
    "\n",
    "Following the hint to disregard variables that do not depend on w, we see and applying the negative sign, we see:\n",
    "\n",
    "$$-log(P(w|D)) = NLL - log(P(w)) $$\n",
    "\n",
    "We know multivariate Gaussian distribution with $\\mu$ =0 and $\\Sigma$ = $\\Sigma$ takes the form:\n",
    "\n",
    "$$P(w) = \\frac{e^{\\frac{-1}{2}w^T\\Sigma^{-1}w}}{\\sqrt{(2\\pi)^d\\det{\\Sigma}}}$$ \n",
    "\n",
    "Where the det stands for the determinant of $\\Sigma$ and d represents the number of features \n",
    "\n",
    "When taking the log of this, we observe:\n",
    "\n",
    "$$-logP(w) = - \\frac{d}{2}log{(2\\pi\\det{\\Sigma})} + w^T\\Sigma^{-1}w$$ \n",
    "\n",
    "Additionally, we have proven in previous homework that our NLL is equivalent to the ERM of logistic regression with Hinge Loss, giving us:\n",
    "\n",
    " $$-logP(w|D) =  \\sum_i^{n}log(1 +e^{-y_i\\theta^{T}x_i}) - \\frac{d}{2}log{(2\\pi\\det{\\Sigma})} + \\frac{1}{2}w^T\\Sigma^{-1}w $$\n",
    " \n",
    "Next, since our objective is to find the sigma, such that the MAP estimate for w is the same as the minimizer of regularized logistic regression, we would take the argmin of our negative log posterior. \n",
    "\n",
    "Note, we take the argmin because the negative log posterior is now a convex function rather than a concave, so the lowest  point in this distribution is the most frequently occuring value. \n",
    "\n",
    "This gives us:  \n",
    "\n",
    "$$ W_{MAP} = \\underset{w}{argmin}(-logP(w|D)) =   \\underset{w}{argmin}\\sum_i^{n}log(1 +e^{-y_i\\theta^{T}x_i}) - \\frac{d}{2}log{(2\\pi\\det{\\Sigma})} + \\frac{1}{2}w^T\\Sigma^{-1}w $$\n",
    "\n",
    "Furthermore we can eliminate the  $\\frac{d}{2}log{(2\\pi\\det{\\Sigma})}$ term as it does not rely on w giving us \n",
    "\n",
    "$$ W_{map} = \\underset{w}{argmin}\\sum_i^{n}log(1 +e^{-y_i\\theta^{T}x_i}) + \\frac{1}{2}w^T\\Sigma^{-1}w $$\n",
    "\n",
    "The Empirical Risk Minimizer for Regularized Logistic Regression is \n",
    "\n",
    "$$ERM{L(w, \\lambda)} = \\underset{w}{argmin}\\frac{1}{n}\\sum_i^{n}log(1 +e^{-y_i\\theta^{T}x_i}) + \\lambda||w||^2$$\n",
    "\n",
    "Therefore if:\n",
    "$$\\Sigma = \\frac{1}{2n\\lambda}I $$ we would have:\n",
    "\n",
    "$$w_{MAP} = \\underset{w}{argmin}\\sum_i^{n}log(1 +e^{-y_i\\theta^{T}x_i}) + n\\lambda||w||^2 $$\n",
    "\n",
    "we can scale the whole side by $\\frac{1}{n}$ and not alter our $W_{map}$ giving us:\n",
    "\n",
    "$$w_{MAP} = \\underset{w}{argmin}\\frac{1}{n}\\sum_i^{n}log(1 +e^{-y_i\\theta^{T}x_i}) + \\lambda||w||^2 $$\n",
    "\n",
    "which equals the ERM of Regularized Logistic Regression.\n",
    "\n",
    "### Q.E.D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e508b7f",
   "metadata": {},
   "source": [
    "# Problem 4 Prompt\n",
    "\n",
    "In the Bayesian approach, the prior should reflect your beliefs about the parameters before seeing the data and, in particular, should be independent on the eventual size of your dataset. Imagine choosing a prior distribution $w\\sim\\mathcal{N}(0,I)$. For a dataset $\\mathcal{D}$ of size $n$, how should you choose $\\lambda$ in our regularized\n",
    "logistic regression objective function so that the ERM is equal to the mode of the posterior distribution of $w$ (i.e. is equal to the MAP estimator). \n",
    "\n",
    "\n",
    "## Problem 4 Answer\n",
    "In this case our prior, has a covariance matrix that simply equals the identity matrix, we can use equation 9 from the previous problem, and plug the identity matrix in for $\\Sigma$, giving us:\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{2n\\lambda}I $$ \n",
    "\n",
    "$$ I = \\frac{1}{2n\\lambda}I $$\n",
    "$$ \\lambda = \\frac{1}{2n} $$\n",
    "\n",
    "### Q.E.D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1ae4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T15:53:27.589215Z",
     "start_time": "2022-03-30T15:53:27.578556Z"
    }
   },
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9bccba",
   "metadata": {},
   "source": [
    "## Coin Flipping with Partial Observability \n",
    "This is continuing your analysis done in HW4, you may use the results you obtained in HW4\n",
    "\n",
    "Consider flipping a biased coin where $p(z=H\\mid \\theta_1) = \\theta_1$. However, we cannot directly observe the result $z$. Instead, someone reports the result to us,which we denote by $x$.\n",
    "\n",
    "Further, there is a chance that the result is reported incorrectly, if it's a head. Specifically, we have $p(x=H\\mid z=H, \\theta_2) = \\theta_2$ and $p(x=T\\mid z=T) = 1$.\n",
    "\n",
    "# Problem 5 Prompt\n",
    "\n",
    "We additionally obtained a set of clean results $\\mathcal{D}_c$ of size $N_c$, where $x$ is directly observed without the reporter in the middle. \n",
    "\n",
    "Given that there are $c_h$ heads and $c_t$ tails, estimate $\\theta_1$ and $\\theta_2$ by MLE taking the two data sets into account. Note that the likelihood is $L(\\theta_1, \\theta_2) = p(\\mathcal{D}_r, \\mathcal{D}_c\\mid \\theta_1, \\theta_2)$.\n",
    "\n",
    "## Problem 5 Answer \n",
    "\n",
    "Let $n_h, n_t$ = the number of heads and tails, respectively from $D_r$ \n",
    "\n",
    "We know $D_c$ is conditionally independent from $\\theta_2$ and $D_r$ given $\\theta_1$ as it is a seperate coin draw of coin flips from the same coin as $D_r$ meaning:\n",
    "\n",
    "$$P(D_c |\\theta_1, \\theta_2, D_r) = P(D_c|\\theta_1) $$\n",
    "\n",
    "Using the definition of likelihood stated in hte problem, we have: \n",
    "\n",
    "$$L(\\theta_1, \\theta_2) = p(\\mathcal{D}_r, \\mathcal{D}_c\\mid \\theta_1, \\theta_2) $$\n",
    "\n",
    "Applying the definition of conditional probability, we get:\n",
    "\n",
    "$$L(\\theta_1, \\theta_2) = \\frac{P(\\mathcal{D}_r, \\mathcal{D}_c, \\theta_1, \\theta_2)}{P(\\theta_1,\\theta_2)} $$\n",
    "\n",
    "Using the chain rule, and selecting $D_c$ as the variable to condition on we get:\n",
    "\n",
    "$$L(\\theta_1, \\theta_2) =\\frac{P(D_c | D_r, \\theta_1, \\theta_2)P( D_r, \\theta_1, \\theta_2)}{P(\\theta_1,\\theta_2)} $$\n",
    "\n",
    "Using the chain rule, and selecting $D_r$ as the variable to condition this time on we get:\n",
    "\n",
    "$$L(\\theta_1, \\theta_2) =\\frac{P(D_c | D_r, \\theta_1, \\theta_2)P(D_r | \\theta_1, \\theta_2)*P(\\theta_1,\\theta_2)}{P(\\theta_1,\\theta_2)} $$\n",
    "\n",
    "We can simplify the numerator using the conditional independence assumption stated in Equation 1 and, plugging in the likelihood from the previous homework to get: \n",
    "\n",
    "$$L(\\theta_1, \\theta_2) = P(D_c | \\theta_1) * (\\theta_1\\theta_2)^{n_h} *(1-\\theta)^{n_t} $$\n",
    "\n",
    "Lastly, we know that $P(D_c | \\theta_1)$ is a Bernouli random variable, meaning it's probability distribution is \n",
    "$\\theta_1^{c_h}*(1-\\theta_1)^{c_t}$, giving us:\n",
    "\n",
    "$$L(\\theta_1, \\theta_2) = \\theta_1^{c_h}*(1-\\theta_1)^{c_t} * (\\theta_1\\theta_2)^{n_h} *(1-\\theta_1\\theta_2)^{n_t} $$\n",
    "\n",
    "We can take the log-likelihood and take partial derivatives, and set to 0 to solve for the MLE  getting us: \n",
    "\n",
    "$$\\Delta_{\\theta_1}Log(L(\\theta_1, \\theta_2)) =  \\frac{c_h}{\\theta_1} + \\frac{c_t}{1-c_t} + \\frac{n_h}{\\theta_1} - \\frac{n_t\\theta_2}{1-\\theta_2\\theta_1} $$\n",
    "\n",
    "$$\\Delta_{\\theta_2}Log(L(\\theta_1, \\theta_2)) =   \\frac{n_h}{\\theta_2} - \\frac{n_t\\theta_1}{1-\\theta_2\\theta_1} $$\n",
    "\n",
    "When setting the second partial to zero, we observe: \n",
    "\n",
    "$$\\frac{n_h}{\\theta_2} = \\frac{n_t\\theta_1}{1-\\theta_2\\theta_1} $$\n",
    "\n",
    "We can multiply both sides by $\\frac{\\theta_2}{\\theta_1}$ giving us:\n",
    "\n",
    "$$\\frac{n_h}{\\theta_1} = \\frac{n_t\\theta_2}{1-\\theta_2\\theta_1} $$\n",
    "\n",
    "We observe: \n",
    "\n",
    "$$\\frac{n_h}{\\theta_1} - \\frac{n_t\\theta_2}{1-\\theta_2\\theta_1} = 0 $$ which we can substitute into our first partial giving us:\n",
    "\n",
    "\n",
    "$$\\Delta_{\\theta_1}Log(L(\\theta_1, \\theta_2)) = 0 = \\frac{c_h}{\\theta_1} - \\frac{c_t}{1-c_t} $$\n",
    "\n",
    "**This is the typical Bernouli MLE giving us:**\n",
    "\n",
    "$$\\theta_{1MLE} = \\frac{c_h}{c_h+c_t} $$\n",
    "\n",
    "Plugging $\\theta_{1MLE}$ as $\\hat{h}$ into our second partial, we get:\n",
    "\n",
    "\n",
    "$$ 0 = \\frac{n_h}{\\theta_2} - \\frac{n_t\\hat{h}}{1-\\hat{h}\\theta_2} $$\n",
    "\n",
    "Solving for $\\theta_2$ we get:\n",
    "\n",
    "$$ \\theta_2 = \\frac{n_h}{n_h\\hat{h}+n_t\\hat{h}} $$\n",
    "\n",
    "factoring out the $\\hat{h}$ in the denominator we get: \n",
    "\n",
    "$$ \\theta_2 = \\frac{n_h}{(\\hat{h}(n_h+n_t))} $$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$ \\theta_2 = \\frac{n_h}{(n_h+n_t)} *\\frac{1}{\\hat{h}} $$\n",
    "\n",
    "Which means the **MLE estimate for $\\theta_2$ is the proportion of \"dirty\" observed heads divided by the proportion of clean heads** \n",
    "\n",
    "\n",
    "### Q.E.D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0b69a",
   "metadata": {},
   "source": [
    "# Problem 6 Prompt\n",
    "\n",
    "Since the clean results are expensive, we only have a small number of those and we are worried that we may over fit the data.\n",
    "\n",
    "To mitigate over fitting we can use a prior distribution on $\\theta_1$ if available. Let's imagine that an oracle gave use the prior $p(\\theta_1) = \\beta(h, t)$.\n",
    "\n",
    "Derive the MAP estimates for $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "## Problem 6 answer\n",
    "\n",
    "Definition of posterior, using some proportionality constant:\n",
    "$$ P(\\theta_1, \\theta_2|D_r,D_c) \\propto P(D_r,D_c|\\theta_1, \\theta_2) *P(\\theta_1, \\theta_2) $$\n",
    "\n",
    "\n",
    "We know: \n",
    "\n",
    "1. $\\theta_1$  is independent from $\\theta_2$ \n",
    "2. the joint likelihood of $D_r, D_c$ given $\\theta_1, \\theta_2$ from the problem above\n",
    "3. the Prior of $\\theta_1$ is a $\\beta(h,t)$\n",
    "4. Following advice from [campuswire](https://campuswire.com/c/G6A12AE75/feed/307), since we are not imposing a prior on $\\theta_2$ we can treat it as an unknown constant, a\n",
    "\n",
    "Giving us:\n",
    "\n",
    "$$ P(\\theta_1, \\theta_2|D_r,D_c) \\propto \\theta_1^{c_h}*(1-\\theta_1)^{c_t} * (\\theta_1\\theta_2)^{n_h} *(1-\\theta_1\\theta_2)^{n_t}  *[\\theta_1^{h-1}*(1-\\theta_1)^{t-1}] * a $$\n",
    "\n",
    "Taking the log  we get:\n",
    "\n",
    "$$ Log(P(\\theta_1, \\theta_2) \\propto c_hlog(\\theta_1) +c_tlog(1-\\theta_1) + n_hlog(\\theta_1\\theta_2) + n_tlog(1-\\theta_1\\theta2) + (ah-a)log(\\theta_1) + (at-a)log(1-\\theta_1)$$\n",
    "\n",
    "Taking paritial derivative w.r.t both $\\theta$s we get:\n",
    "\n",
    "$$\\Delta_{\\theta_1}Log(L(\\theta_1, \\theta_2)) =  \\frac{c_h}{\\theta_1} - \\frac{c_t}{1-\\theta_1} + \\frac{n_h}{\\theta_1} - \\frac{n_t\\theta_2}{1-\\theta_2\\theta_1} + \\frac{ah-a}{\\theta_1} -\\frac{at-a}{1-\\theta_1} $$\n",
    "\n",
    "$$\\Delta_{\\theta_2}Log(L(\\theta_1, \\theta_2)) =   \\frac{n_h}{\\theta_2} - \\frac{n_t\\theta_1}{1-\\theta_2\\theta_1} $$\n",
    "\n",
    "From the previous problem we know:\n",
    "\n",
    "$$\\frac{n_h}{\\theta_1} - \\frac{n_t\\theta_2}{1-\\theta_2\\theta_1} = 0 $$\n",
    "\n",
    "giving us:\n",
    "\n",
    "$$\\Delta_{\\theta_1}Log(L(\\theta_1, \\theta_2)) =  \\frac{c_h}{\\theta_1} - \\frac{c_t}{1-\\theta_1}  + \\frac{ah-a}{\\theta_1} -\\frac{at-a}{1-\\theta_1} $$\n",
    "\n",
    "simplifying we get:\n",
    "\n",
    "$$\\Delta_{\\theta_1}Log(L(\\theta_1, \\theta_2)) = \\frac{c_h+ah-a}{\\theta_1} - \\frac{c_t+at-a}{1-\\theta_1}$$\n",
    "\n",
    "Setting to 0 and to solve for $\\theta_{1MAP}$ we get:\n",
    "\n",
    "$$ 0 = (1-\\theta_1)(c_h+ah-a) - (\\theta_1c_t + \\theta_1at +\\theta_1a) $$\n",
    "\n",
    "We epxand the multiplication, factor theta, giving us:\n",
    "\n",
    "$$\\theta_{1MAP} = \\frac{c_h+ah-a}{c_h+ah+2a+c_t+at}$$\n",
    "\n",
    "Where a represents some unknown constant in lieu of a prior for $\\theta_2$ and h and t are the parameters of our $\\beta$ distribution\n",
    "\n",
    "We substitute $\\theta_{1MAP}$ for $theta_1$ in our second partial derivative getting:\n",
    "\n",
    "$$\\Delta_{\\theta_2}Log(L(\\theta_1, \\theta_2)) =   \\frac{n_h}{\\theta_2} - \\frac{n_t\\theta_{1MAP}}{1-\\theta_2\\theta_{1MAP}} $$\n",
    "\n",
    "Setting to 0 and solving we get:\n",
    "\n",
    "$$ 0 = (1-\\theta_2\\theta_{1MAP})(n_h) - n_t\\theta_2\\theta_{1MAP} $$\n",
    "\n",
    "$$ 0  = n_h - n_h\\theta_2\\theta_{1MAP} - nt\\theta_2\\theta_{1MAP} $$\n",
    "\n",
    "Isolating $\\theta_2$ and solving for $\\theta_{2MAP}$ we get:\n",
    "\n",
    "$$\\theta_{2MAP} = \\frac{n_h}{n_h\\theta_{1MAP} + n_t\\theta_{1MAP}} $$\n",
    "\n",
    "Similar to problem 5 we can simplify this to be \n",
    "\n",
    "$$\\theta_{2MAP} = \\frac{n_h}{n_h+n_t}*\\frac{1}{\\theta_{1MAP}} $$ \n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\theta_{1MAP} = \\frac{c_h+ah-a}{c_h+ah+2a+c_t+at}$$\n",
    "\n",
    "### Q.E.D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a602b",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d6e67",
   "metadata": {},
   "source": [
    "## Derivation for multi-class modeling\n",
    "\n",
    "Suppose our output space and our action space are given as follows:\n",
    "\n",
    "$\\mathcal{Y}=\\mathcal{A}=\\left\\{ 1,\\ldots,k\\right\\} $. \n",
    "\n",
    "\n",
    "Given a non-negative class-sensitive loss function \n",
    "$$\\Delta:\\mathcal{Y}\\times\\mathcal{A}\\to[0,\\infty)$$ \n",
    "\n",
    "and a class-sensitive feature mapping \n",
    "$$\\Psi:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}^{d}$$\n",
    "\n",
    "Our prediction is\n",
    "function $f:\\mathcal{X}\\to\\mathcal{Y}$ is given by\n",
    "\n",
    "\n",
    "$$f_{w}(x)=argmax_{y\\in\\mathcal{Y}}\\left\\langle w,\\Psi(x,y)\\right\\rangle$$\n",
    "\n",
    "For training data $(x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\in\\mathcal{X}\\times\\mathcal{Y}$, let $J(w)$ be the $\\ell_{2}$-regularized empirical risk function for the multiclass hinge loss. We can write this as\n",
    "\n",
    "\\[\n",
    "J(w)=\\lambda\\|w\\|^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\max_{y\\in\\mathcal{Y}}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]\n",
    "\\]\n",
    "for some $\\lambda>0$.\n",
    "\n",
    "# Problem 7 Prompt\n",
    "\n",
    "Show that $J(w)$ is a convex function of $w$. You may use any of the rules about convex functions described in our [notes on convex optimatzion](https://davidrosenberg.github.io/mlcourse/Notes/convex-optimization.pdf),\n",
    "in previous assignments, or in the Boyd and Vandenberghe book, though you should cite the general facts you are using. \n",
    "\n",
    "\n",
    "Hint: If $f_{1},\\ldots,f_{m}:\\mathbb{R}^{n}\\to\\mathbb{R}$\n",
    "are convex, then their point wise maximum $f(x)=\\max\\left\\{ f_{1}(x),\\ldots,f_{m}(x)\\right\\} $\n",
    "is also convex.\n",
    "\n",
    "## Problem 7 Answer\n",
    "\n",
    "Our definition of the regularized empirical risk function for multiclass hinge loss is:\n",
    "\n",
    "\\[\n",
    "J(w)=\\lambda\\|w\\|^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\max_{y\\in\\mathcal{Y}}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]\n",
    "\\]\n",
    "for some $\\lambda>0$.\n",
    "\n",
    "This is also equivalent to:\n",
    "\n",
    "$$J(w) =\\lambda\\|w\\|^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\max_{y\\neq y_i}[\\max(0,1-\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle] $$\n",
    "\n",
    "(Taken from [David Rosenberg's 2017 notes on Multiclass SVM](https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/11b.multiclass.pdf), slide 34/48 from lecture 24)\n",
    "\n",
    "Now we know:\n",
    "\n",
    "1. all norms are convex (page 5 from the notes on convexity),\n",
    "2. all norms multiplied by a positive scalar $\\lambda$ are also convex, \n",
    "3. 1-$m_w$ is linear if $m_w$ is linear, and linear and affine functions are convex\n",
    "4. as stated in page 4 of the notes, the maximum between (0, 1-m) is convex, as y=0 is a linear function, and their pointwise maximum is also convex.\n",
    "5. Lastly, the addition of two positive convex functions is convex, making the addition of the norm and the convex pointwise maximium a convex function. \n",
    "\n",
    "To prove 1-$m_w$ is linear\n",
    " \n",
    "We define $m_w$ as :\n",
    "\n",
    "$$ m_w = \\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle $$ since dot products are linear, 1-$m_w$ is linear\n",
    "\n",
    "\n",
    "Proving that the equation **must** be convex\n",
    "\n",
    "### Q.E.D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4997c",
   "metadata": {},
   "source": [
    "# Problem 8 Prompt\n",
    "\n",
    "Since $J(w)$ is convex, it has a subgradient at every point. Give an expression for a subgradient of $J(w)$. You may use any standard results about subgradients, including the result from an earlier homework\n",
    "about subgradients of the pointwise maxima of functions. \n",
    "\n",
    "Hint: It may be helpful to refer to $\\hat{y}_{i}=argmax_{y\\in\\mathcal{Y}}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$.\n",
    "\n",
    "## Problem 8 Answer\n",
    "Using the hint, we define $\\hat{y}=argmax_{y\\in\\mathcal{Y}}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$, giving us the loss function \n",
    "\n",
    "\n",
    "$$ J(w)=\\lambda\\|w\\|^{2}+\\frac{1\n",
    "}{n}\\sum_{i=1}^{n}\\left[\\Delta\\left(\\hat{y_i},y_i\\right)+\\left\\langle w,\\Psi(x_{i},\\hat{y_i})-\\Psi(x_{i},y_{i})\\right\\rangle \\right] $$\n",
    "\n",
    "Taking the subgradient of this expression, we get:\n",
    "\n",
    "$$\\nabla J(w)) = 2\\lambda w + \\frac{1}{n}\\sum_{i=1}^{n}(\\Psi(x_{i},\\hat{y_i})-\\Psi(x_{i},y_{i}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37bcb90",
   "metadata": {},
   "source": [
    "# Problem 9 Prompt\n",
    "\n",
    "Give an expression for the stochastic subgradient based on the point $(x_{i},y_{i})$.\n",
    "\n",
    "## Problem 9 answer\n",
    "\n",
    "The stochastic subgradient based on the point $(x_{i},y_{i})$ is:\n",
    "\n",
    "$$\\nabla J(w)) = 2\\lambda w + (\\Psi(x_{i},\\hat{y_i})-\\Psi(x_{i},y_{i}))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240f19a",
   "metadata": {},
   "source": [
    "# Problem 10 Prompt\n",
    "\n",
    "Give an expression for a minibatch subgradient, based on the points $(x_{i},y_{i}),\\ldots,\\left(x_{i+m-1},y_{i+m-1}\\right)$.\n",
    "\n",
    "## Problem 10 Answer\n",
    "\n",
    "The expression for a minibatch subgradient, \n",
    "\n",
    "$$\\nabla J(w)) = 2\\lambda w + \\frac{1}{m}\\sum_i^{i+m-1}(\\Psi(x_{i},\\hat{y_i})-\\Psi(x_{i},y_{i}))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31af8e",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa098261",
   "metadata": {},
   "source": [
    "## Optional Problem: Hinge Loss is a special Case of Generalized Hinge Loss\n",
    "\n",
    "Let $\\mathcal{Y}=\\left\\{ -1,1\\right\\} $. Let $\\Delta(y,y')=1({y\\neq\\hat{y}}).$\n",
    "\n",
    "If $g(x)$ is the score function in our binary classification setting, then define our compatibility function h, as \n",
    "\\begin{eqnarray*}\n",
    "h(x,1) & = & g(x)/2\\\\\n",
    "h(x,-1) & = & -g(x)/2.\n",
    "\\end{eqnarray*}\n",
    "Show that for this choice of $h$, the multiclass hinge loss reduces to hinge loss: \n",
    "\n",
    "\\[\n",
    "\\ell\\left(h,\\left(x,y\\right)\\right)=\\max_{y'\\in\\mathcal{Y}}\\left[\\Delta(y,y')+h(x,y')-h(x,y)\\right]=\\max\\left\\{ 0,1-yg(x)\\right\\} \n",
    "\\]\n",
    "\n",
    "## Answer\n",
    "\n",
    "If y = 1, and y' $\\neq$ y, we would observe $\\Delta(y,y')$ = 1, giving us:\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[1 + h(x,-1) -h(x,1)]$$\n",
    "\n",
    "Simplifying to:\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[1 + -g(x)/2 -g(x)/2]$$\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[1 -yg(x)]$$\n",
    "\n",
    "If y = -1 and and y' $\\neq$ y, we would still  observe $\\Delta(y,y')$ = 1, giving us:\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[1 + h(x,1) -h(x,-1)]$$\n",
    "\n",
    "Simplifying to:\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[1 + g(x)/2 - -g(x)/2]$$\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[1 + g(x)]$$\n",
    "\n",
    "this can equivalently be re-expressed as:\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[1  - yg(x)]$$\n",
    "\n",
    "since y = -1\n",
    "\n",
    "In our last case when y' = y, we would not have any loss, from $\\Delta(y,y')$ giving us:\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[0  +h(x,y) - h(x,y')]$$\n",
    "\n",
    "Since y = y' the h(x,y) = h(x,y') giving us\n",
    "\n",
    "\n",
    "$$l(h(x,y)) = \\underset{y'\\in \\mathcal{Y}}\\max[0,0]$$\n",
    "\n",
    "Therefore, we can see that with choice of h, the multiclass hinge loss reduces to:\n",
    "\n",
    "**max(0,1-yg(x))**\n",
    "\n",
    "\n",
    "### Q.E.D.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97c3d8",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04518a4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:30:27.759900Z",
     "start_time": "2022-04-07T18:30:26.349240Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from sklearn.datasets.samples_generator import make_blobs\n",
    "except:\n",
    "    from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cdd12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:30:27.845604Z",
     "start_time": "2022-04-07T18:30:27.761160Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the  training data\n",
    "np.random.seed(2)\n",
    "X, y = make_blobs(n_samples=300,cluster_std=.25, centers=np.array([(-3,1),(0,2),(3,1)]))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b5edd",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "In this problem we will work on a simple three-class classification example. The data is generated and plotted for you in the skeleton code.\n",
    "\n",
    "## One-vs-All (also known as One-vs-Rest)\n",
    "\n",
    "First we will implement one-vs-all multiclass classification. Our approach will assume we have a binary base classifier that returns a score, and we will predict the class that has the highest score.\n",
    "\n",
    "# Problem 11 Prompt\n",
    "\n",
    "Complete the methods fit, decision function and predict from OneVsAllClassifier in the skeleton code. Following the OneVsAllClassifier code is a cell that extracts the results of the fit and plots the decision region. You can have a look at it first to make sure you understand how the class will be used.\n",
    " \n",
    " ## Problem 11 Answer\n",
    " \n",
    " done below\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0703dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:32:54.208143Z",
     "start_time": "2022-04-07T18:32:54.191930Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "class OneVsAllClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "    One-vs-all classifier\n",
    "    We assume that the classes will be the integers 0,..,(n_classes-1).\n",
    "    We assume that the estimator provided to the class, after fitting, has a \"decision_function\" that \n",
    "    returns the score for the positive class.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_classes):      \n",
    "        \"\"\"\n",
    "        Constructed with the number of classes and an estimator (e.g. an\n",
    "        SVM estimator from sklearn)\n",
    "        @param estimator : binary base classifier used\n",
    "        @param n_classes : number of classes\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes \n",
    "        self.estimators = [clone(estimator) for _ in range(n_classes)]\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        This should fit one classifier for each class.\n",
    "        self.estimators[i] should be fit on class i vs rest\n",
    "        @param X: array-like, shape = [n_samples,n_features], input data\n",
    "        @param y: array-like, shape = [n_samples,] class labels\n",
    "        @return returns self\n",
    "        \"\"\"\n",
    "        #initialize a dictionary for each class\n",
    "        label_dict = {}\n",
    "        #for each class, create a np array with length n, that takes value 1 if y equals that class, 0 otherwise\n",
    "        for pred_class in range(self.n_classes):\n",
    "            label_dict[pred_class] = np.where(y==pred_class,1,0)\n",
    "        #evaluate the SVM's fit method on our training data, for each class using a binary class\n",
    "        for i in range(self.n_classes):\n",
    "            self.estimators[i].fit(X,label_dict[i])\n",
    "        self.fitted = True  \n",
    "        return self   \n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Returns the score of each input for each class. Assumes\n",
    "        that the given estimator also implements the decision_function method (which sklearn SVMs do), \n",
    "        and that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_features] input data\n",
    "        @return array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "\n",
    "        if not hasattr(self.estimators[0], \"decision_function\"):\n",
    "            raise AttributeError(\n",
    "                \"Base estimator doesn't have a decision_function attribute.\")\n",
    "        \n",
    "        #Replace the following return statement with your code\n",
    "        \n",
    "        #Initialize a n_samples by n_classes matrix full of zeros\n",
    "        score = np.zeros([X.shape[0],self.n_classes])\n",
    "        \n",
    "        #for each column, apply SVM decision function\n",
    "        for i in range(self.n_classes):\n",
    "            score[:,i] = self.estimators[i].decision_function(X)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples,n_features] input data\n",
    "        @returns array-like, shape = [n_samples,] the predicted classes for each input\n",
    "        \"\"\"\n",
    "        #Replace the following return statement with your code\n",
    "        \n",
    "        #calculate the score to get probabilities for each class by running one vs all decision function on x\n",
    "        score = self.decision_function(X)\n",
    "        #return the highest value in each \n",
    "        y = np.argmax(score, axis=1)\n",
    "        return(y)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88567d85",
   "metadata": {},
   "source": [
    "# Problem 12 Prompt\n",
    "\n",
    "Include the results of the test cell in your submission.\n",
    "\n",
    "## Problem 12 Answer\n",
    "\n",
    "done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea2fc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:33:12.210975Z",
     "start_time": "2022-04-07T18:33:12.061017Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here we test the OneVsAllClassifier\n",
    "from sklearn import svm\n",
    "svm_estimator = svm.LinearSVC(loss='hinge', fit_intercept=False, C=200)\n",
    "clf_onevsall = OneVsAllClassifier(svm_estimator, n_classes=3)\n",
    "clf_onevsall.fit(X,y)\n",
    "\n",
    "for i in range(3) :\n",
    "    print(\"Coeffs %d\"%i)\n",
    "    print(clf_onevsall.estimators[i].coef_) #Will fail if you haven't implemented fit yet\n",
    "\n",
    "# create a mesh to plot in\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = min(X[:,0])-3,max(X[:,0])+3\n",
    "y_min, y_max = min(X[:,1])-3,max(X[:,1])+3\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z = clf_onevsall.predict(mesh_input)\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "## Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y, clf_onevsall.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd5542",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd31ef",
   "metadata": {},
   "source": [
    "## Multiclass SVM\n",
    "\n",
    "In this question, we will implement stochastic subgradient descent for the linear multiclass SVM, as described in class and in this problem set. We will use the class-sensitive feature mapping approach with the “multivector construction”, as described in the multiclass lecture.\n",
    "\n",
    "# Problem 13 Prompt\n",
    "\n",
    "Complete the function featureMap in the skeleton code.\n",
    "\n",
    "## Problem 13 Answer\n",
    "\n",
    "Done Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047784d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:30:28.252188Z",
     "start_time": "2022-04-07T18:30:28.056277Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def zeroOne(y,a) :\n",
    "    '''\n",
    "    Computes the zero-one loss.\n",
    "    @param y: output class\n",
    "    @param a: predicted class\n",
    "    @return 1 if different, 0 if same\n",
    "    '''\n",
    "    return int(y != a)\n",
    "\n",
    "def featureMap(X,y,num_classes) :\n",
    "    '''\n",
    "    Computes the class-sensitive features.\n",
    "    @param X: array-like, shape = [n_samples,n_inFeatures] or [n_inFeatures,], input features for input data\n",
    "    @param y: a target class (in range 0,..,num_classes-1)\n",
    "    @return array-like, shape = [n_samples,n_outFeatures], the class sensitive features for class y\n",
    "    '''\n",
    "    \n",
    "    ##when we do subgradient descent we pass in a row vector, numpy\n",
    "    ##by default does not treat the row vector in the proper dimmension\n",
    "    ##essentially, without this if statement, the multiclass prediction will only predict on\n",
    "    #two classes because of vector operations \n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape((1,2))\n",
    "        \n",
    "        \n",
    "    #The following line handles X being a 1d-array or a 2d-array\n",
    "    num_samples, num_inFeatures = (1,X.shape[0]) if len(X.shape) == 1 else (X.shape[0],X.shape[1])\n",
    "    #goal, take our X in R^d vector and our y in R^(Num Classes) vector and createa. new feature mapping that is\n",
    "    #R^(d*num_classes), returning a matrix that is R^n_samples by (d*num_classes)\n",
    "    \n",
    "    ##get the appropriate column dimmension by multiplying num_classes by current num in features\n",
    "    num_outFeatures = num_classes*num_inFeatures\n",
    "    \n",
    "    #create a matrix that is full of 0's\n",
    "    X_out = np.zeros([num_samples,num_outFeatures])\n",
    "    # for each row, we look at the class of y, \n",
    "    #and assign our x values to the corresponding set of columns to that y\n",
    "    #i.e. if y had 3 labels, and x was in r^2, and a row had a label of 2, we would set the 3rd and 4th rows to\n",
    "    #be the values of our x for that row in our X_out\n",
    "    \n",
    "    \n",
    "    #create an exception to work on a singular point, so i can run subgradient descent on a singular point\n",
    "    if type(y) != np.ndarray:\n",
    "        y = np.array([y])\n",
    "    for i in range(num_samples):\n",
    "        modified_starting_col_idx, modified_ending_col_idx = y[i]*num_inFeatures, (y[i]+1)*num_inFeatures\n",
    "        X_out[i,modified_starting_col_idx:modified_ending_col_idx] = X[i]\n",
    "        \n",
    "    return X_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c96abd",
   "metadata": {},
   "source": [
    "# Problem 14 Prompt\n",
    "\n",
    "Complete the function sgd.\n",
    "\n",
    "## Problem 14 Answer\n",
    "\n",
    "Done Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fd585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:30:28.256074Z",
     "start_time": "2022-04-07T18:30:28.253124Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "def sgd(X, y, num_outFeatures, subgd, eta = .1, T = 10000):\n",
    "    '''\n",
    "    Runs subgradient descent, and outputs resulting parameter vector.\n",
    "    @param X: array-like, shape = [n_samples,n_features], input training data \n",
    "    @param y: array-like, shape = [n_samples,], class labels\n",
    "    @param num_outFeatures: number of class-sensitive features\n",
    "    @param subgd: function taking x,y,w and giving subgradient of objective\n",
    "    @param eta: learning rate for SGD\n",
    "    @param T: maximum number of iterations\n",
    "    @return: vector of weights\n",
    "    '''\n",
    "    num_samples = X.shape[0]\n",
    "    #initialize a w vector in r^(num_out features), then start sub gradient descent \n",
    "    w=np.zeros(num_outFeatures)\n",
    "    for iteration in range(T):\n",
    "        \n",
    "        #note this function assumes subgd feature maps our original X into X_out\n",
    "        #additionally, since our sub gradient function takes only a specific input, we will shuffle our data\n",
    "        #to grab random datapoints each time\n",
    "        \n",
    "        idx = np.arange(num_samples)\n",
    "        shuffle(idx)\n",
    "        \n",
    "        X, y =X[idx],y[idx]\n",
    "         \n",
    "        w = w - eta*subgd(X[0], y[0],w)\n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4c5e4",
   "metadata": {},
   "source": [
    "# Problem 15 Prompt\n",
    "\n",
    "Complete the methods subgradient, decision function and predict from the class MulticlassSVM.\n",
    " \n",
    "## Problem 15 Answer\n",
    "\n",
    "Done Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e712f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:30:28.263547Z",
     "start_time": "2022-04-07T18:30:28.257087Z"
    }
   },
   "outputs": [],
   "source": [
    "class MulticlassSVM(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Implements a Multiclass SVM estimator.\n",
    "    '''\n",
    "    def __init__(self, num_outFeatures, lam=1.0, num_classes=3, Delta=zeroOne, Psi=featureMap):       \n",
    "        '''\n",
    "        Creates a MulticlassSVM estimator.\n",
    "        @param num_outFeatures: number of class-sensitive features produced by Psi\n",
    "        @param lam: l2 regularization parameter\n",
    "        @param num_classes: number of classes (assumed numbered 0,..,num_classes-1)\n",
    "        @param Delta: class-sensitive loss function taking two arguments (i.e., target margin)\n",
    "        @param Psi: class-sensitive feature map taking two arguments\n",
    "        '''\n",
    "        self.num_outFeatures = num_outFeatures\n",
    "        self.lam = lam\n",
    "        self.num_classes = num_classes\n",
    "        self.Delta = Delta\n",
    "        self.Psi = lambda X,y : Psi(X,y,num_classes)\n",
    "        self.fitted = False\n",
    "    \n",
    "    def subgradient(self,x,y,w):\n",
    "        '''\n",
    "        Computes the subgradient at a given data point x,y\n",
    "        @param x: sample input\n",
    "        @param y: sample class\n",
    "        @param w: parameter vector\n",
    "        @return returns subgradient vector at given x,y,w\n",
    "        '''\n",
    "        #for each class, we calculate the margin to determine which class our prediction for a single point\n",
    "        #we calculate margin by taking the dot product of our weight vector, and the Psi of (x, given class)\n",
    "        #we take the maximum margin as our predicted class for that point, and then calculate the gradient \n",
    "        #using that predicted class\n",
    "\n",
    "        y_hat=0\n",
    "        buest_guess_margin = self.Delta(y, initial_y) +np.dot(w,self.Psi(x,initial_y).T) -np.dot(w,self.Psi(x,y).T)\n",
    "\n",
    "        for samp_class in range(self.num_classes):\n",
    "            class_margin = self.Delta(y, samp_class) + np.dot(w, self.Psi(x,samp_class).T) - np.dot(w, self.Psi(x,y).T)\n",
    "\n",
    "\n",
    "\n",
    "            if class_margin > buest_guess_margin:\n",
    "                buest_guess_margin = class_margin\n",
    "                y_hat = samp_class\n",
    "                \n",
    "        local_sgd=2*self.lam*w+self.Psi(x,y_hat)-self.Psi(x,y)\n",
    "        return(local_sgd)\n",
    "            \n",
    "    def fit(self,X,y,eta = 0.1,T=10000):\n",
    "        '''\n",
    "        Fits multiclass SVM\n",
    "        @param X: array-like, shape = [num_samples,num_inFeatures], input data\n",
    "        @param y: array-like, shape = [num_samples,], input classes\n",
    "        @param eta: learning rate for SGD\n",
    "        @param T: maximum number of iterations\n",
    "        @return returns self\n",
    "        '''\n",
    "        self.coef_ = sgd(X,y,self.num_outFeatures,self.subgradient,eta,T)\n",
    "        self.fitted = True\n",
    "        return(self)    \n",
    "\n",
    "     \n",
    "    def decision_function(self, X):\n",
    "        '''\n",
    "        Returns the score on each input for each class. Assumes\n",
    "        that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_inFeatures]\n",
    "        @return array-like, shape = [n_samples, n_classes] giving scores for each sample,class pairing\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "\n",
    "        #we take our X in matrix, transform it, and multiply by coefs on a cell by cell basis \n",
    "        \n",
    "        num_samples=X.shape[0]\n",
    "        score=np.zeros([num_samples,self.num_classes])\n",
    "        for i in range(num_samples):\n",
    "            for j in range(self.num_classes):\n",
    "                X_out=self.Psi(X[i],j)\n",
    "                score[i][j]=np.dot(self.coef_,X_out.T)\n",
    "        return(score)\n",
    "\n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples, n_inFeatures], input data to predict\n",
    "        @return array-like, shape = [n_samples,], class labels predicted for each data point\n",
    "        '''\n",
    "\n",
    "        score = self.decision_function(X)\n",
    "        #return the highest value in each \n",
    "        y = np.argmax(score, axis=1)\n",
    "        return(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29275a",
   "metadata": {},
   "source": [
    "# Problem 16 Prompt\n",
    "\n",
    "Following the multiclass SVM implementation, we have included another block of test code. Make sure to include the results from these tests in your assignment, along with your code.\n",
    "\n",
    "## Problem 16 Answer\n",
    "\n",
    "Done below in two cells, the first cell shows a more fine tuned version of the model with $\\eta$ = .001, while the other uses the default $\\eta$ parameter given to us of .1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051efcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:30:32.692063Z",
     "start_time": "2022-04-07T18:30:28.264409Z"
    }
   },
   "outputs": [],
   "source": [
    "#the following code tests the MulticlassSVM and sgd\n",
    "#will fail if MulticlassSVM is not implemented yet\n",
    "est = MulticlassSVM(6,lam=1)\n",
    "#Note eta = .001\n",
    "est.fit(X,y,eta =.001)\n",
    "print(\"w:\")\n",
    "print(est.coef_)\n",
    "Z = est.predict(mesh_input)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y, est.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536c8f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:32:11.365403Z",
     "start_time": "2022-04-07T18:32:06.994461Z"
    }
   },
   "outputs": [],
   "source": [
    "#the following code tests the MulticlassSVM and sgd\n",
    "#will fail if MulticlassSVM is not implemented yet\n",
    "est = MulticlassSVM(6,lam=1)\n",
    "#default eta, performs worse than an eta of .001 \n",
    "est.fit(X,y)\n",
    "print(\"w:\")\n",
    "print(est.coef_)\n",
    "Z = est.predict(mesh_input)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y, est.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b11358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
