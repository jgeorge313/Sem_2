{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5cdbe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T01:01:43.936843Z",
     "start_time": "2022-05-05T01:01:41.886658Z"
    }
   },
   "outputs": [],
   "source": [
    "from Model_Eval_helper import DataLoader, mean_encode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69048298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T01:01:50.627417Z",
     "start_time": "2022-05-05T01:01:50.623111Z"
    }
   },
   "outputs": [],
   "source": [
    "DataLoader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f66a74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T01:01:51.123633Z",
     "start_time": "2022-05-05T01:01:51.120372Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_encoding_list, cat_feats = DataLoader.generate_special_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62787ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:42:34.242550Z",
     "start_time": "2022-05-05T00:36:38.521139Z"
    }
   },
   "outputs": [],
   "source": [
    "%time data, test = DataLoader.import_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536522ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:27:24.395727Z",
     "start_time": "2022-05-05T00:27:24.269307Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1516b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:51:12.156732Z",
     "start_time": "2022-05-05T00:51:12.130124Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_classifier(data,test, meanenc_feats, cat_feats):\n",
    "    excluded_feats = ['SK_ID_CURR','TARGET'] + ['prev_sum_CODE_REJECT_REASON_CLIENT','bureau_sum_CREDIT_ACTIVE_Active']\n",
    "    y = data['TARGET']\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=90210)\n",
    "    oof_preds = np.zeros(data.shape[0])\n",
    "    sub_preds = np.zeros(test.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    scores = [] #fold scores\n",
    "    clf_list = [] #get a list of the models we generate\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(data,data['TARGET'])):\n",
    "        trn, val = data.iloc[trn_idx], data.iloc[val_idx]\n",
    "        \n",
    "        val_test = pd.concat([val,test],axis=0,sort=False)\n",
    "        val_size = val.shape[0]\n",
    "        test_size = test.shape[0]\n",
    "\n",
    "        print ('doing mean_encoding')\n",
    "        trn, val_test = helper.mean_encode(trn, val_test, meanenc_feats, 'TARGET', drop=True)\n",
    "        features = [f_ for f_ in trn.columns if f_ not in excluded_feats]\n",
    "\n",
    "        val  = val_test.iloc[0:val_size, :].copy(deep=True)\n",
    "        test_x = val_test[features].iloc[-test_size:,:].copy(deep=True)\n",
    "\n",
    "        trn_x, trn_y = trn[features], trn['TARGET']\n",
    "        val_x, val_y = val[features], val['TARGET']\n",
    "\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=26,\n",
    "            metric='auc',\n",
    "            colsample_bytree=0.3,\n",
    "            subsample=0.9320,\n",
    "            max_depth=4,\n",
    "            reg_alpha=4.8299,\n",
    "            reg_lambda=3.6335,\n",
    "            min_split_gain=0.0068,\n",
    "            min_child_weight=9.8138,\n",
    "            silent=True,\n",
    "            verbose=-1,\n",
    "            n_jobs = 16,\n",
    "            random_state = n_fold * 619,\n",
    "            class_weight = {0:1,1:1.0122}\n",
    "        )\n",
    "        \n",
    "        clf = BaggingClassifier(model, 3)\n",
    "\n",
    "        clf.fit(trn_x, trn_y, \n",
    "                eval_set= [(val_x, val_y)], \n",
    "                eval_metric='auc', verbose=200, early_stopping_rounds=100,\n",
    "                categorical_feature = cat_feats,\n",
    "               )\n",
    "\n",
    "        oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_x)[:, 1] / folds.n_splits\n",
    "    \n",
    "        fold_score = roc_auc_score(val_y, oof_preds[val_idx])\n",
    "        scores.append(fold_score)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, fold_score))\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features\n",
    "        fold_importance_df[\"importance_gain\"] = clf.feature_importances_gain_\n",
    "        fold_importance_df[\"importance_split\"] = clf.feature_importances_split_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        #store the last fold's validation set so we can see what happened on some specific cases \n",
    "        if n_fold == 3:\n",
    "        #if n_fold == 5:\n",
    "            val_x_df, val_y_Df = deepcopy(val_x), deepcopy(valy.deep_copy)\n",
    "            clf_list.append(clf)\n",
    "\n",
    "        del clf, trn_x, trn_y, val_x, val_y\n",
    "        del trn, val\n",
    "        gc.collect()\n",
    "        return(oof_preds, feature_importance_df, sub_preds, clf_list, val_x_df, val_y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce20dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:48:50.233760Z",
     "start_time": "2022-05-05T00:48:50.151448Z"
    }
   },
   "outputs": [],
   "source": [
    "%time oof_preds, feature_importance_df, sub_preds, clf_list, val_x, val_y = train_classifier(data,test,mean_encoding_list, cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bb7dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:52:13.810369Z",
     "start_time": "2022-05-05T00:52:13.671757Z"
    }
   },
   "outputs": [],
   "source": [
    "helper.mean_encode(0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ba534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:30:47.801451Z",
     "start_time": "2022-05-05T00:30:47.761346Z"
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8e8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5e327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94263ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:43:38.917640Z",
     "start_time": "2022-05-05T00:43:38.898580Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_encode(train, val, features_to_encode, target, drop=False):\n",
    "    train_encode = train.copy(deep=True)\n",
    "    val_encode = val.copy(deep=True)\n",
    "    for feature in features_to_encode:\n",
    "        train_global_mean = train[target].mean()\n",
    "        train_encode_map = pd.DataFrame(index = train[feature].unique())\n",
    "        train_encode[feature+'_mean_encode'] = np.nan\n",
    "        kf = KFold(n_splits=5, shuffle=False)\n",
    "        for rest, this in kf.split(train):\n",
    "            train_rest_global_mean = train[target].iloc[rest].mean()\n",
    "            encode_map = train.iloc[rest].groupby(feature)[target].mean()\n",
    "            encoded_feature = train.iloc[this][feature].map(encode_map).values\n",
    "            train_encode[feature+'_mean_encode'].iloc[this] = train[feature].iloc[this].map(encode_map).values\n",
    "            train_encode_map = pd.concat((train_encode_map, encode_map), axis=1, sort=False)\n",
    "            train_encode_map.fillna(train_rest_global_mean, inplace=True) \n",
    "            train_encode[feature+'_mean_encode'].fillna(train_rest_global_mean, inplace=True)\n",
    "            \n",
    "        train_encode_map['avg'] = train_encode_map.mean(axis=1)\n",
    "        val_encode[feature+'_mean_encode'] = val[feature].map(train_encode_map['avg'])\n",
    "        val_encode[feature+'_mean_encode'].fillna(train_global_mean,inplace=True)\n",
    "        \n",
    "    if drop: #drop unencoded features\n",
    "        train_encode.drop(features_to_encode, axis=1, inplace=True)\n",
    "        val_encode.drop(features_to_encode, axis=1, inplace=True)\n",
    "    return train_encode, val_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa575a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T00:43:44.554883Z",
     "start_time": "2022-05-05T00:43:44.537709Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaggingClassifier(object):\n",
    "    \"\"\"\n",
    "    code copied and pasted from the lgbm1 notebook\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator, n_estimators):\n",
    "\n",
    "        self.base_estimator_ = base_estimator\n",
    "        self.n_estimators_ = n_estimators\n",
    "\n",
    "    def fit(self, X, y, eval_set = None, eval_metric = None, verbose = None, early_stopping_rounds = None, categorical_feature = None):\n",
    "        \n",
    "        self.estimators_ = []\n",
    "        self.feature_importances_gain_ = np.zeros(X.shape[1])\n",
    "        self.feature_importances_split_ = np.zeros(X.shape[1])\n",
    "        self.n_classes_ = y.nunique()\n",
    "\n",
    "        if self.n_estimators_ == 1:\n",
    "            print ('n_estimators=1, no downsampling')\n",
    "            estimator = deepcopy(self.base_estimator_)\n",
    "            estimator.fit(X, y, eval_set = [(X, y)] + eval_set,\n",
    "                eval_metric = eval_metric, verbose = verbose, \n",
    "                early_stopping_rounds = early_stopping_rounds)\n",
    "            self.estimators_.append(estimator)\n",
    "            self.feature_importances_gain_ += estimator.booster_feature_importance(importance_type='gain')\n",
    "            self.feature_importances_split_ += estimator.booster_feature_importance(importance_type='split')\n",
    "            return\n",
    "\n",
    "    #average down sampling results\n",
    "        minority = y.value_counts().sort_values().index.values[0]\n",
    "        majority = y.value_counts().sort_values().index.values[1]\n",
    "        print('majority class:', majority)\n",
    "        print('minority class:', minority)\n",
    "\n",
    "        X_min = X.loc[y==minority]\n",
    "        y_min = y.loc[y==minority]\n",
    "        X_maj = X.loc[y==majority]\n",
    "        y_maj = y.loc[y==majority]\n",
    "\n",
    "        kf = KFold(self.n_estimators_, shuffle=True, random_state=42)\n",
    "\n",
    "        for rest, this in kf.split(y_maj):\n",
    "\n",
    "            print('training on a subset')\n",
    "            X_maj_sub = X_maj.iloc[this]\n",
    "            y_maj_sub = y_maj.iloc[this]\n",
    "            X_sub = pd.concat([X_min, X_maj_sub])\n",
    "            y_sub = pd.concat([y_min, y_maj_sub])\n",
    "\n",
    "            estimator = deepcopy(self.base_estimator_)\n",
    "\n",
    "            estimator.fit(X_sub, y_sub, eval_set = [(X_sub, y_sub)] + eval_set,\n",
    "                eval_metric = eval_metric, verbose = verbose, \n",
    "                early_stopping_rounds = early_stopping_rounds,\n",
    "                categorical_feature = categorical_feature)\n",
    "\n",
    "            self.estimators_.append(estimator)\n",
    "            self.feature_importances_gain_ += estimator.booster_.feature_importance(importance_type='gain')/self.n_estimators_\n",
    "            self.feature_importances_split_ += estimator.booster_.feature_importance(importance_type='split')/self.n_estimators_\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        proba = np.zeros([n_samples, self.n_classes_])\n",
    "\n",
    "        for estimator in self.estimators_:\n",
    "\n",
    "            proba += estimator.predict_proba(X, num_iteration=estimator.best_iteration_)/self.n_estimators_\n",
    "\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93ef34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
