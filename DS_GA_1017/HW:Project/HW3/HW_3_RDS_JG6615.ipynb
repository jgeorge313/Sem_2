{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ec692c",
   "metadata": {},
   "source": [
    "## Homework 3 DS GA 1017 Joby George (jg6615) Due 5/5/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766ac24",
   "metadata": {},
   "source": [
    "# Problem 1 Online Job Ads \n",
    "\n",
    "Consider a hypothetical job search website that uses a machine learning system to determine which job openings to show to which users. The system uses historical employment data, and also collects interaction data from its own users: which users were shown which job openings, and which users clicked. The service also receives data from employers, detailing which users were invited to job interviews, and which were hired.\n",
    "\n",
    "## 1A \n",
    "\n",
    "Give three distinct reasons why gender disparities might arise in the operations of such a system\n",
    "\n",
    "1. Pre-existing Bias. Several industries, and positions are currently disproportionately dominated by certain genders. In the Table 4 of the appendix in Automated Experiments on Ad Privacy Settings, by Datta, Datta, and Tschantz the five URL+title pairs that the model identifies as the strongest indicators of being from the female or male group are shown. I have added the table below for context.\n",
    "\n",
    "\n",
    "![Table 4 Appendix](Table_4_Appendix.png)\n",
    "\n",
    "1. The fourth most gendered job ad for men, CDL-A OTR Trucking Jobs, is an example of an industry which is dominated by men. According to the Women in Trucking Nonprofit, approximately 7.9% of truck-drivers are women.$^1$ The highly unbalanced gender ratio in this field would inherently make its way into the model, and the model would not display trucking job advertisements to women as they do not currently reflect a top prospect for this particular job. \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Different ad-engagement patterns across gender. There are multiple reasons that genders could respond to online ads about employment differently. A [2008 Article from Harvard Business School titled How Female Stars succeed in new Jobs](https://hbswk.hbs.edu/item/how-female-stars-succeed-in-new-jobs) offers insights into one potential reason. Top-performing women, those that would be interested in executive and high paying roles are more cautious and vet potential career shifts more seriously than men. They build a network that is not dominated by people at their own firm, but a group of peers and mentors working at different places. These two reasons could lead to women not engaging with ads for high-paying jobs as their mechanism to get high paying jobs, but rather relying on their network and referrals to land into their next job. On the other hand, the article posits male top-performers tend to build a primarily internal network, and vet potnential future employers less strictly. Thus, when looking for new positions, men would be more likely to respond to an advertisement rather than contact a friend they know who could help. The fact that men would be more likely to engage in an advertisement directly influences the algorithm and would then preferentially treat men for high paying jobs. This is just one hypothesis for why women and men would engage with ads in a gendered manner, but if there is a gendered response in click through behvaior, it would ultimately feed into the algorithm itself in which ads are displayed. \n",
    "\n",
    "<br/> \n",
    "\n",
    "3. Emergent bias. Companies posting for jobs have a culture they are explicitly trying to hire for. Thus as the machine learning system receives feedback from employers on which candidates were invited for interviews and which were hired, it would algorithmically enforce the company and industry biases to potential prospects interested in applying for a job. An example of this would be the video game developer industry. In a [Wired article on game-maker Riot Games](https://www.wired.com/story/riot-games-ceo-culture-complaints/), the \"boys club\" and \"fraternity\" culture are used to describe top leadership. Women routinely faced a stiffer barrier to full-time employment, promotions, and a toxic work culture. All of these company and industry wide practices will manifest itself in the data, as women convert fewer job interviews that they apply for, making them less valuable prospects for an online job ad market which would pay money for a conversion rather than a click. \n",
    "\n",
    " Additionally, as women hear about these stories, they won't engage and will opt to not click on an advertisement even if it's shown to them, lowering the click through rate and further deprioritizing women in the algorithm for a job in this specific industry. This creates a chain effect, where fewer women see ads and click, lowering click through rate even more and as the algorithm gets fed more data, it will reinforce the company or industry's bias that women are not desirable talent for those positions.  \n",
    "\n",
    "\n",
    "[1: Footnote for Women in Trucking](http://www.womenintrucking.org/blog/what-have-we-done-to-increase-the-presence-of-women-in-trucking)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90008660",
   "metadata": {},
   "source": [
    "## Problem 1B\n",
    "\n",
    "Suppose that the job search service decides to increase the number of times it presents job openings in STEM to African Americans. To do so, the service observes that STEM job experience (in years) is positively associated with the likelihood that a user clicks on an advertised STEM job opening: the more years of experience, the more likely a user is to click. Consider the following intervention:\n",
    "\n",
    "Pre-process the training dataset, replacing the value of the “job experience” feature for African Americans with the best (highest) possible value for the feature in the dataset.\n",
    "\n",
    "\n",
    "### I and II \n",
    "\n",
    "Under what conditions will this intervention increase the number of times job openings in STEM are shown to African Americans?\n",
    "\n",
    "\n",
    "Under what conditions will this intervention fail to increase the number of times job openings in STEM are shown to African Americans?\n",
    "\n",
    "### Answer\n",
    "Some conditions that would increase the ads for STEM job openings would be shown to African Americans are :\n",
    "\n",
    "\n",
    "1. The ad must still be calibrated for the right job position. If by replacing years of experience to the highest possible value, and all the jobs that are shown to African Americans are now senior and managerial level roles, fresh graduates would not engage with the advertisements, lowering the group's click through rate compared to what the model would expect and thus penalize African Americans from seeing STEM jobs.\n",
    "\n",
    "2. The system must be optimized for a click-through rather than some conversion event (hiring potential prospect). If the algorithm is trying to optimize for conversion, and companies themselves do not want to hire an African American candidate then the algorithm will not prioritize showing these ads to African Americans. If the algorithm is optimized for click-through rate then any engagement with advertisements will be seen as favorable to both the employer and the algorithm and more ads will be shown to African Americans. \n",
    "\n",
    "3. For more advertisements for STEM job openings to be shown to African Americans, the advertisement demographics selected in the marketing campaign would have to be relevant. In determining the relevancy to show which ads to which online browsers, there's a component of relevancy and an auction bid itself. If a company believes an online browser has 20 years of experience and has engaged with a lot of STEM related web-browsing they may be willing to bid a high amount to display the ad. However, because of this pre-processing, we may have distorted the bidders understanding of who is behind the screen. Instead of an engineer with 20 years of experience, the web-browser is a middle school science student (who may use their parent's Google account such that their age would align with the experience) there would be little relevance to the actual person seeing the ad. Thus the model could learn that click-through rate for highly experienced African Americans interested in STEM is negatively correlated with Click Through Rate and display fewer ads.\n",
    " \n",
    " While the example of a middle school student being behind the screen is somewhat contrived, any person casually interested in science and technology, but pursuing a career in a different field would be misrepresented to the auction bidder and the algorithm would create a new understanding between years of experience for African Americans and CTR on these ads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e9948",
   "metadata": {},
   "source": [
    "# Problem 2: AI Ethics: Global Perspectives\n",
    "\n",
    "\n",
    "I watched X video and completed my memo, answering the following questions, below:\n",
    "\n",
    "1. Identify the stakeholders. In particular, which organizations, populations, or groups could be impacted by the data science issues discussed in the lecture? How could the data science application benefit the population(s) or group(s)? How could the population(s) or group(s) be adversely affected?\n",
    "2. Identify and describe an issue relating to data protection or data sharing raised in the lecture.Which vendor(s) owns the data and/or determines how the data is shared or used? To what extent is the privacy of users or persons represented in the data being protected? Is the data protection adequate?\n",
    "3. How does transparency and interpretability, or a lack thereof, affect users or other stakeholders? Are there black boxes?\n",
    "4. What incentives does the vendor (e.g., the data owner, company, or platform) have to ensure data protection, transparency, and fairness? How do these incentives shape the vendor's behavior?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ca3b9",
   "metadata": {},
   "source": [
    "## Memo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13c0f9",
   "metadata": {},
   "source": [
    "# Problem 3:\n",
    "\n",
    "\n",
    "## 3A:\n",
    "\n",
    "Use the provided Colab template notebook to import the 20 newsgroups dataset from sklearn.datasets, importing the same two-class subset as was used in the LIME paper: Atheism and Christianity. Use the provided code to fetch the data, split it into training and test sets, then fit a TF-IDF vectorizer to the data, and train a SGDClassifier classifier.\n",
    "\n",
    "\n",
    "## 3B\n",
    "Generate a confusion matrix (hint: use sklearn.metrics.confusion_matrix) to evaluate the accuracy of the classifier. The confusion matrix should contain a count of correct Christian, correct Atheist, incorrect Christian, and incorrect Atheist predictions. Use SHAP’s explainer to generate visual explanations for any 5 documents in the test set. The documents you select should include some correctly classified and some misclassified documents.\n",
    "\n",
    "I provide a screenshot to load the data, fit the model and generate a confusion matrix below.\n",
    "\n",
    "![3A.png](3A.png)\n",
    "\n",
    "For the five explanations on documents, I provide screenshots of SHAP explanations below, with the text that our classifier was trying to predict an Atheist or Christian label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a98e1d",
   "metadata": {},
   "source": [
    "### Review 1\n",
    "\n",
    "![Review1.png](Review1.png)\n",
    "\n",
    "### Review 2\n",
    "![Review2.png](Review2.png)\n",
    "\n",
    "### Review 3\n",
    "![Review3.png](Review3.png)\n",
    "\n",
    "### Review 4\n",
    "![Review4.png](Review4.png)\n",
    "\n",
    "### Review 5\n",
    "![Review5.png](Review5.png)\n",
    "\n",
    "\n",
    "### Classification Commentary\n",
    "\n",
    "With the five selected reviews, four reviews we predicted Atheist, whereas two of the incorrect predictions were truly Christian, but we predicted Atheist.  3 of those were correct, whereas on the only predicted Christian of this sample, we were incorrect as the true label was Atheist.\n",
    "\n",
    "It's interesting seeing what words are high weights. In our incorrect prediction for the Christian Review, the subject line was \"Why do people become atheists.\" It's simple to see how the phrasing of that subject line skewed the classification to predict Atheist, but interestingly enough the word \"Princeton\" was the strongest contributor to the incorrect Atheist prediction.\n",
    "\n",
    "Similarly, a number of digital prefixes, or organizational domains (cwru, kmr4) were strongly associated with the Atheist label. This does not intuitively make sense, but the weighting in our model means as shown by SHAP shows that they are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14353b",
   "metadata": {},
   "source": [
    "## 3C: \n",
    "\n",
    "Use SHAP’s explainer to study mis-classified documents, and the features (words) that contributed to their misclassification, by taking the following steps:\n",
    "\n",
    "1. Report the accuracy of the classifier, as well as the number of misclassified documents.\n",
    "\n",
    "\n",
    "2. For a document doc_i let us denote by conf_i the difference between the probabilities of the two predicted classes for that document. Generate a chart that shows conf_i for all misclassified documents (which, for misclassified documents, represents the magnitude of the error). Use any chart type you find appropriate to give a good sense of the distribution of errors. \n",
    "\n",
    "\n",
    "3. Identify all words that contributed to the misclassification of documents. (Naturally, some words will be implicated for multiple documents.) For each word (call it word_j), compute (a) the number of documents it helped misclassify (call is count_j) and (b) the total weight of that word in all documents it helped misclassify (weight_j) (sum of absolute values of weight_j for each misclassified document). The reason to use absolute values is that SHAP assigns a positive or a negative sign to weight_j depending on the class to which word_j is contributing. Plot the distribution of count_j and weight_j, and discuss your observations in the report.\n",
    "\n",
    "\n",
    "### 3C.1\n",
    "!['Accuracy_Misclassification.png'](Accuracy_Misclassification.png) \n",
    "\n",
    "As shown above, the accuracy for an untuned classifier was .9344, with 47 documents being misclassified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931294e",
   "metadata": {},
   "source": [
    "### 3C.2 Visualization 1: Total distribution of error magnitude \n",
    "\n",
    "!['Overall_Distribution.png'](Overall_Distribution.png)\n",
    "\n",
    "### 3C.2 Visualization 2: Distribution of error magnitude by Class \n",
    "!['Distribution_by_class.png'](Distribution_by_class.png)\n",
    "\n",
    "\n",
    "### Commentary\n",
    "We can see that most of our errors are closely bunched at .1-.2, and that label with the more errors are Predicted Atheist that were truly Christian (**Note: the legend in the second graph refers to the true class, and not the predicted class.**) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99066fb9",
   "metadata": {},
   "source": [
    "### 3C.3 Visualization of distribution of weights on words that contributed to misclassification\n",
    "\n",
    "\n",
    "\n",
    "### 3C.3 Visualization of distribution of counts on words that contributed to misclassification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
