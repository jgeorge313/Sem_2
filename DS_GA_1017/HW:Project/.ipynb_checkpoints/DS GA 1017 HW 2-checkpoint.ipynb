{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1947e4cd",
   "metadata": {},
   "source": [
    "## Homework 2 DS GA 1017\n",
    "## Joby George (jg6615)\n",
    "### Due 4/14/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d326e9a",
   "metadata": {},
   "source": [
    "# Problem 1: Racial Disparities in predictive Policing\n",
    "\n",
    "The 2016 study by Lum and Isaac found a disparity between the number of drug arrests in Oakland, California. The study, using survey data, showed that drug usage does not differ by racial groups, however, drug related arrests were strongly concentrated in a few predominantly non-white counties.\n",
    "\n",
    "Consider a hypothetical ML system, such as the one scrutinized by Lum and Isaac that uses historical arrest data which optimizes police allocation. The system could complement historical arrest data with other useful datasets (time of year, weather, number of clubs and bars, etc in the neighborhood, etc.)\n",
    "\n",
    "\n",
    "## 1A: Give 3 distinct reasons why racial disparities might arise in the predictions of such a system.\n",
    "\n",
    "1. The historical arrest data suffer from pre-existing bias. The data do not reflect true crime use per-se, but just where arrests were made. This means that the historical policing policies, which have disproportionately impacted minority populations would inform the model to target these groups. \n",
    "\n",
    "\n",
    "2. Intentional biasing of the model to target minorities. While this is the most nefarious reason for disparities in predictive drug policing, it should not be discounted. If the model's goal is to predict drug arrests using previous drug arrest data, that means the defendant in question has violated a law. John Ehrlichman, the domestic policy chief under Nixon who started the war on drugs by criminalizing marijuana is quoted saying:\n",
    "\n",
    "    \"The Nixon campaign in 1968, and the Nixon White House after that, had two enemies: the antiwar left and black people. You understand what I'm saying? We knew we couldn't make it illegal to be either against the war or black, but by getting the public to associate the hippies with marijuana and blacks with heroin, and then criminalizing both heavily, we could disrupt those communities\"$^{1}$\n",
    "\n",
    "    Given the political and economic power incarceration has over communities, and the stark disparities in drug arrests prior to predictive policing, it is not unfathomable that the model creators (or users) wanted to defend existing policing practices by creating an 'unbiased, evidence-based' model that was just as targeted and nefarious as the original legislation which criminalized drugs. \n",
    "    \n",
    "    \n",
    "3. A perverse model optimization goal. According to the Lum and Isaac's article: predictive policing is defined as “the application of analytical techniques, particularly quantitative techniques, to identify likely targets for police intervention and prevent crime or solve past crimes by making statistical predictions.' \n",
    "\n",
    "    This definition is widely vague enough to allow for many optimization goals. If this model's goal is to optimize police allocation to maximize drug arrests, volume becomes the primary incentive for the model to optimize for. \n",
    "\n",
    "    In closely examining the graphics in the Lum & Isaac article, I noticed that arrests are most concentrated in West Oakland, between Intestate 580 and Interstate 980. This is not where the reported drug use is highest, which is actually slightly east to where arrests are concentrated. However, this area is conveniently 10 minutes away from the Oakland police department. If there is a volume incentive, or department quota that must be met, the model would prefer likely drug users closer to the department rather than further away, which concentrates arrests and creates the disparities we see. Highlighting this hypothesis, is the original Lum & Isaac graphic, below, showing the difference between the highest use areas and highest arrest areas. Then another visual from Google maps showing an approximate 10 minute drive distance from the Oakland Police department to the area with highest arrests. \n",
    "    \n",
    "John Ehrlichman Quote Source: https://www.vox.com/2016/3/22/11278760/war-on-drugs-racism-nixon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa07883",
   "metadata": {},
   "source": [
    "**Lum and Isaac Graphic, with circle highlighting most active drug use area on left and drug arrest area on right**\n",
    "\n",
    "![Lum's and Isaac Graphic, with circle highlighting most active drug use area and drug arrest area](police_dpt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d768655",
   "metadata": {},
   "source": [
    "**Graphic showing distance from most active arrest area to police department**\n",
    "![Graphic showing distance from most active arrest area to police department](lum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10fcf8",
   "metadata": {},
   "source": [
    "## Problem 1B:  \n",
    "\n",
    "Propose two mitigation strategies to counteract racial disparities in the predictions of such a system. Note: It is insufficient to state that we could use a specific pre-, in- or post-processing technique that we covered in class when we discussed fairness in classification. Additional details are needed to demonstrate your understanding of how the ideas from fairness in classification would translate to this scenario.\n",
    "\n",
    "\n",
    "1. A specific fairness constraint within the model's loss function could mitigate these racial disparities . As mentioned above, if the model is trying to maximize the number of arrests, distance from the police department greatly impacts who will be targeted. However, if there is a fairness constraint, for example number of arrested drug users per capita must be approximately uniformly distributed by zip code, this would prevent the model from exclusively selecting the closest likely targets and ensure likely targets from all of Oakland are included. \n",
    "\n",
    "\n",
    "2. A second way to mitigate the disparities would be to use Reject Option Post-Processing on the dataset. Our assumption here is that the unprivileged class are People of Color. However, for this model to even be legally admissible, race should not be a feature in the model. Therefore, we would use geography as the variable we would create privileged and unprivileged groups, setting the zip codes with the highest arrest concentration as unprivileged and the zip codes with minimal arrests as the privileged groups. \n",
    "\n",
    "    It's still worth noting that even this may me be legally inadmissible, as this could be similar to 'red-lining' but for police data, however, it depends on the level of specificity of geographic location (a 5 digit zip code, vs a 3 digit zip code for example). By doing this, likely targets for drug use within the privileged group would get de-emphasized and likely targets for drug use outside the concentrated areas would be prioritized by the model in order to geographically re-balance arrest concentration. \n",
    "    \n",
    "In looking at these two different recommendations, the first proposal seems much more legally fair.  fairness constraint that per-capita likely targets by zip code must be approximately uniform enforces that all populations are considered, which is in line with the mission of police to protect and serve the entirety of Oakland, and it also does not try to remediate existing racial bias by specifically calling out populations as privileged and unprivileged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17f78a",
   "metadata": {},
   "source": [
    "# Problem 2  Randomized response\n",
    "\n",
    "The simplest version of randomized response involves flipping a single fair coin (50% probability of heads and 50% probability of tails). As in the example we saw in class, an individual is asked a potentially incriminating question, and flips a coin before answering. If the coin comes up tails, he answers truthfully, otherwise he answers “yes”. Is this mechanism differentially private? If so, what $\\epsilon$ value does it achieve? Carefully justify your answer.\n",
    "\n",
    "## Problem 2 answer:\n",
    "\n",
    "\n",
    "Epsilon, is calculated as the ratio of ln($\\frac{P(A|P)}{P(A|-P)}$)\n",
    "\n",
    "\n",
    "\n",
    "Let: \n",
    "        \n",
    "        A = the response given by the individual, in our example we set it to No\n",
    "        P = True outcome of the event, which we set to Yes \n",
    "        \n",
    "\n",
    "P(A|P) = 0 (if coin flip is tails, he answers yes instead of no)\n",
    "$\\newline$\n",
    "P(A|-P) = 0 (if coin flip is heads, he answers yes instead of no)\n",
    "\n",
    "\n",
    "Therefore, the ratio of $\\frac{P(A|P)}{P(A|-P)}$ is undefined, meaning this process **is not** differentially private. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1ba4b",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Consider the dataset below, and assume that sex is one of {M, F}; edu is one of {HS, BS, MS}; and loan is one of {yes, no}. Here, sex is the protected attribute, and loan represents the binary classification outcome (the target variable): loan=yes is the positive outcome, loan=no is the negative outcome.\n",
    "\n",
    "![rule_mining](rule_mining.png)\n",
    "\n",
    "A classification association rule (CAR) is a non-trivial association rule of the form X1, ..., Xn →Y, where Y is an assignment of a value to the target variable (loan=yes or loan=no), X1, ..., Xn is an assignment of values to one or several other variables. For example: sex=M, edu=BS → loan=yes is a CAR, while loan=yes → sex=F is not.\n",
    "\n",
    "To mine CARs from a dataset, you may think of each tuple (row) as a “transaction”, and then apply the Apriori algorithm we covered in class (during the data profiling lecture in Week 6) to find CARs that meet or exceed the specified confidence and support thresholds.\n",
    "\n",
    "\n",
    "## Problem 3.A\n",
    "\n",
    "List all CARs that relate the likelihood of the classification outcome (loan=yes or loan=no), with the value of the sensitive attribute sex. These CARs should list sex on the left-hand-side, either on its own or in combination with other attributes. List only those CARs that have support >= 3 and confidence >= 0.6. For each CAR you list, state its support and confidence.\n",
    "\n",
    "| Rule                   | Implies | Loan Status | Support | Confidence |\n",
    "|------------------------|---------|-------------|---------|------------|\n",
    "| (gender=F)             | ->      | Loan = No   | 11      | .688      |\n",
    "| (gender=F & edu=HS)    | ->      | Loan = No   | 6       | 1      |\n",
    "| (gender = M)           | ->      | Loan = Yes  | 11      | .688       |\n",
    "| (gender = M, edu = BS) | ->      | Loan = Yes  | 4       | .667       |\n",
    "| (gender = M, edu = MS) | ->      | Loan = Yes  | 4       | 1      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7c934",
   "metadata": {},
   "source": [
    "## Problem 3B\n",
    "\n",
    "Suppose that you are required to release differentially private versions of the frequent item-sets (the union of their left-hand-side and right-hand-side) that correspond to the CARs you computed in part (a), along with their support. Your overall privacy budget is ε=1. Use sequential and parallel composition to allocate portions of the privacy budget to each frequent item-set you will release. Your goal is to maximize utility of the information you release, while staying within the privacy budget.\n",
    "\n",
    "\n",
    "Write down a way to allocate portions of the privacy budget to each frequent itemset you will release, to achieve good utility. Be specific, write down an epsilon value for each itemset. Carefully justify your solution using sequential and parallel composition.\n",
    "\n",
    "\n",
    "### Answer:\n",
    "\n",
    "Query list:\n",
    "\n",
    "        1. Select gender, education, count(loan_status) as num_approved where gender = M and loan_status = 'yes' and education in ('BS', 'MS) groupBy gender, education (epsilon budget = .25)\n",
    "        2. Select gender, count(loan_status) as num_approved where gender = M and loan_status = 'yes' groupBy gender (epsilon budget = .25)\n",
    "        3. Select gender, count(loan_status) as num_rejected where gender = F and loan_status = 'no' groupBy gender (epsilon budget = .25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
