{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13b28f0",
   "metadata": {},
   "source": [
    "# DS GA 1017 HW 1\n",
    "## Algorithmic Fairness\n",
    "\n",
    "# Joby George (jg6615)\n",
    "## Due 3/2/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f34d05",
   "metadata": {},
   "source": [
    "# Problem 1A Prompt:\n",
    "\n",
    "## Fairness from the point of view of different stakeholders\n",
    "\n",
    "For each of the following metrics \n",
    "\n",
    "        A. Accuracy\n",
    "        B. Positive Predictive value\n",
    "        C. False Positive Rate\n",
    "        D. False Negative Rate \n",
    "        E. Statistical Parity\n",
    "        \n",
    "        \n",
    "Provide a 1-2 sentence summary of which stakeholders benefit and are harmed by optimizing a model using that metric, and why. Add commentary on whether or not it would be reasonable within the context of the [COMPAS](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) investigation by ProPublica, and [Northpointe's response](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64aad05",
   "metadata": {},
   "source": [
    "## Problem 1A Prelude:\n",
    "\n",
    "Before directly answering the question, I believe it is worthwhile to re-iterate some frameworks reviewed in lecture regarding algorithimic fairness in the problem space of criminal justice while philosophically entertaining various vantage points. \n",
    "\n",
    "When thinking about algorithimic fairness there are several important questions:\n",
    "\n",
    "        1. Who are we building this model for?\n",
    "        2. How do we define and measure whether the automated decision system improves upon our current system.\n",
    "        3. Algorithimic Impact Assessment (AIA) criteria \n",
    "        \n",
    "The stakeholders involved in the scope the COMPAS model are numerous and of extreme consequence:\n",
    "\n",
    "    1. The consumer: the United States Federal and State Governments and it's judiciary system\n",
    "    2. The producer: Northepoint \n",
    "    3. The subjects: Defendants on trial \n",
    "    4. Long term impacted stakeholders: Future United States citizens \n",
    "\n",
    "\n",
    "# Who are We building this Model For\n",
    "    \n",
    "In the Mirror Mirror comic, a core question about Automated Decision Systems was: who are building models for? \n",
    "\n",
    "These stakeholders impacted by COMPAS all have various different needs and priorities, touched upon below.\n",
    "\n",
    "### The United States itself\n",
    "\n",
    "To create a more functioning society for everyone, the government of the United States would prefer to minimize crime, especially violent crime as it destablizes communities and undermines the authority of the United States government itself. Examples of unfettered crime being disasterous to the average citizen include many narcostates that corrode the integrity of democracy and enforce the fairness motto of *might is right.*\n",
    "\n",
    "However, minimizing crime through incarceration comes with a cost. The United States is the world leader in incarcerated population per 100,000 people$^1$. Unsurprisingly when it comes to crime, and most importantly violent crime, the US is not the world's leader. According to the World Bank, the United States has an intentional homicide rate of around 5 people per 100,000, closer to countries like Afghanistan (7/100k) and the Phillipines (6/100k) than other Western Democracies France and Britain (both with 1/100K)$^2$\n",
    "\n",
    "It is evident from a performance, and financial perspective the United States government is eager to find a scalable, more performant and cheaper solution to criminal justice, where less people are in jail and less crime occurs, especially violent crime.\n",
    "\n",
    "\n",
    "### Northpointe\n",
    "\n",
    "Northpointe has a simple goal compared to the consumer. Provide a model that is legally compliant and more performant than the existing system when it comes to reducing crime rates and incarceration years of non-recidivist defendants. If they can provide this solution, they have a very willing and large customer poised to buy their model. \n",
    "\n",
    "\n",
    "### Defendants on trial\n",
    "\n",
    "Defendants are people who have been apprenhended by police for committing some crime (whether or not they have actually committed the crime in question). The defendant is primarily interested in him or herself, maximizing their own utility. In the majority of cases, we could assume this would mean that the defendant pays a minimal bail or serves a minimum sentencing. \n",
    "\n",
    "###  Future United States citizens \n",
    "\n",
    "Future United Citizens are stakeholders that are impacted by emergent bias. If the COMPAS model is widely used and has a bias, certain communities can bear the cost of disproportionate incarceration which undoubtedly impacts their families and children.\n",
    "\n",
    "America has seen paralels with this in the War on Drugs, where low level dealers of marijuana and crack cocaine were severely punished through the use of mandatory minimums compared to white drug users and dealers.\n",
    "\n",
    "HumanRightsWatch.org states this succintly: \"In the poor urban minority communities from which most black drug offenders are taken, the high percentage of men and, increasingly, women sent to prison may also undermine their communities' moral and social cohesion. By damaging the human and social capital of already disadvantaged neighborhoods, the \"war on drugs\" may well be counterproductive, diminishing opportunities for social and economic mobility and even contributing to an increase in crime rates.\"$^3$\n",
    "\n",
    "\n",
    "# Defining a better society \n",
    "\n",
    "In class we've discussed the different viewpoints of fairness and what a more fair or better society would look like.\n",
    "\n",
    "The three most commonly discussed viewpoints were:\n",
    "        \n",
    "        1. Utilitarianism, which boils down to creating the most good for the most people, \n",
    "        2. Raising the minimum, not compromising the well being of an underprivleged minority \n",
    "            to raise the average for a larger body. \n",
    "        3. Rawl's theory of Justice, positing that if no one knew who they would in a hypothetical society, \n",
    "            the designers would design a system where any subgroup would not be\n",
    "            harmed to benefit another subgroup.\n",
    "        \n",
    "In thinking about what an improved society would look like, the main goals to optimize would be (in no particular order):\n",
    "\n",
    "### 1. Crime Prevention \n",
    "\n",
    "Minimizing the ability and frequency of crime, especially violent crimes which create loss of life for innocent victims.\n",
    "\n",
    "### 2. Criminal Justice \n",
    "\n",
    "Preventing someone who did not commit a crime to wronlgy have their liberties infringed (inappropriate bail, inordinate sentencing).\n",
    "\n",
    "### 3. Social Justice\n",
    "\n",
    "Alleviating the burden of citizens for exploding financial costs from high incarceration rates and\n",
    "preventing any reformed system to propogate negative societal externalities that lead to worse societal outcomes. \n",
    "\n",
    "Ideally, a system would co-optimize among all three of these priorities, but there exists an inherent tension, especially when considering the framework of fairness to be applied (utilitarianism vs raising the minimum vs Rawlsian).\n",
    "\n",
    "# Algorithimic Impact Assessment\n",
    "\n",
    "The best framework we have now to think about how to balance risks in ADS is the AIA which posits:\n",
    "\n",
    "    1. The likelihood of harm caused by an ADS and \n",
    "    2. The severity of harm caused by an ADS\n",
    "    3. Determine the level of oversight on an ADS\n",
    "    \n",
    "Taking all of these things into account, I will now proceed to answer the primary question for Problem 1.\n",
    "\n",
    "\n",
    "### Sources:\n",
    "1: [BBC, prison stats](http://news.bbc.co.uk/2/shared/spl/hi/uk/06/prisons/html/nn2page1.stm) \n",
    "<br/>\n",
    "2: [World Bank Data](https://data.worldbank.org/indicator/VC.IHR.PSRC.P5?most_recent_value_desc=true) \n",
    "<br/>\n",
    "3. [humrightswatch.org](https://www.hrw.org/legacy/reports/2000/usa/Rcedrg00.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa32eb6",
   "metadata": {},
   "source": [
    "# Problem 1 Answer\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy is defined as the number of correct predictions divided by the total sample size. It gives a baseline estimate to the average performance of a model, but can often be a misleading evaluation criteria. For example, for cancer diagnosis, a simple yet highly accurate model would be predicting that no new patient has cancer, given the incidence rate of Cancer is approximately .22% globally.$^4$.\n",
    "\n",
    "In the case of COMPAS, optimizing for accuracy benefits **Northepointe** and the direct interpreter's of the model's report (**the US judiciary system**) the most. By optimizing for accuracy, the model score can be interpreted in aggregate as a probability of recidivism. \n",
    "\n",
    "Given the difference in base rates of recidivisim in sub-groups, it's probable that we'll see an imbalance in false positive and false negatives amongst these sub-groups, especially since the larger population (whites) have a lower recidivism base rate in our data. \n",
    "\n",
    "While not perfect, optimizing for accuracy does have some advantages, namely the interpretability of the model in aggregate, and the balance between false positive and false negative cost, both of which are extremely high for the stakeholders of the error. Therefore, I believe it is the third best metric to optimize for. \n",
    "\n",
    "## Positive Predictive value\n",
    "\n",
    "Positive Predictive value is defined as: \n",
    "\n",
    "$$ \\frac{True \\space Positive}{True \\space Positive  + False \\space Positive}$$ .\n",
    "\n",
    "Or more intuitively, if the COMPAS model scores someone as high risk, what is the **actual** probability of recidivism. \n",
    "\n",
    "Just like in medicine, this is strongly determined by the base of the underlying phenonema (rate of recidivisim and not recidivism). In optimizing for PPV, we can balance the imbalanced false positive risk associated with African Americans.\n",
    "\n",
    "PPV as the  best metric to optimize for, since it does have a statistically meaningful interpretation, however we wouldn't fix the false negative problem for Whites. \n",
    "\n",
    "##  False Positive Rate\n",
    "\n",
    "False positive rate is defined as:\n",
    "\n",
    "$$ \\frac{Labelled \\space High \\space risk \\space and \\space did \\space not \\space recidivate}{Did not Recidivate} $$\n",
    "\n",
    "If we optimize (minimize) false positive rate, we are incentivized to score people as less risky, and are more likely to make a false negative error. This would benefit defendants, as they would be less likely to have their rightsi infringed, however this would harm victims of recidivists.  Given the high costs of a false negative, especially in the case of violent crime I would rank False Positive Rate as the worst metric to optimize against.\n",
    "\n",
    "\n",
    "## False Negative Rate \n",
    "\n",
    "False negative rate is defined as:\n",
    "\n",
    "$$ \\frac{Labelled \\space low \\space risk \\space and \\space did \\space  \\space recidivate}{Recidivated} $$\n",
    "\n",
    "Optimizing this would lead to a model that would be more likely to incarcerate people, harming defendants and the US tax payer. However, I do think, with a carefully applied use of a COMPAS model optimized on False Negative Rate, that the tool could be used well. If the model is optimized towards false negative risk, then defendants that score low on the model's risk score would be unlikely to re-offend. If justices had that information it could help ease the burden on the judicial system and allow true negatives some relief. However, the model should not be used in a punitive way, such that high risk scores are used to penalize defendants. Meaning the justice system would still be overwhelmed by the number of cases it has to work. \n",
    "\n",
    "The big risk would be whether the score distribution would still remain the same as it does by sub-groups, where Black Americans would have fewer in the least risky bucket after the algorithim is repurposed for a different type of prediction. With this in doubt, I would rank false negative rate as the second best metric to optimize for. \n",
    "\n",
    "\n",
    "##  Statistical Parity\n",
    "\n",
    "Statistical Parity is that the model's false positive and false negative error are balanced among sub groups. \n",
    "This would benefit defendents and the US justice system, as it does mean that the risk assessment could be employed without fear of racial bias. I would recommend optimizing for this metric, as while there may be more errors in general, no one group is bearing the majority of the consequences of the model's error. \n",
    "\n",
    "While this would guarantee fairness, the broad usage and lower predictive power of the metric are downsides. I would not recommend optimizing for this metric. \n",
    "\n",
    "\n",
    "### Sources:\n",
    "4: [CA: A Cancer Journal for Clinicians](https://acsjournals.onlinelibrary.wiley.com/doi/10.3322/caac.21660) \n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304411b",
   "metadata": {},
   "source": [
    "# Problem 1B Prompt: Tech Corp Recruiting System\n",
    "\n",
    "Describe the:\n",
    "\n",
    "        1. Pre-existing Bias\n",
    "        2. Technical Bias\n",
    "        3. Emergent Bias\n",
    "        \n",
    "of an algorithimic hiring reccomendation system, detailling\n",
    "\n",
    "        an example of how this type of bias may arise in the scenario described above\n",
    "        a stakeholder group that may be harmed by this type of bias\n",
    "        propose an intervention that may help mitigate this type of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3c40c",
   "metadata": {},
   "source": [
    "# Problem 1B Answer:\n",
    "\n",
    "## Pre-existing Bias\n",
    "\n",
    "Pre-existing Bias would exist in the data gathering process when training the AI. Tech as an industry is heavily dominanted by men, meaning women resumes would be deprioritized by the algorithim. The primary stakeholders that would be hurt by this type of bias are those in small minority groups in the tech industry, whether racial (Blacks, Latinos), gender (women), or disability status, the algorithim would not prioritize these groups given these populations small prevlanece in the training data.\n",
    " \n",
    "An intervention that would help mitigate this type of bias would be over-sampling or synthetic data creation to ensure that the algorithim has enough sample of under-represented groups. This way the decision making system would not codify existing industry biases.\n",
    "\n",
    "## Technical Bias\n",
    "\n",
    "Technical bias is a bias that is caused by limitations in technology or how it is used. In this case, the ADS sorts candidates by descending score. Candidates whose names fall outside the first screen's worth of top candidates, or are highly ranked but have a first and last name that starts with a Z would be chosen less frequently, given their worse placement in the sorting algorithim.\n",
    "\n",
    "To remediate these seperate biases, rather than a strict descending order that's displayed on a page that has to be iterated on for the next set fo candidates, the algorithim could present the top x candidates, in a shuffled order by rating and name. This way, someone with a last name starting with an A with the same score as someone with a last name starting with a Z would not be systemically favored. Additionally, top candidates would all be given a more equal chance by the recruiter, while the recruiter him or herself would be more engaged with each individual candidate as they have to apply their own heuristics to each candidate, rather than exclusively on the first page.\n",
    "\n",
    "## Emergent Bias\n",
    "Emergent bias is a cause-and-effect feedback loop caused by ADS. In this case, groups that are already biased-against by the ADS are further discriminated by the ADS as the next iteration of training data is further skewed by the implementation of Prophecy. \n",
    "\n",
    "For example, women of color might comprise 10% of ProphecyV1's training data. Once Prophecy is in use, the proportion of new hires that are women of color is now 5%, as the algorithim negatively penalizes candidates for these demographic attributes. Therefore, when re-versinong to ProphecyV2 and each future iteration, the model will more strongly penalize these demographic attributes as women of color are now further under-represented in the training data. Eventually, women of color will come to officially, or unofficially learn of this bias, and avoid applying to the company, completing the feed-back loop of emergent bias.\n",
    "\n",
    "Remediating this bias can be done by the modelling or HR functions. This vicious cycle can be transformed into a virtuous cycle if the model developers or company correct for this bias by over-sampling, sample reweighting, or modifying their loss function. \n",
    "\n",
    "Alternatively,instituting explicit DEI organizational hiring policy will interrupt the emergent bias's feedback loop. If both practices are done simultaneously, the organizational will be able to cultivate a more tolerant culture that will create a virtuous cycle for talent. \n",
    "\n",
    "Of important note in regards to emergent bias, is that it can be addressed through non-technical intervention.n This highlights that when thinking of how to solve a problem data scientists should not limit themselves to only modelling based interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562787d",
   "metadata": {},
   "source": [
    "# Problem 1C: A Prompt:\n",
    "\n",
    "University Admissions at Best University are defined by:\n",
    "\n",
    "        1. SAT Score\n",
    "        2. GPA\n",
    "        3. Income Bracket (low/mid/high)\n",
    "        \n",
    "\n",
    "Describe the arguement the Formal Equality of Opportunity (EOP) fairness doctrines would recommend to the admission office at Best University:\n",
    "\n",
    "\n",
    "\n",
    "        1. Formal Equality of Opportunity\n",
    "\n",
    "\n",
    "# 1C: A Answer:\n",
    "\n",
    "Formal EOP would require that the admissions criteria be the moment in time evaluation criteria (SAT/ GPA). The doctrine would be blind to Income Bracket as it deems it irrelevant to the qualification criteria, thus selecting the students with the best SAT and GPA would be a fair system that negates heriditary advantage.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4703392",
   "metadata": {},
   "source": [
    "# Problem 1C: B Prompt:\n",
    "\n",
    "Suppose that income-based differences are observed in applicants’ SAT scores: the median score is lower for applicants from low-income families, as compared to those from medium- and high-income families. Which EO doctrine(s) is/are consistent with the goal of correcting such differences in the applicant pool? Briefly justify your answer\n",
    "\n",
    "\n",
    "# 1C: B Answer:\n",
    "\n",
    "The two doctrines that would attempt to correct for this are:\n",
    "\n",
    "        1. Substantive / Luck egalitarian \n",
    "        2. Substantive / Rawlsian.\n",
    "        \n",
    "Luck Egalitarians would posit that nothing you did not chose for yourself should impact your life prospects. Therefore, wealth, which is determined by birth as a child should not factor into the admissions criteria for Best University.\n",
    "\n",
    "Rawlsian Egalitarins would posit a slightly different arguement. Equally taleneted babies at birth must be given equal life prospects. Best university could either use equalize odds, such that the true positive rate and false positive rates are equal, or implement equality of opportunity which enforces true positive rate to be the same.\n",
    "\n",
    "\n",
    "### Equal Odds:\n",
    "\n",
    "$$ P(\\hat{Y} = 1 | income = Low, Y = 1) = P(\\hat{Y} = 1 | Income \\neq Low, Y = 1)  $$ and\n",
    "\n",
    "$$ P(\\hat{Y} = 0 | income = Low, Y = 0) = P(\\hat{Y} = 0 | Income \\neq Low, Y = 0)  $$ \n",
    "\n",
    "\n",
    "\n",
    "### Equal Opportunity:\n",
    "$$ P(\\hat{Y} = 1 | income = (Low), Y = 1) = P(\\hat{Y} = 1 | Income \\neq Low, Y = 1) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2d59c",
   "metadata": {},
   "source": [
    "# Problem 1C: C Prompt:\n",
    "\n",
    "Describe an applicant selection procedure that is fair according to luck-egalitarian EO.\n",
    "\n",
    "# 1C: C Answer:\n",
    "        \n",
    "Luck Egalitarians would posit that nothing you did not chose for yourself should impact your life prospects. Therefore, wealth, which is determined by birth as a child should not factor into the admissions criteria for Best University. Since this is impossible to enforce at the moment in time of admissions, we would compare members of the low income group to each other and members of the high income group to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf968a6",
   "metadata": {},
   "source": [
    "# Problem 1D: \n",
    "\n",
    "Prove that if the underlying base rate between two groups is different, we cannot simultaneoulsy achieve:\n",
    "\n",
    "        1. Equal Accuracy\n",
    "        2. Equal False Positive Rates\n",
    "        3. Equal False Negative Rates.\n",
    "        \n",
    "        \n",
    "        \n",
    "### Definitions    \n",
    "        \n",
    "Using the Compas example from Chouldechova's Fiar *Fair prediction with disparate impact*: \n",
    "\n",
    "**Equal accuracy** \n",
    "\n",
    "$$P(Y = 1 | S \\geq s, R=b) = P(Y = 1 | S \\geq s , R=w) $$\n",
    "\n",
    "\n",
    "**Equal False Positive Rates**\n",
    "\n",
    "$$P(S < s_{threshold}  | Y=1, R=b) = P(S < s_{threshold}| Y=1, R=w) $$\n",
    "\n",
    "**Equal False Negative Rates** \n",
    "\n",
    "$$P(S \\geq s_{threshold}  | Y=0, R=b) = P(S \\geq s_{threshold}| Y=0, R=w) $$\n",
    "\n",
    "**Base Rate assumption**\n",
    "\n",
    "\n",
    "$$P(S \\geq s_{threshold} | R =b) \\neq P(S \\geq s_{threshold} | R =w) $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ P(S \\geq s_{threshold} |R = w)  = \\frac{TP_{w}+FN_{w}}{N_{w}} $$\n",
    "\n",
    "## Solution\n",
    "\n",
    "We know:\n",
    "\n",
    "$$FNR_{w}= \\frac{FN_{w}}{FN_{w} + TP_{w}}$$\n",
    "\n",
    "## Step 1\n",
    "Plugging in $FN_{w}$ + $TP_{w}$ = $N_{w}$ $P_{w}$ (N is the total sample here), we get: \n",
    "\n",
    "$$FNR_{w}= \\frac{FN_{w}}{NP_{w}}$$\n",
    "\n",
    "## Step 2:\n",
    "Plugging in $FN_{w}$ = $N_{w}$ $P_{w}$ - $TP_{w}$ from our above formula for $P_{w}$ we get:\n",
    "\n",
    "$$FNR_{w}= \\frac{N_{w}P_{w}-TP_{w}}{N_{w}P_{w}}$$\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "$$FNR_{w}=  1-\\frac{TP_{w}}{N_{w}P_{w}} $$\n",
    "\n",
    "## Step 3:\n",
    "Plugging in our definition for TP using Accuracy we get:\n",
    "\n",
    "$$FNR_{w}=  1-\\frac{N_{w}Accuracy_w-TN_{w}}{N_{w}P_{w}} $$\n",
    "\n",
    "Simplifying this we get: \n",
    "\n",
    "$$FNR_{w}=  1-\\frac{Accuracy_w}{P_w} - \\frac{TN_{w}}{N_{w}P_{w}} $$\n",
    "\n",
    "\n",
    "## Step 4: \n",
    "Plugging in our definition for TN using FPR we get:\n",
    "\n",
    "\n",
    "$$FNR_{w}=  1-\\frac{Accuracy_w}{P_w} - \\frac{\\frac{FP_{w}(1-FPR_w)}{FPR_w}}{N_{w}P_{w}} $$\n",
    "\n",
    "\n",
    "## Step 5: \n",
    "We can plug in our definition of FPR:\n",
    "\n",
    "$$FPR_w = \\frac{FP_w}{FP_w + TN_w}$$\n",
    "\n",
    "$$ \\frac{\\frac{FP_{w}(1-FPR_w)}{FPR_w}}{N_{w}P_{w}}  = $$ \n",
    "\n",
    "$$ \\frac{FP_w(1-(\\frac{FP_w}{FP_w + TN_w})FP_w+TN_w}{{FP_w}N_{w}P_{w}}$$\n",
    "\n",
    "## Step 6: \n",
    "\n",
    "Simplifying this we get \n",
    "$$ \\frac{FP_w(1-(\\frac{FP_w}{FP_w + TN_w})FP_w+TN_w}{{FP_w}N_{w}P_{w}}$$\n",
    "\n",
    "$$ \\frac{(FP_w+TN_w-FP_w)}{N_{w}P_{w}}$$\n",
    "\n",
    "and lastly \n",
    "\n",
    "$$ \\frac{TN_w}{N_{w}P_{w}}$$\n",
    "\n",
    "which was gives us the original equation we were algebraically manipulating:\n",
    "\n",
    "$$ 1-\\frac{Accuracy_w}{P_w} - \\frac{TN_{w}}{N_{w}P_{w}} $$\n",
    "We can validate that this expression does in fact equal FNR:\n",
    "\n",
    "$$FNR_w =  1-\\frac{Accuracy_w}{P_w} - \\frac{TN_{w}}{N_{w}P_{w}} $$\n",
    "\n",
    "$$ = 1 - \\frac{TN_w+TP_w}{FN_w+TP_w} - \\frac{TN_w}{FN_w+TP_w} $$ \n",
    "\n",
    "$$ = 1 - \\frac{TP_W}{FN_w+TP_w} $$\n",
    "$$ = \\frac{FN_w+TP_w-TP_W}{FN_w+TP_w} $$\n",
    "= $FNR_w$\n",
    "\n",
    "### Q.E.D. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326ce32",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1f196",
   "metadata": {},
   "source": [
    "# Problem 3 \n",
    "\n",
    "## Facial Recognition technology in Police Investigations\n",
    "\n",
    "\n",
    "Police facial recognition to identify criminal suspects and potentially incarcerate them. based ADS.\n",
    "\n",
    "The use case from a Data Scientists perspective is facial matching, using camera data to come up with the known person that matches that facial data. This general technology can be used for a number of purposes (FaceID, for example), but in this case the Police were using it to identify criminal suspects based on camera data.\n",
    "\n",
    "## beneficiaries\n",
    "\n",
    "The primary stakeholders are the police investigations unit, and the corporations selling their technology. \n",
    "\n",
    "### Police Investigations\n",
    "\n",
    "Facial recognition, if it was as infallible as DNA matching, would be an utopian technology for law enforement as they could ID the true suspect with any picture of the criminal. However, our current technology has systemic performance problems with minorities, especially those with darker skin, leading to incorrect ID of suspects.\n",
    "\n",
    "### Tech Companies\n",
    "\n",
    "The companies selling this facial recognition software to Police Departments are financially rewarded for their development of the facial recognition software, even if it comes with considerable flaws.\n",
    "\n",
    "### Cauciasn Males\n",
    "\n",
    "White men have been shown to have the most representation in the training data, and comprise a large portion of the technology professionals managing facial recognition software. In studies, it has been shown that the White Males have an extremely low error rate on commercial facial recognition software, this prevents them from being falsely incarcerated, and creates an illusion of accuracy in the model\n",
    "\n",
    "## penalized stakeholders\n",
    "\n",
    "### Women\n",
    "\n",
    "Studies have found that women in general, and especially women PoC, facial recognition error rates are magnitude highers compared to other demographic groups. This means that women, and women PoC, are more lik\n",
    "\n",
    "### PoC (and the intersection)\n",
    "\n",
    "Similarly to women, PoC, and those with darker complexions are found have demonstrably lower accuracy. As mentioned before, the intersection of these groups see magnifying error rates and are thus exponentially more likely to be misidentified by the police. \n",
    "\n",
    "## Disparate Impact for PoC in facial recognition\n",
    "\n",
    "[Dr. Timnit Gebru and Joy Buolamwini's paper](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) contains the main known ethical issues with facial recognition today. \n",
    "\n",
    "In the paper, the authors recap existing biases:\n",
    "\n",
    "        1. Pre-existing bias of imbalaned datasets for intersections of PoC and gender\n",
    "        2. Pre-existing bias, the companies that are responsible for fixing them are not incentivized to fix them or proclaim the model's errors to their customers.\n",
    "        3.Emergent bias -- incorcetly incarcerating and violating civil liberties of PoC as more localities and countries adopt this law enforcement practice\n",
    "        \n",
    "        \n",
    "\n",
    "In this case, a [few large corporations backed out of agreements with law enforcement](https://www.washingtonpost.com/technology/2020/06/11/microsoft-facial-recognition/), however, Google, the company which [notoriously fired Dr. Gebru](https://www.bbc.com/news/technology-55164324) is now [claiming to have made signficant progress in this space.](https://www.youtube.com/watch?v=gGZx0RrL-ow)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20b85a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
