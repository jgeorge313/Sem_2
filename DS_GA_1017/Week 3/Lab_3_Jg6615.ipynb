{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:rds_env]",
      "language": "python",
      "name": "conda-env-rds_env-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Lab 3 Jg6615.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DKrcGGhQild"
      },
      "source": [
        "# Lab 3: Exploring Fairness When Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SyL9goGBjya"
      },
      "source": [
        "In this lab, we will detect bias that may be introduced while training classifiers. We will mitigate this bias via pre-processing and post-processing.  \n",
        "\n",
        "This notebook has four stages in which we will: \n",
        "1. Import and split the data into train/val/test sets.\n",
        "2. Train a classifier to predict credit using original data with or without sensitive features.\n",
        "3. Preprocess the data using the reweighting algorithm and train a classifier using the reweighted data.\n",
        "4. Post-process the predictions using the calibrated equality of odds algorithm. For each prediction from step 2, 3, and 4, we will measure bias using fairness metrics including mean outcomes, disparate impact, false positive rate, and false negative rate. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKooNAay1wNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a781b5-ab53-4bb0-d9dc-838fc48a3431"
      },
      "source": [
        "!pip install numba==0.48\n",
        "!pip install aif360==0.2.2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba==0.48 in /usr/local/lib/python3.7/dist-packages (0.48.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba==0.48) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.48) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba==0.48) (1.19.5)\n",
            "Requirement already satisfied: aif360==0.2.2 in /usr/local/lib/python3.7/dist-packages (0.2.2)\n",
            "Requirement already satisfied: pandas>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from aif360==0.2.2) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from aif360==0.2.2) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from aif360==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.3->aif360==0.2.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.3->aif360==0.2.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.3->aif360==0.2.2) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->aif360==0.2.2) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->aif360==0.2.2) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyPDr4QuQilp"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "random.seed(6)\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.datasets import GermanDataset, StandardDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY7AwOvjQilr"
      },
      "source": [
        "## Step 1: Load the data\n",
        "\n",
        "The German Credit Risk dataset contains 1000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes. The original dataset can be found at https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvvwkowuQilr"
      },
      "source": [
        "### 1.1 Read in the aif360 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9hntAiQils"
      },
      "source": [
        "dataset_orig = GermanDataset(protected_attribute_names=['age'],           \n",
        "                             privileged_classes=[lambda x: x >= 25], \n",
        "                             features_to_drop=['personal_status', 'sex'])      # age >=25 is considered privileged\n",
        "\n",
        "# Store definitions of priviledged and unpriviledged groups\n",
        "privileged_groups = [{'age': 1}]\n",
        "unprivileged_groups = [{'age': 0}]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eYd60CSQilt"
      },
      "source": [
        "### 1.2 Split into train/val/test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hNDW2gWQilu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e7fb2a-66c5-4437-b42a-e2da0f3c53ac"
      },
      "source": [
        "# Split original data into train and test data\n",
        "train_orig, test_orig = dataset_orig.split([0.8], shuffle=True, seed=10)\n",
        "# Split training data in to training and validation data for hyperparameter tuning\n",
        "train_orig, val_orig = train_orig.split([0.75], shuffle=True)\n",
        "\n",
        "# Convert to dataframes\n",
        "train_orig_df, _ = train_orig.convert_to_dataframe()\n",
        "val_orig_df, _ = val_orig.convert_to_dataframe()\n",
        "test_orig_df, _ = test_orig.convert_to_dataframe()\n",
        "\n",
        "print(\"Train set: \", train_orig_df.shape)\n",
        "print(\"Val set: \", val_orig_df.shape)\n",
        "print(\"Test set: \", test_orig_df.shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set:  (600, 58)\n",
            "Val set:  (200, 58)\n",
            "Test set:  (200, 58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y1Ubj8DQilw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "a3cda750-6597-42d0-dbea-460429197249"
      },
      "source": [
        "print(train_orig_df.columns)\n",
        "train_orig_df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['month', 'credit_amount', 'investment_as_income_percentage',\n",
            "       'residence_since', 'age', 'number_of_credits', 'people_liable_for',\n",
            "       'status=A11', 'status=A12', 'status=A13', 'status=A14',\n",
            "       'credit_history=A30', 'credit_history=A31', 'credit_history=A32',\n",
            "       'credit_history=A33', 'credit_history=A34', 'purpose=A40',\n",
            "       'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43',\n",
            "       'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48',\n",
            "       'purpose=A49', 'savings=A61', 'savings=A62', 'savings=A63',\n",
            "       'savings=A64', 'savings=A65', 'employment=A71', 'employment=A72',\n",
            "       'employment=A73', 'employment=A74', 'employment=A75',\n",
            "       'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103',\n",
            "       'property=A121', 'property=A122', 'property=A123', 'property=A124',\n",
            "       'installment_plans=A141', 'installment_plans=A142',\n",
            "       'installment_plans=A143', 'housing=A151', 'housing=A152',\n",
            "       'housing=A153', 'skill_level=A171', 'skill_level=A172',\n",
            "       'skill_level=A173', 'skill_level=A174', 'telephone=A191',\n",
            "       'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202',\n",
            "       'credit'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-febe9150-690d-4d78-adf3-a5d159f315b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>credit_amount</th>\n",
              "      <th>investment_as_income_percentage</th>\n",
              "      <th>residence_since</th>\n",
              "      <th>age</th>\n",
              "      <th>number_of_credits</th>\n",
              "      <th>people_liable_for</th>\n",
              "      <th>status=A11</th>\n",
              "      <th>status=A12</th>\n",
              "      <th>status=A13</th>\n",
              "      <th>status=A14</th>\n",
              "      <th>credit_history=A30</th>\n",
              "      <th>credit_history=A31</th>\n",
              "      <th>credit_history=A32</th>\n",
              "      <th>credit_history=A33</th>\n",
              "      <th>credit_history=A34</th>\n",
              "      <th>purpose=A40</th>\n",
              "      <th>purpose=A41</th>\n",
              "      <th>purpose=A410</th>\n",
              "      <th>purpose=A42</th>\n",
              "      <th>purpose=A43</th>\n",
              "      <th>purpose=A44</th>\n",
              "      <th>purpose=A45</th>\n",
              "      <th>purpose=A46</th>\n",
              "      <th>purpose=A48</th>\n",
              "      <th>purpose=A49</th>\n",
              "      <th>savings=A61</th>\n",
              "      <th>savings=A62</th>\n",
              "      <th>savings=A63</th>\n",
              "      <th>savings=A64</th>\n",
              "      <th>savings=A65</th>\n",
              "      <th>employment=A71</th>\n",
              "      <th>employment=A72</th>\n",
              "      <th>employment=A73</th>\n",
              "      <th>employment=A74</th>\n",
              "      <th>employment=A75</th>\n",
              "      <th>other_debtors=A101</th>\n",
              "      <th>other_debtors=A102</th>\n",
              "      <th>other_debtors=A103</th>\n",
              "      <th>property=A121</th>\n",
              "      <th>property=A122</th>\n",
              "      <th>property=A123</th>\n",
              "      <th>property=A124</th>\n",
              "      <th>installment_plans=A141</th>\n",
              "      <th>installment_plans=A142</th>\n",
              "      <th>installment_plans=A143</th>\n",
              "      <th>housing=A151</th>\n",
              "      <th>housing=A152</th>\n",
              "      <th>housing=A153</th>\n",
              "      <th>skill_level=A171</th>\n",
              "      <th>skill_level=A172</th>\n",
              "      <th>skill_level=A173</th>\n",
              "      <th>skill_level=A174</th>\n",
              "      <th>telephone=A191</th>\n",
              "      <th>telephone=A192</th>\n",
              "      <th>foreign_worker=A201</th>\n",
              "      <th>foreign_worker=A202</th>\n",
              "      <th>credit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>822</th>\n",
              "      <td>36.0</td>\n",
              "      <td>2712.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830</th>\n",
              "      <td>24.0</td>\n",
              "      <td>2375.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>703</th>\n",
              "      <td>30.0</td>\n",
              "      <td>2503.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>36.0</td>\n",
              "      <td>3535.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>18.0</td>\n",
              "      <td>6361.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-febe9150-690d-4d78-adf3-a5d159f315b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-febe9150-690d-4d78-adf3-a5d159f315b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-febe9150-690d-4d78-adf3-a5d159f315b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     month  credit_amount  ...  foreign_worker=A202  credit\n",
              "822   36.0         2712.0  ...                  0.0     2.0\n",
              "830   24.0         2375.0  ...                  0.0     1.0\n",
              "703   30.0         2503.0  ...                  0.0     1.0\n",
              "776   36.0         3535.0  ...                  0.0     1.0\n",
              "659   18.0         6361.0  ...                  0.0     1.0\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9G0RdfbQilw"
      },
      "source": [
        "As a reminder of what we did last week, let's calculate some fairness metrics on the training data (hint, use `BinarylabelDatasetMetric`):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_orig_df2 = StandardDataset(train_orig_df, label_name='credit', protected_attribute_names=['age'], \n",
        "                privileged_classes=[[1]], favorable_classes=[1])"
      ],
      "metadata": {
        "id": "hGVfqF3R5iD5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoazzmDQQilx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dabbb18-efb2-4d5d-a2cf-dedd74bcd2b2"
      },
      "source": [
        "train_orig_metrics = BinaryLabelDatasetMetric(\n",
        "     train_orig_df2, \n",
        "    unprivileged_groups,\n",
        "    privileged_groups\n",
        "  )\n",
        "print(\"Original training dataset\")\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % train_orig_metrics.mean_difference())\n",
        "print(\"Disparate impact = %f\" %train_orig_metrics.disparate_impact())"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training dataset\n",
            "Difference in mean outcomes between unprivileged and privileged groups = -0.135481\n",
            "Disparate impact = 0.814721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vXjQtpLQily"
      },
      "source": [
        "## Step 2: Train a classifier to predict credit using the original data\n",
        "\n",
        "We will be training a logistic regression model to predict good/bad credit, then fine-tuning the model over a set of hyperparameters. Then, we'll see how well this basic model does on some fairness metrics. \n",
        "\n",
        "### 2.1 Training and evaluating a logistic regression model \n",
        "First, we need to split our data up into the explantory variables (x) and the outcome variable (y). We will recode the outcome so that the values are 0 (= bad credit) and 1 (= good credit). This is the format that the sklearn logistic regression function expects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq3P6dXAQilz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec4fe7a-f4e8-4c66-d5a0-c82e9000bdc3"
      },
      "source": [
        "x_train = train_orig_df.drop(\"credit\", axis=1)\n",
        "y_train = train_orig_df.credit.replace({2:0}) \n",
        "print(\"Outcomes: \")\n",
        "y_train.value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outcomes: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    426\n",
              "0.0    174\n",
              "Name: credit, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI8BpcTIQil0"
      },
      "source": [
        "Next, we can fit our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHtshyRkQil1"
      },
      "source": [
        "# Set up the logistic regression model with the given hyperparameters\n",
        "initial_lr = LogisticRegression(C=0.5, penalty=\"l1\", solver='liblinear')\n",
        "    \n",
        "# Fit the model using the training data\n",
        "initial_lr = initial_lr.fit(x_train, y_train, sample_weight=None)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QKB7FaGQil2"
      },
      "source": [
        "Now that we have a trained model, we should evaluate it on our validation set. For now, we'll look the AUC as well as accuracy when we use a cutoff of 0.5 (that is, predicted values over 0.5 are interpreted as good credit, and vice versa).\n",
        "\n",
        "Let's write a funciton to do that, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA-5D7WYQil2"
      },
      "source": [
        "def evaluate(model, X, y_true):\n",
        "    '''Calculates the AUC and accuracy for a trained logistic regression model'''\n",
        "    \n",
        "    # Calculate predicted values\n",
        "    y_pred = model.predict_proba(X)\n",
        "    # This returns a tuple for each observation containing the probability of being in each class.\n",
        "    # Since we're doing binary classification, all we need to know is the probability that the outcome = 1 (good credit)\n",
        "    y_pred = [row[1] for row in y_pred] # This pulls the predicted probability that y = 1 for each observation\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true, [pred_prob >= 0.5 for pred_prob in y_pred])\n",
        "    \n",
        "    # Calculate AUC\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    \n",
        "    return accuracy, auc\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv-4T2iiQil3"
      },
      "source": [
        "# Before we call the function, we need to set up the validation data properly, the way we did for the training data.\n",
        "x_val = val_orig_df.drop(\"credit\", axis=1)\n",
        "y_val = val_orig_df.credit.replace({2:0}) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc52k1sgQil4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002e0dfd-0030-4725-ab9c-333b6427201c"
      },
      "source": [
        "accuracy, auc = evaluate(initial_lr, x_val, y_val)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"AUC: \", auc)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.72\n",
            "AUC:  0.7818138931477768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-PyJ6fdQil4"
      },
      "source": [
        "### 2.2 Hyperparameter tuning the logistic regression model \n",
        "\n",
        "For hyperparameter tuning, we want to be able to easily train models with a variety of hyperparameter and determine which one performs the best on the validation data. We can use the functions we wrote above to do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJucCUYUQil5"
      },
      "source": [
        "def tune_logistic_regression(train_df, val_df, penalty_types, C_values, weights=None, verbose=True):\n",
        "    '''Tunes logistic regression models over the hyperparameters penalty type and C\n",
        "       to maximize the AUC'''\n",
        "    # Pre-process the training and validation data\n",
        "    x_train = train_df.drop(\"credit\", axis=1)\n",
        "    y_train = train_df.credit.replace({2:0}) \n",
        "    x_val = val_df.drop(\"credit\", axis=1)\n",
        "    y_val = val_df.credit.replace({2:0}) \n",
        "\n",
        "    # Create empty lists where we will store the results of hyperparameter tuning \n",
        "    parameters = []\n",
        "    models = []\n",
        "    val_aucs = []\n",
        "    \n",
        "    # Loop through the hyperparameters of interest\n",
        "    for penalty in penalty_types:\n",
        "        for C in C_values:\n",
        "            \n",
        "            # Train the logistic regression model with the given hyperparameters\n",
        "            lr = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
        "    \n",
        "            # Fit the model using the training data\n",
        "            lr = lr.fit(x_train, y_train, sample_weight=weights)\n",
        "            \n",
        "            # Get the evalution metrics on the validation set \n",
        "            accuracy, auc  = evaluate(lr, x_val, y_val)\n",
        "            \n",
        "            # Store the results\n",
        "            parameters.append({'penalty': penalty, 'C': C})\n",
        "            models.append(lr)\n",
        "            val_aucs.append(auc)\n",
        "            \n",
        "            # Print the results\n",
        "            if verbose:\n",
        "                print(\"\\nParmeters: \\tpenalty={} \\tC={}\".format(penalty, C))\n",
        "                print(\"Validtion AUC: {}\".format(auc))\n",
        "            \n",
        "    \n",
        "    # Determine the best model -- that is, the one with the AUC\n",
        "    best_model_index = np.argmax(val_aucs)\n",
        "    best_model = models[best_model_index]\n",
        "    \n",
        "    print(\"\\nBest model parameters: \", parameters[best_model_index])\n",
        "    print(\"Best model AUC: \", val_aucs[best_model_index])\n",
        "    \n",
        "    # Return best model\n",
        "    return best_model, parameters, models, val_aucs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZDRZopwQil6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af37e04-e1f9-4b00-de17-0b2191407ed8"
      },
      "source": [
        "best_lr, parameters, models, val_aucs = tune_logistic_regression(train_orig_df, val_orig_df, penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parmeters: \tpenalty=l1 \tC=0.001\n",
            "Validtion AUC: 0.4036442976766128\n",
            "\n",
            "Parmeters: \tpenalty=l1 \tC=0.1\n",
            "Validtion AUC: 0.7456067932539214\n",
            "\n",
            "Parmeters: \tpenalty=l1 \tC=1\n",
            "Validtion AUC: 0.7706097417148248\n",
            "\n",
            "Parmeters: \tpenalty=l1 \tC=10\n",
            "Validtion AUC: 0.7441915320202854\n",
            "\n",
            "Parmeters: \tpenalty=l1 \tC=100\n",
            "Validtion AUC: 0.743012147658922\n",
            "\n",
            "Parmeters: \tpenalty=l1 \tC=1000\n",
            "Validtion AUC: 0.741243071116877\n",
            "\n",
            "Parmeters: \tpenalty=l1 \tC=10000\n",
            "Validtion AUC: 0.7411251326807408\n",
            "\n",
            "Parmeters: \tpenalty=l1 \tC=100000\n",
            "Validtion AUC: 0.7413610095530133\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=0.001\n",
            "Validtion AUC: 0.6545583205566695\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=0.1\n",
            "Validtion AUC: 0.7965561976648191\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=1\n",
            "Validtion AUC: 0.7690765420450525\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=10\n",
            "Validtion AUC: 0.7648307583441443\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=100\n",
            "Validtion AUC: 0.7653025120886896\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=1000\n",
            "Validtion AUC: 0.7668357117584621\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=10000\n",
            "Validtion AUC: 0.7648307583441444\n",
            "\n",
            "Parmeters: \tpenalty=l2 \tC=100000\n",
            "Validtion AUC: 0.7676612808114164\n",
            "\n",
            "Best model parameters:  {'penalty': 'l2', 'C': 0.1}\n",
            "Best model AUC:  0.7965561976648191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j05GktrQil7"
      },
      "source": [
        "Let's plot the results so that we understand what hyperparameter tuning actually did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz0veGCfQil7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6b042885-3bb5-4410-8ce4-6b4f7a41a172"
      },
      "source": [
        "val_aucs_l1 = [val_aucs[i] for i in range(len(val_aucs)) if parameters[i]['penalty']==\"l1\"]\n",
        "val_aucs_l2 = [val_aucs[i] for i in range(len(val_aucs)) if parameters[i]['penalty']==\"l2\"]\n",
        "C_values = [parameters[i]['C'] for i in range(len(parameters)) if parameters[i]['penalty']==\"l2\"]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.semilogx( C_values, val_aucs_l1, marker='.', markerfacecolor='blue', markersize=12, color='blue', linewidth=4, label='L1 Penalty')\n",
        "ax.semilogx( C_values, val_aucs_l2, marker='.', markerfacecolor='red', markersize=12, color='red', linewidth=4, label='L2 Penalty')\n",
        "ax.set_xlabel(\"C\")\n",
        "ax.set_ylabel(\"AUC\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8deHQFgFQVGrYVNxgV4EyUWtWkVcqFaoiFZEq0VFLSCot7d6vXbRUm0frdcNF6yKRRQVq0XrUm0F+6utJgguQFUEgVC0bIIKQkI+vz++EzMzmSwTcnImyfv5eJwHc75zzswnh2Q+8z3fzdwdERGRdK3iDkBERHKTEoSIiGSkBCEiIhkpQYiISEZKECIikpEShIiIZNQ67gAayp577um9e/eOOwwRkSZlwYIF6929e6bnmk2C6N27N8XFxXGHISLSpJjZyuqe0y0mERHJSAlCREQyijRBmNlwM3vPzJaZ2TUZnu9pZq+Y2UIze9vMTk167trEee+Z2SlRxikiIlVF1gZhZnnANOAkoAQoMrO57r4k6bD/BR5397vNrB/wHNA78fgcoD+wL/CymR3k7jujildERFJFWYMYAixz9+XuvgOYDYxMO8aBzonHXYB/JR6PBGa7+3Z3XwEsS7yexG3WLOjdG1q1Cv/OmhV3RCISkSh7Me0HrE7aLwGOSDvmp8CfzGwS0BE4Mencf6Sdu180YUqdzZoFl1wC27aF/ZUrYfz48Hjs2PjiEpFIxN1IPQaY4e4FwKnATDOrc0xmNt7Mis2seN26dZEFKQlXX12ZHCps3QpTpsCXX8YTk4hEJsoEsQbokbRfkChLdhHwOIC7/x1oB+xZx3Nx9+nuXujuhd27ZxznIQ3lpZfgk08yP7d+PeyzT6hN/PWvUF7euLGJSCSiTBBFQF8z62Nm+YRG57lpx6wChgGY2aGEBLEucdw5ZtbWzPoAfYE3IoxVavL738O3v13zMZs3w333wTe/CQccANdfD++/3zjxiUgkIksQ7l4GTAReBJYSeistNrMbzGxE4rCrgUvM7C3gUeBCDxYTahZLgBeACerBFJOHHoKzzoIdO+p+zkcfwc9/DgcfDEccAXfeGWoZItKkWHNZcrSwsNA11UYDu/12mDy5annXrvDpp7D33nDYYfDmm1BbG1Dr1vCtb8H558Ppp0O7dtHELCJZMbMF7l6Y6bm4G6klF7nDjTdWTQ6tWsGMGbBxY2hnWLsWXngB1qyBZ5+F7363+g/+sjJ45hk4++zQXnHJJfDqq2qvEMlhqkFIKnf4r/+CW25JLc/Ph0cfhVGjaj5/82aYMwdmzoT582t/v9694bzzQs3ioIPqHbaI1E9NNQglCKm0cydceincf39qeYcO8PTTcNJJ2b3eypVh7MTMmfDPf9Z+/JAhIVF897ugXmkijUIJQmq3Y0f4Jv/EE6nlXbrAc8/BN75R/9d2hwULQqJ49FG1V4jkECUIqdnWrTB6NDz/fGr5XnvBiy/CwIEN916lpfCnP4Vk8Yc/1D7ArkuX0Ivq/PPhmGNCO4iINBglCKne5s3hW/pf/5pa3qMHvPxytO0CmzfDk0+GZDFvXu3H9+pV2V5x8MHRxSXSgqgXk2S2bh2ccELV5NC3L/y//xd9o3GXLjBuHLzySmiv+MUv4NBDqz9+5UqYOhUOOSS0V9xxx1e3qzSHoEjDU4JoqUpKwqjnN99MLT/ssJAwevZs3Hh69oRrr4XFi6G4OHSx3Wuv6o8vKoIrroB996Xk8NP59HuTmL+yJ2Xeinkre/Py92cpSYjsIt1iaomWLYMTTwzfyJMddRT88Y9hIFwuKC0Nc0DNnBl6UWUxIWAprXm5zbc48X+Pos0enUNtpXPnsKU/zs+P8IeQrMyaBdddB6tWhS8NU6dqpuCazJoF//M/sHo17Lcf3Hxz1tdLbRBS6Z134OST4eOPU8tPPBGeego6dYonrtps2YLPeZLP7p5Jp+J5tKIBf2/btk1NHDUlk5oet84we36ufuDVNa6dO0Oirth27Ejdb8iyd94J7V5lZZXv37o1nHYaDB4c/p/atg0JveJxpv3ayvLyorteFdxh+/bQAWTbttR/M5XV9Fx1x2/aFB4n69ABpk/P6ndMCUKC118P3Uc3bUotP+OM0P20bdt44qqBe7jj9MQTYfzdihVQwGrGMovzmUl/ltT+Io2lffvUJPPFF/Dee+FDtkJeXugy3KdPGEXunvnfupbV57kNG8Lo9+S/fbPw5aB169QP72by+ZAiLy+7xPLxx6Gbdvr/4yGHhP/n6j7A47p2vXqF+dDqSAlCQkPw6aeHD61k3/teGBiX6dtvTNzhjTcqk0L6nbCkIxnEQuZxPJ35rDFDFMldZllNYVNTgsidTwWJzty5YQ6k7dtTyydNgltvzYmxBeXloYJTkRRWr679nPx8o+CUw3lrv7s58oHxtNlRWd0utTY8l/8d/rm9D53ZQhc205ktKY8r/m2NJgqWZqQBO5goQTR3s2bBBRekVo8hrNfws5+FbxsxKS+Hv/89JIUnnwwdq2rTti0MHx7Gzp1+eribA2PhGFLuEbeZOpVvnzOWzq/CI4+EpPPpp5le0WnPtq+SxZGHbmHEcZsZOngL3fI2w5YtYduc4XFy2ZYtzfN2jBm0aVO55een7mcqq8sxmcrefjv8MpSWVr5/mzYwYkS4nbN9e9h27Kh8nG1Z+pekKLVpE9oEOnQItx8z/bsrz/3xj3DVVamrPHboENpHGohuMTVnd98NEyZU/eD69a/D8qEx2LkTXnutMin861+1n9OuHZx6ahjs/e1vw267Zf++27eHQeGPPhoGcKevnJrODI4/HsaMgTPPhG7danmD8vJw+y45cTz1VKihJa+lkZ8fbusdfXR4k1atwlbxuDHKnn8ebrghtVdY+/bh9+K73039AK9Pg+6uiLpR3z00gtclkVSU//nPoeE3+f+xbduw1O5pp2X+MG/fvnFu2zbA9arpFhPu3iy2wYMHuyS56Sb38OdQuZm5T5/e6KGUlbnPm+c+YYL7PvtUDSvT1r69++jR7rNnu3/2WcPG89ln7rNmuZ92mnvr1rXH0qaN++mnuz/6qPvnn2f5Zg8/7N6rV7j2vXqF/VyQq3HlqmZ8vYBir+ZzVTWI5sY9DDj75S9Ty1u3hocfDt8QG0FZWVjuYc6csGJpdctZJ+vQIXwhO+usUGPo2DH6ODdsCDWZRx4J8db259ChA4wcCeeeG3oLawiFNHWx9WIys+HAbUAe8Ft3vznt+f8DhiZ2OwB7ufvuied2Au8knlvl7iOogRIE4TbHhAlwzz2p5e3ahU/BU0+N9O3LysKUShVJobZJWyEkgdNPD7ePvvWt8AEcl5ISeOyxcBtqwYLaj+/WLcQ9ZkwYlJ4Dbf0iWYslQZhZHvA+cBJQAhQBY9w9Y8d1M5sEDHL3cYn9z929zqO2WnyCKC2FCy8MX4WT7bZbWO3tm9+M7G1feSUkhaeeqtvS0506hXbH0aNDg3P79pGEtkvefz8kikcfDUMZarPvvnDOOSFZDB4ca9u/SFbiShBHAT9191MS+9cCuPtN1Rz/GvATd38psa8EUVdffhm6sT7zTGr5HnuEltnBgxvsrZJH9ld8208fWpFJ586VSeGUU5rOEg/usHBhZbJYs6b2c/r2DYlizJjQ+UYkl8XSSA2MJtxWqtg/H7izmmN7AWuBvKSyMqAY+AfwnWrOG584prhnz54N0mDT5GzZ4j50aNWW1X33dV+8uEHf6uGH3fPz69bIDO5durh/73vuzzzj/uWXDRpKLHbudJ8/3/2yy9z32KNu16BXL/fddw9tm1/7mvvNN7svXer+wQfuK1a4r17tvnat+7p17ps2hQb0bdvcS0vdy8uj+1macZurZIk4GqnNbDQw3N0vTuyfDxzh7hMzHPsjoMDdJyWV7efua8xsf+AvwDB3/7C692uRNYiNG8ON+zfeSC3ff/8wp02fPg36dvvuC2vX1nzM7rvDd74TGpqHDcvJ2TsaRMU8go88EuYRrEstqj7y8kL/gobc1qwJk+EmD41p3Trc7hs4MDS8N/TWunXdbrs19amrmmJcOX+LycwWAhPc/bVqXmsG8Ky7z6nu/Vpcgli7NnSjeffd1PL+/cMn19e+1uBvWdMf+LhxISmccELL69mzdWu4u/foo2F11uRxXlKptiSyZUuYQih5lohWraBfv/DlBCp/B5N/F6MuW7Uq3GZMn4pp4EAoKAj76XXH9LK6HJNt2bp1YW6y5OtVj7n6YrvF1BpYDvQB8oG3gP4ZjjsE+IhEskqUdQXaJh7vCXwA9Kvp/VrUOIjly93337/q/Yz//E/39esjecu1a6u/jdJS7+5lsnGj+29/6z5sWN1vxWnT1pBbr17Z/c5Swy2myIb6uXuZmU0EXiR0c33A3Reb2Q2JgOYmDj0HmJ0ItMKhwL1mVk5Y1Ohmr6b3U4uzdCmcdFLV1tLjjw9zLtVnmHEd3H135vIOHcJCcBJ07QoXXRS2goLMjdqtW4cJN8vKat7SZ0cRqYtVqxrutTRQrilZsCB0AdqwIbX89NNDB/6I+otu2xY+0NLHNfTqlTv3YnPRrFkwfnzqlP3Z3AJwD0mitkSS7TZvHtx3X+rMEW3awKhR4Q7ljh0Nu1XMHi6NI8vZvjWba7Pw6qthIqLP0qa1HjMGHnoo/IVHZNas1OTQuXMYVBZRZaXZqEgC9W1ENKtsWG5Io0eHxQMbs9HVvXKJieq2uXNDbTR5Pr2KKY+GDk19rUz/ZvtcXY9/9dUw9jR9Sq3LL4fjjktts6jY0vejKHvxxTBhQvKUWg08Vx8Z7zs1xa1Zt0E895x7u3ZVbzZedlmY6ChC5eXu/funvu3VV0f6ltKC5Wr32+YcF5qLqQl7/PHw1S55GUaAH/0Ibrop8iG7f/pTuKtVoVUrWL48VGNFpOmr6RaTZo/JZb/9bZi/IT053HRTWJy8EeZz+L//S90/80wlB5GWQm0QueqWWzKv2TBtGvzgB40SwpIl8MILqWVXXdUoby0iOUA1iFzjDj/+cdXkkJcHM2c2WnKAsNZNsiOPDJuItAyqQeSS8vLQZeOOO1LL27YNbREjapzxvEGtXx/yUbIrr2y0txeRHKAEkSvKyuDii0OX1WQdO4Y1MocNa9Rw7rkntftcz56hn7yItBxKELlg+/YwnuGpp1LLu3YN6wcfcUSjhzNtWmrZFVc0zhK7IpI79Ccfty++gDPOCBPsJdt771D2H//R6CHNng0ff1y536lTqNyISMuiBBGnTz8NizC/ljaJba9eYbruAw9s9JDcq3ZtHTcOunRp9FBEJGZKEHH55JMwAu2tt1LLDzkk1Bwq5hFuZPPmpYZkFm4viUjLo26ucVi1KqwRnZ4cBg0KE7/ElBygau1h5Eg44IB4YhGReClBNLb334djjgn/JjvmGHjlFejePZ64CCE9+2xqmQbGibRcShCNadEiOPZYWL06tXz48DA1Y8w3+m+7LXU2y8GDQ94SkZZJCaKxvPZaWNTn3/9OLT/rrDDOoUOHWMKqsHEjzJiRWnbllY0y3ZOI5KhIE4SZDTez98xsmZldk+H5/zOzRYntfTP7NOm5C8zsg8R2QZRxRu6ll8IqcJs3p5aPGxcWMs6BRZzvuy91YZt99w25S0Rarsh6MZlZHjANOAkoAYrMbK4nLR3q7lcmHT8JGJR43A34CVAIOLAgce6mqOKNzFNPhRlZk1cbgfD1/De/yYmv6KWlVWf3mDQpJ/KWiMQoyhrEEGCZuy939x3AbGBkDcePAR5NPD4FeMndNyaSwkvA8AhjjcZDD4Xlu9KTw89+ljPJAeCJJ1LXTu7QISyVKSItW5QJYj8guTW2JFFWhZn1AvoAf8nmXDMbb2bFZla8Ln3B5LjdcQdceGGYgC/ZrbeG2VpzJDlkGhh3wQXQrVs88YhI7siVRupzgDnuvjObk9x9ursXunth9xi7h6Zwh5//vOroslat4IEHYPLkeOKqxt/+BukL8eVYiCISkygTxBqgR9J+QaIsk3OovL2U7bm5wx1++EO4/vrU8jZtwnTd3/9+PHHVIL32cNppcPDB8cQiIrklygRRBPQ1sz5mlk9IAnPTDzKzQ4CuwN+Til8ETjazrmbWFTg5UZa7du4MN+5/85vU8vbt4ZlnwlqdOWb5cnj66dQyDYwTkQqR9WJy9zIzm0j4YM8DHnD3xWZ2A1Ds7hXJ4hxgtnvlEC1332hmNxKSDMAN7r4xqlh32Y4dcP75oZaQrHNneO45OProeOKqxe23pzaRDBgAQ4fGF4+I5Bbz5KGzTVhhYaEXp99Mbwxbt4aeSs8/n1revXsYHT1oUOPHVAebN4cpnz7/vLLswQdDu7qItBxmtsDdCzM9p9lcd8WWLXD66WGCvWQFBWG67hy+mX///anJYe+9w5pFIiIVlCDqa/36MIfSggWp5QceGJJDr17xxFUHZWXh9lKyCRPC0tciIhVypZtr07JmTZiuOz05DBgAf/1rTicHCIO7V66s3G/bFi67LL54RCQ3KUFk68MPwxSnS5emlh95ZFhtZ599YgkrG+ldW88/P9ZZxkUkRylBZOPdd0Ny+Oij1PJhw8KEfF27xhJWNl5/Hf7+99SyKVPiiUVEcpsSRF298Ua4rfTxx6nlI0eGVXY6dYonriyl1x5OOQX6948nFhHJbUoQdfHKK6GWsCltMtnzz4c5c6Bdu3jiytKqVSHcZFdemflYEREliNo88wx861upfUIhdPuZMQNaN52OYHfcEQZ8V+jXD04+Ob54RCS3KUHU5JFH4IwzYPv21PLrrguftq2azuX7/POwKFCyKVNyZlJZEclBTecTrrHdcw+cd17qV26AX/0qzNbaxD5ZH3wwdUG7PfcMP56ISHWUIDL55S/h8svD7KwVzODee8NsrU3Mzp1w222pZZdfHuYRFBGpTtO5gd4Y3OF//gduvjm1vHVrmDkzLB3aBD3zTBi+USE/H37wg/jiEZGmQQmiQnk5TJwId9+dWt6uXej6c9pp8cTVANK7to4Z0yTG84lIzJQgAEpLw2I+s2allnfqFMY4HHdcPHE1gDffrDqXoLq2ikhdqA1ixgzo0qVqcujWDf7ylyadHKBq7eGEE+Cww+KJRUSalpZdg7j//rAKXPKqORASxquvNvkhxmvWwOzZqWWqPYhIXUVagzCz4Wb2npktM7NrqjnmbDNbYmaLzeyRpPKdZrYosVVZqnSXbd8eWmrTkwNAx45NPjkATJsWpvaucNBBcOqp8cUjIk1LZDUIM8sDpgEnASVAkZnNdfclScf0Ba4Fjnb3TWa2V9JLbHP3gVHFR9u2YanQTNaujextG8vWraFXbrLJk5vU2D4RiVmUHxdDgGXuvtzddwCzgZFpx1wCTHP3TQDu/u8I46mqZ8/sypuQ3/0ONiat4t21K1xwQXzxiEjTE2WC2A9YnbRfkihLdhBwkJn9zcz+YWbDk55rZ2bFifLvZHoDMxufOKZ43bp12Uf4i19Ahw6pZR06wNSp2b9WDikvh1tvTS279NJw50xEpK7ivuHQGugLHA+MAe4zs90Tz/VKLKR9LnCrmR2QfrK7T3f3Qncv7F6fFW/GjoXp08MKcGbh3+nTQ3kT9vzz8N57lfutW4chHiIi2YiyF9MaoEfSfkGiLFkJ8Lq7lwIrzOx9QsIocvc1AO6+3MzmAYOAD2loY8c2+YSQLr1r69lnw37pdTcRkVpEWYMoAvqaWR8zywfOAdJ7Iz1NqD1gZnsSbjktN7OuZtY2qfxoYAlSq7ffhj//ObVMXVtFpD4iq0G4e5mZTQReBPKAB9x9sZndABS7+9zEcyeb2RJgJ/BDd99gZt8A7jWzckISuzm595NUL73t4dhjobAwnlhEpGkzT56xtAkrLCz04uLiuMOI1SefhA5Yyb13f//7sKSFiEgmZrYg0d5bRdyN1NKA7rorNTnsvz+MGBFfPCLStClBNBNffll1ItorroC8vHjiEZGmTwmimZg1C5KHgnTuDOPGxRePiDR9ShDNgHvVrq2XXAK77RZPPCLSPChBNAMvvQSLF1fut2oFkybFF4+INA9KEM1Aeu3hzDPDoHARkV2hBNHELV0KL7yQWqaBcSLSEJQgmrj0gXFHHglHHRVPLCLSvChBNGHr14dpvZOp9iAiDUUJogm7554w/qFCz54walR88YhI86IE0URt3x6WFE02aVKY2ltEpCFUmyDM7BQzG52hfLSZnRRtWFKbxx6Djz+u3O/UCS6+OL54RKT5qakG8WNgfobyecANkUQjdZJpYNy4cbD77pmPFxGpj5oSRFt3r7KOp7uvB7R4ZYzmzYNFiyr3zcK8SyIiDammBNHZzKrc0TazNkD76EKS2qTXHkaOhAOqLMgqIrJrakoQvyesEf1VbcHMOgH3JJ6TGHzwATz7bGqZuraKSBRqShD/C3wCrDSzBWb2JrACWJd4rlZmNtzM3jOzZWZ2TTXHnG1mS8xssZk9klR+gZl9kNguqPuP1Lzddltog6gweHBYNU5EpKFV2ynS3cuAa8zsZ8CBieJl7r6tLi9sZnnANOAkoAQoMrO5yUuHmllf4FrgaHffZGZ7Jcq7AT8BCgEHFiTO3ZT1T9iMbNoEDz6YWnbllaENQkSkoVWbIMwsfciVA7ub2SJ3/6wOrz2EkFCWJ15vNjASSF5b+hJgWsUHv7v/O1F+CvCSu29MnPsSMBx4tA7v22xNnw5bt1bu77svnHVWfPGISPNW07Cq0zOUdQMGmNlF7v6XWl57P2B10n4JcETaMQcBmNnfgDzgp+7+QjXn7pf+BmY2HhgP0LNnz1rCadpKS+GOO1LLJk6E/Px44hGR5q+mW0zfz1RuZr2Ax6n6YV/f9+8LHA8UAK+a2X/U9WR3nw5MBygsLPRaDm/S5syBNWsq9zt0gEsvjS8eEWn+sp5qw91XAm3qcOgaoEfSfkGiLFkJMNfdS919BfA+IWHU5dwWwx1uuSW17IILoFu3eOIRkZYh6wRhZocA2+twaBHQ18z6mFk+cA4wN+2Ypwm1B8xsT8Itp+XAi8DJZtbVzLoCJyfKWqS//Q2Ki1PLJk+OJxYRaTlqaqR+htAwnawb8DXgvNpe2N3LzGwi4YM9D3jA3Reb2Q1AsbvPpTIRLAF2Aj909w2J97+RkGQAbqhosG6J0gfGnXYaHHxwPLGISMth7plv3ZvZcWlFDmwkJInvuvuEiGPLSmFhoRenf81uBlasgAMPhPLyyrI//xlOOCG+mESk+TCzBe5emOm5mhqpv5qoz8wGAecCZxEGyz3Z0EFKZrffnpocBgyAoUPji0dEWo6abjEdBIxJbOuBxwg1Dn08NZLNm+H++1PLNDBORBpLTeMg/gn8Ffi2uy8DMDPN+tOI7r8fPksakrj33jBmTHzxiEjLUlMvplHAWuAVM7vPzIYB+u7aSMrKwu2lZD/4AbRtG088ItLyVJsg3P1pdz8HOAR4BZgC7GVmd5vZyY0VYEv19NOwcmXlftu2cPnl8cUjIi1PreMg3P0Ld3/E3U8nDFhbCPwo8shauPSureefD927xxOLiLRMWQ2Uc/dN7j7d3YdFFZDA66/Da6+llk2ZEk8sItJyZT2SWqKXXns4+WTo3z+eWESk5VKCyDGrVoWJ+ZJddVU8sYhIy6YEkWPuvBN27qzc79cv1CBERBqbEkQO+fzzsChQsilTNDBOROKhBJFDHnwwjJ6usOeecF6t0yKKiERDCSJH7NwJt92WWnbZZdC+fTzxiIgoQeSIZ5+FDz+s3M/Phwk5NV+uiLQ0ShA5Ir1r65gxsM8+8cQiIgJKEDnhzTdh/vzUsis1LaKIxCzSBGFmw83sPTNbZmbXZHj+QjNbZ2aLEtvFSc/tTCpPX6q0WUmvPQwdCocdFk8sIiIVaprue5eYWR4wDTgJKAGKzGyuuy9JO/Qxd5+Y4SW2ufvAqOLLFf/6F8yenVqmgXEikguirEEMAZa5+3J33wHMBkZG+H5N0rRpYWrvCgcdBKeeGl88IiIVokwQ+wGrk/ZLEmXpzjSzt81sjpn1SCpvZ2bFZvYPM/tOpjcws/GJY4rXrVvXgKE3jq1b4Z57UssmT4ZWahkSkRwQ90fRM0Bvdx8AvAQ8lPRcr8RC2ucCt5rZAeknJ2aWLXT3wu5NcC7s3/0ONm6s3O/aFS64IL54RESSRZkg1gDJNYKCRNlX3H2Du29P7P4WGJz03JrEv8uBecCgCGNtdOXlcOutqWWXXgodO8YTj4hIuigTRBHQ18z6mFk+cA6Q0hvJzL6WtDsCWJoo72pmbROP9wSOBtIbt5u0F16A996r3G/dGiZmaqoXEYlJZL2Y3L3MzCYCLwJ5wAPuvtjMbgCK3X0ucIWZjQDKgI3AhYnTDwXuNbNyQhK7OUPvpyYtvWvr2WfDfplaaEREYmLuHncMDaKwsNCLi4vjDqNO3nkHBgxILSsqgsLCeOIRkZbLzBYk2nuriLuRukVKrz0cc4ySg4jkHiWIRvbJJzBrVmqZBsaJSC5Sgmhkd98NO3ZU7u+/P4wYEV88IiLVUYJoRF9+CXfdlVp2xRWQlxdPPCIiNVGCaESzZkHygO/OnWHcuPjiERGpiRJEI3Gv2jh9ySWw227xxCMiUhsliEby8suweHHlfqtWMGlSfPGIiNRGCaKRpNcezjwTevWKJxYRkbpQgmgES5fC88+nlmnFOBHJdUoQjSB9Ur4jjoCjjoonFhGRulKCiNj69WFa72QaGCciTYESRMTuvTeMf6jQsyeMGhVfPCIidaUEEaHt2+HOO1PLJk0KU3uLiOQ6JYgIPfYYfPxx5X7HjnDxxfHFIyKSDSWIiGQaGHfRRbD77vHEIyKSLSWIiMyfD4sWVe6bhXmXRESaikgThJkNN7P3zGyZmV2T4fkLzWydmS1KbBcnPXeBmX2Q2C6IMs4opNceRo6EAw6IJxYRkfqIrLnUzPKAacBJQAlQZGZzMywd+pi7T0w7txvwE6AQcGBB4txNUcXbkD74AJ55JrVMA+NEpKmJsgYxBFjm7svdfQcwGxhZx3NPAV5y942JpPASMDyiOBvcbbeFNogKgwfDsXFo8B4AAA/tSURBVMfGF4+ISH1EmSD2A1Yn7ZckytKdaWZvm9kcM+uR5bk5Z9MmePDB1LIrrwxtECIiTUncjdTPAL3dfQChlvBQNieb2XgzKzaz4nXJCy3E6L77YOvWyv1994WzzoovHhGR+ooyQawBeiTtFyTKvuLuG9x9e2L3t8Dgup6bOH+6uxe6e2H37t0bLPD6Ki2FO+5ILZs4EfLz44lHRGRXRJkgioC+ZtbHzPKBc4C5yQeY2deSdkcASxOPXwRONrOuZtYVODlRltPmzIGSksr99u3h0kvji0dEZFdE1ovJ3cvMbCLhgz0PeMDdF5vZDUCxu88FrjCzEUAZsBG4MHHuRjO7kZBkAG5w941RxdoQMg2Mu/BC6NYtlnBERHaZeXJ3myassLDQi4uLY3v/v/0Njjkmteyf/4SDD44nHhGRujCzBe5emOm5uBupm4302sNppyk5iEjTpgTRAFasgKeeSi3TwDgRaeqUIBrA7bdDeXnl/oABcMIJ8cUjItIQlCB20ZYtcP/9qWUaGCcizYESxC66/3747LPK/b33hjFj4otHRKShKEHsgrKycHsp2Q9+AG3bxhOPiEhDUoLYBU8/DR99VLnfti1cdlls4YiINCgliF2Q3rX1/PNhr73iiUVEpKEpQdTTG2/Aa6+llk2ZEk8sIiJRUIKop/Taw8knQ//+8cQiIhIFJYh6WL0anngitUwD40SkuVGCqIc77oCdOyv3+/WDU06JLx4RkSgoQWTp889h+vTUsilTNDBORJofJYgszZgBmzdX7u+5J5x3XmzhiIhERgkiCzt3wm23pZZddllYGEhEpLmJbMGg5ujZZ2HZssr9/HyYMCG+eESam9LSUkpKSvjyyy/jDqXZadeuHQUFBbRp06bO50SaIMxsOHAbYUW537r7zdUcdyYwB/hPdy82s96E5UffSxzyD3ePfYxyetfWMWNgn33iiUWkOSopKWG33Xajd+/emBr2Goy7s2HDBkpKSujTp0+dz4ssQZhZHjANOAkoAYrMbK67L0k7bjdgMvB62kt86O4Do4ovWwsXwvz5qWXq2irSsL788kslhwiYGXvssQfr1q3L6rwo2yCGAMvcfbm77wBmAyMzHHcj8Esgp+uU6bWHoUPhsMPiiUWkOVNyiEZ9rmuUCWI/YHXSfkmi7CtmdjjQw93/mOH8Pma20Mzmm9mxEcZZq3/9C2bPTi1T7UFEmrvYejGZWSvgFuDqDE+vBXq6+yDgKuARM+uc4TXGm1mxmRVnW3XKxrRpUFpaud+3b1hzWkSan06dOlUpe/XVVzn88MNp3bo1c+bMqfbcvLw8Bg4cyNe//nXOOusstm7d2qCxHX/88RQXFwPwi1/8okFfO5MoE8QaoEfSfkGirMJuwNeBeWb2EXAkMNfMCt19u7tvAHD3BcCHwEHpb+Du09290N0Lu3fvHskPsXUr3HNPatmUKdBKHYRFYjdrFvTuHf4ee/cO+1Ho2bMnM2bM4Nxzz63xuPbt27No0SLeffdd8vPzuSf9w6MBNUaCiLIXUxHQ18z6EBLDOcBXV9fdNwN7Vuyb2TzgvxK9mLoDG919p5ntD/QFlkcYa7VmzoSNGyv3u3aFCy6IIxKRlqM+zRArV4ZBq3UZuOqe3Wv37t0bgFZZfDM89thjefvtt/niiy+YNGkS7777LqWlpfz0pz9l5MiRzJgxg7lz57J161Y+/PBDzjjjDH71q18BcPnll1NUVMS2bdsYPXo0P/vZz1Je+5prrmHbtm0MHDiQ/v37c8ABB9CtWzemJKaUvu6669hrr72YPHlydj9omsgShLuXmdlE4EVCN9cH3H2xmd0AFLv73BpO/yZwg5mVAuXAZe6+sYbjI1FeDrfemlo2fjx07NjYkYhIU1JWVsbzzz/P8OHDmTp1KieccAIPPPAAn376KUOGDOHEE08EYNGiRSxcuJC2bdty8MEHM2nSJHr06MHUqVPp1q0bO3fuZNiwYbz99tsMGDDgq9e/+eabufPOO1m0aBEAH330EaNGjWLKlCmUl5cze/Zs3njjjV3+OSIdB+HuzwHPpZX9uJpjj096/CTwZJSx1cULL8A//1m537o1TJwYXzwiktsqvtVDqEFcdNFFfOMb32Du3Ln8+te/BkJX3lWrVgEwbNgwunTpAkC/fv1YuXIlPXr04PHHH2f69OmUlZWxdu1alixZkpIg0vXu3Zs99tiDhQsX8sknnzBo0CD22GOPXf55NJK6BuldW88+GwoK4olFRHJfRRtEMnfnySef5OCDD04pf/3112mbtIB9Xl4eZWVlrFixgl//+tcUFRXRtWtXLrzwwjqNLL/44ouZMWMGH3/8MePGjWuQn0dNrdV45x14+eXUMnVtFWkc7rVvDz8MHTqkntehQyiv7dzGdMopp3DHHXfgiTdeuHBhjcdv2bKFjh070qVLFz755BOef/75jMe1adOG0qTulWeccQYvvPACRUVFnNJA6w8oQVQjve3hmGOgsDCeWESkqrFjw9T7vXqFRu1evcL+2LG79rpbt26loKDgq+2WW26hqKiIgoICnnjiCS699FL6Z7F85PXXX09paSkDBgygf//+XH/99TUef9hhhzFo0CAOOeQQzj33XI4++uiMx40fP54BAwYwNvED5+fnM3ToUM4++2zy8vLq/gPXwLyx02lECgsLvaJ/8K765JPwy7Z9e2XZk0/CqFEN8vIiUo2lS5dy6KGHxh1Gk1ReXs7hhx/OE088Qd++fTMek+n6mtkCd8/49Vc1iAzuvjs1OfTpAyMzTRIiIpIDlixZwoEHHsiwYcOqTQ71oUbqNF9+CXfdlVo2eTI0UI1NRKTB9evXj+XLG36omGoQaR55BJJn7ejcGRqoQ4CISJOiBJHEvWrX1osvht12iyceEZE4KUEkefllePfdyv1WreCKK+KLR0QkTkoQSdJrD2eeGXoziYi0REoQCUuXQvp4FA2ME2l5Mk33fcstt9CvXz8GDBjAsGHDWLlyZcZzNd13M3Xbban7RxwBRx0VTywiUkeNNN/3oEGDKC4u5u2332b06NH893//d8bjmtt030oQwPr18LvfpZZddVU8sYgIYWh0XbbzzgvzfLtXzvddl/OyNHToUDok5vU48sgjKSkpqfWcY489lmXLlvHFF18wbtw4hgwZwqBBg/jDH/4AwIwZMxg1ahTDhw+nb9++KUnn8ssvp7CwkP79+/OTn/ykymsnT/c9duxYfvzjH3Nr0vQP1113Hbelf+utD3dvFtvgwYO9vn7+89SZWnr2dC8trffLiUg9LVmyJDyo23RM9d9q0LFjxxqfnzBhgt944401nltaWuojRozwu+66y6+99lqfOXOmu7tv2rTJ+/bt659//rk/+OCD3qdPH//0009927Zt3rNnT1+1apW7u2/YsMHd3cvKyvy4447zt956y93djzvuOC8qKqoS54oVK3zQoEHu7r5z507ff//9ff369dVf3ySE5Rcyfq62+IFyO3aEJUWTTZoUpvYWEUn28MMPU1xczPz58zM+r+m+m5nJk2Ht2sr9/Pww9kFEJNnLL7/M1KlTmT9/fso03ck03XczMmsW3Htvall5Ofzxj/HEIyIJOTbf98KFC7n00kuZO3cue+21V1bnarrvapjZcDN7z8yWmdk1NRx3ppm5mRUmlV2bOO89M2uYnzbN1VdX/V0pK4Prrovi3USkQUU033em6b5/+MMf8vnnn3PWWWcxcOBARowYUefX03TfmV7YLA94HzgJKAGKgDHuviTtuN2APwL5wER3LzazfsCjwBBgX+Bl4CB331nd+9Vnuu/qOjOYhZqEiDQuTfddf01tuu8hwDJ3X+7uO4DZQKZJs28Efgkk32QbCcx29+3uvgJYlni9BrXnnpnLe/Zs6HcSEYlOVNN9R5kg9gNWJ+2XJMq+YmaHAz3cPf2uf63nJs4fb2bFZla8LnkK1jq69dbMtzCnTs36pUREYlMx3fdvfvObBn3d2BqpzawVcAtwdX1fw92nu3uhuxd279496/OjWrJQROovqtveLV19rmuU3VzXAD2S9gsSZRV2A74OzLPQGLAPMNfMRtTh3AYzdqwSgkiuaNeuHRs2bGCPPfbA6jHiWTJzdzZs2EC7du2yOi/KBFEE9DWzPoQP93OAcyuedPfNwFetAGY2D/ivRCP1NuARM7uF0EjdF3gjwlhFJAcUFBRQUlJCfW4ZS83atWtHQUFBVudEliDcvczMJgIvAnnAA+6+2MxuIAztnlvDuYvN7HFgCVAGTKipB5OINA9t2rShT58+cYchCZF1c21s9enmKiLS0sXVzVVERJowJQgREcmo2dxiMrN1QOZlnpq2PYH1cQfRhOh6ZUfXKzvN8Xr1cveM4wSaTYJorsysuLr7g1KVrld2dL2y09Kul24xiYhIRkoQIiKSkRJE7psedwBNjK5XdnS9stOirpfaIEREJCPVIEREJCMlCBERyUgJQkREMlKCaMLM7Dtmdp+ZPWZmJ8cdT64xs45m9lDiGmlS9zrQ71T2Er9nxWb27bhjaWhKEDExswfM7N9m9m5a+XAze8/MlpnZNTW9hrs/7e6XAJcB340y3lyR5XUbBcxJXKO6rzLfzGRzzVri71S6evxt/gh4vHGjbBxKEPGZAQxPLjCzPGAa8C2gHzDGzPqZ2X+Y2bNp215Jp/5v4ryWYAZ1vG6EhaYqlq5tydPFz6Du16xCS/qdSjeDuv9tnkRYluDfjR1kY4hywSCpgbu/ama904qHAMvcfTmAmc0GRrr7TUCV6quFJbduBp539zejjTg3ZHPdCGuZFwCLaMFfhrK5Zma2lBb2O5Uuy9+xTkBHQtLYZmbPuXt5I4YbKSWI3LIfld94IXzAHVHD8ZOAE4EuZnagu98TZXA5rLrrdjtwp5mdBjwTR2A5rLprpt+pzDJeL3efCGBmFwLrm1NyACWIJs3dbyd8CEoG7v4F8P2442hK9DtVP+4+I+4YotBiq905ag3QI2m/IFEmNdN1y56uWXZa5PVSgsgtRUBfM+tjZvnAOUC1a3fLV3Tdsqdrlp0Web2UIGJiZo8CfwcONrMSM7vI3cuAicCLwFLgcXdfHGecuUbXLXu6ZtnR9aqkyfpERCQj1SBERCQjJQgREclICUJERDJSghARkYyUIEREJCMlCBERyUgJQiRCZraPmc02sw/NbIGZPWdmB8Udl0hdaC4mkYgkZtt9CnjI3c9JlB0G7A28H2dsInWhBCESnaFAafKMqO7+VozxiGRFt5hEovN1YEHcQYjUlxKEiIhkpAQhEp3FwOC4gxCpLyUIkej8BWhrZuMrCsxsgJkdG2NMInWmBCESEQ9TJZ8BnJjo5roYuAn4ON7IROpG032LiEhGqkGIiEhGShAiIpKREoSIiGSkBCEiIhkpQYiISEZKECIikpEShIiIZKQEISIiGf1/JKAy44lrxBEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzab0o-5Qil8"
      },
      "source": [
        "### 2.3 Evaluating bias in our predictions\n",
        "\n",
        "Let's put our data back into a aif360 dataset format, so that we can use all of the fairness metrics provided by the package. For now, we'll evaluate bias on the training data. This mimics the development process we'd use in any real application.\n",
        "\n",
        "First, we'll get predicted values using the best model and attach them as a new column in the data frame. We'll use 0.5 as the threshold as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xhb_wlGQil8"
      },
      "source": [
        "# Copy the dataset\n",
        "train_preds_df = train_orig_df.copy()\n",
        "# Calculate predicted values\n",
        "train_preds_df['credit'] = best_lr.predict(x_train)\n",
        "# Recode the predictions so that they match the format that the dataset was originally provided in \n",
        "# (1 = good credit, 2 = bad credit)\n",
        "train_preds_df['credit'] = train_preds_df.credit.replace({0:2})"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-PK4GJ-Qil9"
      },
      "source": [
        "Then we'll create an object of the aif360 StandardDataset class. You can read more about this in the documentation:\n",
        "https://aif360.readthedocs.io/en/latest/modules/standard_datasets.html#aif360.datasets.StandardDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5rMozURQil-"
      },
      "source": [
        "orig_aif360 = StandardDataset(train_orig_df, label_name='credit', protected_attribute_names=['age'], \n",
        "                privileged_classes=[[1]], favorable_classes=[1])\n",
        "preds_aif360 = StandardDataset(train_preds_df, label_name='credit', protected_attribute_names=['age'], \n",
        "                privileged_classes=[[1]], favorable_classes=[1])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33A2DESA9HFg",
        "outputId": "eba3d0e1-27cf-488e-f26e-2176d6239a46"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "aif360.datasets.standard_dataset.StandardDataset"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclM1irQQil_"
      },
      "source": [
        "Now, let's calculate some fairness metrics!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLv4s4agQimA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57507b28-009e-4079-9d3f-1b340769c103"
      },
      "source": [
        "# write your code here\n",
        "pred_metrics = BinaryLabelDatasetMetric(preds_aif360,\n",
        "                  unprivileged_groups,\n",
        "                  privileged_groups)\n",
        "\n",
        "\n",
        "print(\"mean difference = %f\" %pred_metrics.mean_difference())\n",
        "print(\"disparate impact = %f\" %pred_metrics.disparate_impact())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean difference = -0.221386\n",
            "disparate impact = 0.742480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7df8vbdGQimC"
      },
      "source": [
        "Recall from last week that we identified bias in the training data. We should therefore not find it surprising that we have bias in a model trained on that data.\n",
        "\n",
        "Now, since we have true values and predicted values, let's compare the true positive rate and false positive rate by group. This is similar to the analysis ProPublica did. \n",
        "\n",
        "Note that aif360 is pretty picky about what goes into this ClassificationMetric class, which is the reason for all the inefficient copying of datasets above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcQplgqVQimD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9564f1-bee4-4e9a-934e-8a9e1a8d32dd"
      },
      "source": [
        "orig_vs_preds_metrics = ClassificationMetric(orig_aif360, preds_aif360,\n",
        "                                                   unprivileged_groups=unprivileged_groups,\n",
        "                                                   privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_preds_metrics.error_rate_difference())\n",
        "\n",
        "\n",
        "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_preds_metrics.false_negative_rate(privileged=True))\n",
        "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_preds_metrics.false_negative_rate(privileged=False))\n",
        "print(\"False negative rate ratio = %f\" % orig_vs_preds_metrics.false_negative_rate_ratio())\n",
        "\n",
        "\n",
        "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_preds_metrics.false_positive_rate(privileged=True))\n",
        "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_preds_metrics.false_positive_rate(privileged=False))\n",
        "print(\"False positive rate ratio = %f\" % orig_vs_preds_metrics.false_positive_rate_ratio())\n",
        "\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error rate difference (unprivileged error rate - privileged error rate)= 0.090362\n",
            "\n",
            "False negative rate for privledged groups = 0.054054\n",
            "False negative rate for unprivledged groups = 0.214286\n",
            "False negative rate ratio = 3.964286\n",
            "\n",
            "False positive rate for privledged groups = 0.625000\n",
            "False positive rate for unprivledged groups = 0.421053\n",
            "False positive rate ratio = 0.673684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpZ6MIDRQimE"
      },
      "source": [
        "This confirms it: our model is even *more* biased than the original credit scores.  \n",
        "\n",
        "Let's try to fix that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOv7pZhwQimF"
      },
      "source": [
        "## Step 3: Train a classifier to predict credit using the original data, excluding the sensitive feature\n",
        "\n",
        "We've talked several times in class about how removing a sensitive attribute is not enough. Let's see if that's true in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrCaX_SXQimG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d969bb0c-8fdb-45a7-a952-d96b6e2f32de"
      },
      "source": [
        "best_lr_noage, _, _, _ = tune_logistic_regression(train_orig_df.drop('age', axis=1), val_orig_df.drop('age', axis=1),\n",
        "                                                                       penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000], verbose=False)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best model parameters:  {'penalty': 'l2', 'C': 0.1}\n",
            "Best model AUC:  0.7816959547116406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcN-PxAcQimH"
      },
      "source": [
        "Note that the AUC of our best model is *slightly* worse than before: by excluding a feature, we've lost some predictive power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koTe6gNlQimH"
      },
      "source": [
        "Now let's check the same bias metrics again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BedpLxvSQimH"
      },
      "source": [
        "preds_df_noage = train_orig_df.copy()\n",
        "preds_df_noage['credit'] = best_lr_noage.predict(x_train.drop('age', axis=1))\n",
        "preds_df_noage['credit'] = preds_df_noage.credit.replace({0:2})"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O0TMLAmQimI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c644faa-6167-45e7-fa41-940dae706b84"
      },
      "source": [
        "noage_preds_aif360 = StandardDataset(preds_df_noage, label_name='credit', protected_attribute_names=['age'], \n",
        "                privileged_classes=[[1]], favorable_classes=[1])\n",
        "\n",
        "\n",
        "noage_preds_metrics = BinaryLabelDatasetMetric(noage_preds_aif360, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "print(\"Mean difference = %f\" % noage_preds_metrics.mean_difference())\n",
        "print(\"Disparate Impact = %f\" % noage_preds_metrics.disparate_impact())\n",
        "\n",
        "orig_vs_noage_preds_metrics = ClassificationMetric(orig_aif360, noage_preds_aif360,\n",
        "                                                   unprivileged_groups=unprivileged_groups,\n",
        "                                                   privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_noage_preds_metrics.error_rate_difference())\n",
        "\n",
        "\n",
        "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_noage_preds_metrics.false_negative_rate(privileged=True))\n",
        "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_noage_preds_metrics.false_negative_rate(privileged=False))\n",
        "print(\"False negative rate ratio = %f\" % orig_vs_noage_preds_metrics.false_negative_rate_ratio())\n",
        "\n",
        "\n",
        "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_noage_preds_metrics.false_positive_rate(privileged=True))\n",
        "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_noage_preds_metrics.false_positive_rate(privileged=False))\n",
        "print(\"False positive rate ratio = %f\" % orig_vs_noage_preds_metrics.false_positive_rate_ratio())\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean difference = -0.156337\n",
            "Disparate Impact = 0.815603\n",
            "\n",
            "Error rate difference (unprivileged error rate - privileged error rate)= 0.079724\n",
            "\n",
            "False negative rate for privledged groups = 0.062162\n",
            "False negative rate for unprivledged groups = 0.160714\n",
            "False negative rate ratio = 2.585404\n",
            "\n",
            "False positive rate for privledged groups = 0.602941\n",
            "False positive rate for unprivledged groups = 0.473684\n",
            "False positive rate ratio = 0.785623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx5z3o5BQimI"
      },
      "source": [
        "Scroll up -- how do these numbers for our model that doesn't use age compare to the model that *does* use it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjBBTBfXDufF"
      },
      "source": [
        "The mean difference and disparate impact metrics became more fair for the unprivleged group as compared to the model that did use age.\n",
        "\n",
        "However, our AUC marginally decreased as well from an optimal value ~ .796 to .785"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAw3h8fUQimJ"
      },
      "source": [
        "## Step 4: Preprocess the data using the reweighting algorithm, then train a classifier to predict credit using the re-weighted data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APqjkl5aQimJ"
      },
      "source": [
        "# Fit the weights to our training data\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)\n",
        "RW_fit = RW.fit(train_orig)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziXVHt8uQimK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fe44d3-3110-4267-e4e3-783e77d6252b"
      },
      "source": [
        "# Pull the actual values of the weights for the training data\n",
        "train_reweighed = RW_fit.transform(train_orig)\n",
        "training_weights = train_reweighed.instance_weights\n",
        "training_weights"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.71736842, 1.07897059, 0.97097297, 1.19178571, 0.97097297,\n",
              "       0.97097297, 0.71736842, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 0.97097297, 1.07897059, 0.97097297,\n",
              "       1.19178571, 0.97097297, 0.97097297, 1.07897059, 1.19178571,\n",
              "       0.71736842, 0.97097297, 1.19178571, 1.07897059, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 1.19178571,\n",
              "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 1.19178571,\n",
              "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.19178571, 0.97097297, 1.07897059, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.71736842, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.19178571,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
              "       0.97097297, 1.07897059, 0.71736842, 1.07897059, 0.97097297,\n",
              "       0.71736842, 1.07897059, 0.97097297, 1.07897059, 0.71736842,\n",
              "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 1.19178571,\n",
              "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.71736842, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       1.19178571, 1.19178571, 1.07897059, 0.97097297, 1.07897059,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 0.97097297, 1.07897059, 0.71736842,\n",
              "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
              "       1.07897059, 0.71736842, 1.19178571, 1.07897059, 1.07897059,\n",
              "       1.19178571, 1.07897059, 1.07897059, 1.07897059, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 1.07897059,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.07897059, 1.19178571, 0.97097297,\n",
              "       0.71736842, 1.19178571, 0.71736842, 0.97097297, 1.07897059,\n",
              "       1.07897059, 1.07897059, 0.97097297, 1.07897059, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.71736842,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.71736842, 1.07897059, 0.97097297, 0.97097297,\n",
              "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 1.07897059,\n",
              "       1.07897059, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.71736842, 1.19178571, 0.97097297,\n",
              "       0.97097297, 1.07897059, 1.07897059, 1.19178571, 1.07897059,\n",
              "       0.97097297, 1.07897059, 0.97097297, 1.19178571, 0.97097297,\n",
              "       1.07897059, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.71736842, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 1.07897059,\n",
              "       1.07897059, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.71736842, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.71736842, 0.71736842,\n",
              "       1.07897059, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
              "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.71736842, 0.97097297, 1.19178571, 0.97097297,\n",
              "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 1.19178571,\n",
              "       0.71736842, 0.97097297, 1.07897059, 1.07897059, 1.19178571,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.71736842, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.97097297,\n",
              "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 1.07897059, 1.07897059, 0.97097297,\n",
              "       1.07897059, 0.97097297, 1.07897059, 1.19178571, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.07897059, 1.07897059, 1.19178571,\n",
              "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.19178571, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.71736842,\n",
              "       0.97097297, 0.97097297, 1.19178571, 1.07897059, 0.97097297,\n",
              "       1.07897059, 1.07897059, 0.97097297, 1.07897059, 1.19178571,\n",
              "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.71736842, 1.07897059,\n",
              "       0.71736842, 0.97097297, 0.71736842, 0.97097297, 1.19178571,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.19178571, 1.07897059, 0.97097297, 0.71736842, 0.97097297,\n",
              "       0.97097297, 1.07897059, 1.19178571, 0.97097297, 0.97097297,\n",
              "       1.19178571, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 1.07897059, 0.71736842, 0.71736842,\n",
              "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 0.97097297, 1.19178571, 0.97097297,\n",
              "       1.07897059, 0.71736842, 0.97097297, 0.97097297, 1.07897059,\n",
              "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 1.07897059,\n",
              "       0.71736842, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 1.07897059, 0.97097297, 0.71736842, 0.97097297,\n",
              "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.19178571,\n",
              "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.97097297,\n",
              "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.71736842,\n",
              "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.19178571, 0.97097297, 0.97097297, 1.07897059,\n",
              "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.71736842, 0.71736842,\n",
              "       0.97097297, 1.19178571, 1.19178571, 1.19178571, 1.19178571,\n",
              "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 1.19178571,\n",
              "       1.07897059, 1.19178571, 0.97097297, 1.07897059, 0.97097297,\n",
              "       1.19178571, 0.71736842, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.71736842, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
              "       1.07897059, 0.97097297, 1.19178571, 1.07897059, 0.97097297,\n",
              "       1.19178571, 1.07897059, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 1.07897059, 0.97097297, 1.19178571,\n",
              "       0.97097297, 0.97097297, 0.97097297, 0.71736842, 1.07897059,\n",
              "       1.19178571, 1.07897059, 1.19178571, 0.97097297, 1.07897059,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 1.07897059, 1.07897059, 0.97097297,\n",
              "       1.07897059, 1.07897059, 1.07897059, 0.97097297, 1.07897059,\n",
              "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
              "       0.97097297, 1.07897059, 0.97097297, 1.07897059, 0.97097297])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3pXwJ_AQimK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdfe5d4f-fd1a-4da1-f1f9-db796f7777e5"
      },
      "source": [
        "# Train a model using weights\n",
        "best_lr_weights, _, _, _ = tune_logistic_regression(train_orig_df.drop('age', axis=1), val_orig_df.drop('age', axis=1),\n",
        "                                                                       penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000], \n",
        "                                                                       weights=training_weights, verbose=False)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best model parameters:  {'penalty': 'l2', 'C': 0.1}\n",
            "Best model AUC:  0.775209340724142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvcEwpwPQimL"
      },
      "source": [
        "Hey look-- our AUC fell again. As we've discussed, there's often a tradeoff between fairness and other metrics that stakeholders care about. \n",
        "\n",
        "Let's see if the fairness metrics changed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ej_9ovOQimL"
      },
      "source": [
        "train_preds_df_weights = train_orig_df.copy()\n",
        "train_preds_df_weights['credit'] = best_lr_weights.predict(x_train.drop('age', axis=1))\n",
        "train_preds_df_weights['credit'] = train_preds_df_weights.credit.replace({0:2})"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQi9TyYNQimL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d56853-d84c-4ec6-91d7-77083e1b7c8d"
      },
      "source": [
        "preds_weights_aif360 = StandardDataset(train_preds_df_weights, label_name='credit', protected_attribute_names=['age'], \n",
        "                privileged_classes=[[1]], favorable_classes=[1])\n",
        "preds_weights_metrics = BinaryLabelDatasetMetric(preds_weights_aif360, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "print(\"Mean difference = %f\" % preds_weights_metrics.mean_difference())\n",
        "print(\"Disparate Impact = %f\" % preds_weights_metrics.disparate_impact())\n",
        "\n",
        "orig_vs_preds_weights_metrics = ClassificationMetric(orig_aif360, preds_weights_aif360,\n",
        "                                                   unprivileged_groups=unprivileged_groups,\n",
        "                                                   privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_preds_weights_metrics.error_rate_difference())\n",
        "\n",
        "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_preds_weights_metrics.false_negative_rate(privileged=True))\n",
        "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_preds_weights_metrics.false_negative_rate(privileged=False))\n",
        "print(\"False negative rate ratio = %f\" % orig_vs_preds_weights_metrics.false_negative_rate_ratio())\n",
        "\n",
        "\n",
        "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_preds_weights_metrics.false_positive_rate(privileged=True))\n",
        "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_preds_weights_metrics.false_positive_rate(privileged=False))\n",
        "print(\"False positive rate ratio = %f\" % orig_vs_preds_weights_metrics.false_positive_rate_ratio())\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean difference = -0.079892\n",
            "Disparate Impact = 0.905548\n",
            "\n",
            "Error rate difference (unprivileged error rate - privileged error rate)= 0.067110\n",
            "\n",
            "False negative rate for privledged groups = 0.064865\n",
            "False negative rate for unprivledged groups = 0.089286\n",
            "False negative rate ratio = 1.376488\n",
            "\n",
            "False positive rate for privledged groups = 0.602941\n",
            "False positive rate for unprivledged groups = 0.552632\n",
            "False positive rate ratio = 0.916560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE9Fq1HXQimM"
      },
      "source": [
        "Again, our AUC score (i.e. accuracy) went down marginally. \n",
        "\n",
        "Originally we were at an AUC ~ .796 compared to the reweighted row model's AUC of .775. However, our fairness figures are substantially better than our original models \n",
        "\n",
        "Mean difference was -.22 versus -.07 when comparing the original to the reweighted model, respectively.\n",
        "\n",
        "Disparate Impact was .74 versus .91 when comparing the original to the reweighted model, respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7316kh6-QimM"
      },
      "source": [
        "## Step 5: Post-process the predictions from the model that we trained using weights by using the calibrated equality of odds algorithm \n",
        "\n",
        "The equality of odds algorithm is a method for adjusting predicted probabilities to ensure that the false negative rate is equal for the privilged and unprivilged groups. (This also ensures that the true positive rate is equal.) To do so, the algorithm uses the predicted probabilities and determines *two* threshold probabilitties for each group. Above the upper threshold, all members of the group are assigned to the positive class, and below the lower threshold, all members of the group are assigned to the negative class. But between the two thresholds, individuals are randomly assigned a class. \n",
        "\n",
        "For details, see M. Hardt, E. Price, and N. Srebro, Equality of Opportunity in Supervised Learning, Conference on Neural Information Processing Systems, 2016.\n",
        "\n",
        "Which definitions of fairness does this post-processing algorithm contradict?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual Fairness -- similar people (those scored similarly) should be treated the same.\n"
      ],
      "metadata": {
        "id": "p8UgArMd-OeC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5aLfsvnQimN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28b38d0-5d7c-471e-981a-be81700b504b"
      },
      "source": [
        "# Transform our predictions using the aif360 implementation of the equality of odds algorithm\n",
        "eq_odds = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, seed=47)\n",
        "preds_weights_eq_odds_aif360 = eq_odds.fit_predict(orig_aif360, preds_weights_aif360)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah9hMQguEcmM"
      },
      "source": [
        "Again, calculate fairness metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_jogmsyQimN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fe7610-2819-4802-94e6-b9fc8f8aa36a"
      },
      "source": [
        "# write code to calculate fairness metrics here (see end of step 4, above)\n",
        "preds_weights_eq_odds_metrics = BinaryLabelDatasetMetric(preds_weights_eq_odds_aif360, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "\n",
        "# print the metrics\n",
        "print(\"Mean difference = %f\" % preds_weights_eq_odds_metrics.mean_difference())\n",
        "print(\"Disparate Impact = %f\" % preds_weights_eq_odds_metrics.disparate_impact())\n",
        "\n",
        "orig_vs_preds_weights_eq_odds_metrics = ClassificationMetric(orig_aif360, preds_weights_eq_odds_aif360,\n",
        "                                                   unprivileged_groups=unprivileged_groups,\n",
        "                                                   privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_preds_weights_eq_odds_metrics.error_rate_difference())\n",
        "\n",
        "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_negative_rate(privileged=True))\n",
        "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_negative_rate(privileged=False))\n",
        "print(\"False negative rate ratio = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_negative_rate_ratio())\n",
        "\n",
        "\n",
        "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_positive_rate(privileged=True))\n",
        "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_positive_rate(privileged=False))\n",
        "print(\"False positive rate ratio = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_positive_rate_ratio())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean difference = 0.008662\n",
            "Disparate Impact = 5.382979\n",
            "\n",
            "Error rate difference (unprivileged error rate - privileged error rate)= -0.126819\n",
            "\n",
            "False negative rate for privledged groups = 1.000000\n",
            "False negative rate for unprivledged groups = 1.000000\n",
            "False negative rate ratio = 1.000000\n",
            "\n",
            "False positive rate for privledged groups = 0.007353\n",
            "False positive rate for unprivledged groups = 0.026316\n",
            "False positive rate ratio = 3.578947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTz1Iq2iQimO"
      },
      "source": [
        "What's changed in these metrics? How could the algorithm have caused that? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-8BAK8QimO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27536be8-b246-4292-e2a4-961532bf95b2"
      },
      "source": [
        "# Test how accuracy has changed\n",
        "print(\"\\nAccuracy (on training data) before equality of odds algorithm = %f\" % orig_vs_preds_weights_metrics.accuracy())\n",
        "print(\"\\nAccuracy (on training data) after equality of odds algorithm = %f\" % orig_vs_preds_weights_eq_odds_metrics.accuracy())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy (on training data) before equality of odds algorithm = 0.780000\n",
            "\n",
            "Accuracy (on training data) after equality of odds algorithm = 0.286667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmtPedyUE3Pi"
      },
      "source": [
        "Our Disparate Impact became heavily favorable for the unprivledged group, however, the mean difference is about .008. This is due to the small sample size of the unprivledged group relative to the privledged group. \n",
        "\n",
        "Our accuracy has become substantially worse than before, with a 28% accuracy compared to ~.79. By randomly assigning uncertain outcomes, we lose predictive accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IVwhkfCP_Qq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}