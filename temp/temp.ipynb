{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639ece43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T17:34:58.490931Z",
     "start_time": "2022-04-27T17:34:57.533388Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "import time\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RankingMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a4126b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T17:34:58.521992Z",
     "start_time": "2022-04-27T17:34:58.502798Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#A class used to preprocess data and to return train/val/test splits\n",
    "class DataPreprocessor():\n",
    "    def __init__(self, spark, file_path) -> None:\n",
    "        self.spark = spark                              #Spark Driver\n",
    "        self.file_path = file_path                      #File Path to Read in Data\n",
    "\n",
    "\n",
    "    #Main Method - Call this in partition_data.py to get train/val/test splits returned\n",
    "    def preprocess(self, sanity_checker=False):\n",
    "        \"\"\"\n",
    "        Goal: Save train/val/test splits to netID/scratch - all using self methods\n",
    "        Step 1: self.clean_data: clean the data, format timestamp to date, and remove duplicate movie titles\n",
    "        Step 2: self.create_train_val_test_splits: reformats data, drops nans, and returns train,val and test splits\n",
    "        input:\n",
    "        -----\n",
    "        sanity_checker: boolean - Flag that decides if we call self.sanity_check()\n",
    "        -----\n",
    "        output: \n",
    "        train: RDD of Training Set Data\n",
    "        val: RDD of Validation Set Data\n",
    "        test: RDD of Validation Set Data\n",
    "        \"\"\"\n",
    "        #Format Date Time and Deduplicate Data\n",
    "        clean_data = self.clean_data()                                                  #No args need to be passed, returns RDD of joined data (movies,ratings), without duplicates\n",
    "        #Get Utility Matrix\n",
    "        train, val, test = self.create_train_val_test_splits(clean_data)                #Needs clean_data to run, returns train/val/test splits\n",
    "        \n",
    "        #Check if we should perform sanity check\n",
    "        if sanity_checker:\n",
    "            flag = self.sanity_check(train,val,test)\n",
    "            #If flag == True we're good\n",
    "            if flag:\n",
    "                print(\"The val and test splits are disjoint!\")\n",
    "            #Otherwise raise exception\n",
    "            else:\n",
    "                raise Exception(\"The Validation and Test sets are not disjoint!\")\n",
    "\n",
    "        #Return train val test sets\n",
    "        return train, val, test\n",
    "    \n",
    "    #preprocess calls this function\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        goal: for movie titles with multiple movieIDs, in the movies dataset,\n",
    "        remove the duplicate IDs with the least ratings for each movie. \n",
    "        Additionally, remove those IDs from the ratings dataset, so we get a 1:1 mapping\n",
    "        between movie title and movie ID\n",
    "\n",
    "        inputs: None, however - self.file_path -> this should link to your hfs/netid/\n",
    "        outputs: all_data - a RDD of joined data (movies,reviews) - deduplicated of titles that appear more than once\n",
    "                this loses only 6 records (reviews from users) for small\n",
    "        \"\"\"\n",
    "\n",
    "        #Import the movies data + add to schema so it can be used by SQL + header=True because there's a header\n",
    "        movies = self.spark.read.csv(self.file_path + 'movies.csv', header=True, \\\n",
    "                                    schema='movieId INT, title STRING, genres STRING')\n",
    "    \n",
    "        #Same for ratings - TIMESTAMP MUST BE STRING\n",
    "        ratings = self.spark.read.csv(self.file_path + 'ratings.csv', header=True, \\\n",
    "                    schema='userId INT, movieId INT, rating FLOAT, timestamp STRING') \n",
    "        \n",
    "        #Get the MM-dd-yyyy format for timestamp values producing new column, Date\n",
    "        ratings = ratings.withColumn(\"date\",from_unixtime(col(\"timestamp\"),\"MM-dd-yyyy\"))\n",
    "        ratings = ratings.drop(\"timestamp\") #Drop timestamp, we now have date\n",
    "\n",
    "        #Join Dfs - Join Movies with Ratings on movieId, LEFT JOIN used, select only rating, userId, movieId, title and date\n",
    "        joined = ratings.join(movies, ratings.movieId==movies.movieId, how='left').select(\\\n",
    "                            ratings.rating,ratings.userId,\\\n",
    "                            ratings.movieId,ratings.date,movies.title)\n",
    "\n",
    "        #Find Movie Titles that map to multiple IDs\n",
    "        dupes = joined.groupby(\"title\").agg(countDistinct(\"movieId\").alias(\"countD\")).filter(col(\"countD\")>1)\n",
    "\n",
    "        #Isolate non-dupes into a df\n",
    "        non_dupes = joined.join(dupes, joined.title==dupes.title, how='leftanti')\n",
    "    \n",
    "        #Get all of the dupes data - ratings, userId, ect - again from Joined\n",
    "        dupes = dupes.join(joined, joined.title==dupes.title, how='inner').select(\\\n",
    "                                        joined.movieId,joined.rating,\\\n",
    "                                        joined.date,dupes.title,joined.userId)\n",
    "    \n",
    "        #Clean the dupes accordingly\n",
    "        #Step 1: Aggregate by title/movie Id, then count userId - give alias\n",
    "        #Step 2: Create a window to partition by - we iterate over titles ranking by \n",
    "        #countD (count distinct of userId) - movieId forces a deterministic ranking based off movieId\n",
    "        #Step 3: Filter max_dupes so we only grab top ranking movieIds\n",
    "        windowSpec = Window.partitionBy(\"title\").orderBy(\"countD\",\"movieId\")\n",
    "        max_dupes = dupes.groupBy([\"title\",\"movieId\"]).agg(countDistinct(\"userId\").alias(\"countD\"))\n",
    "        max_dupes = max_dupes.withColumn(\"dense_rank\",dense_rank().over(windowSpec))\n",
    "        max_dupes = max_dupes.filter(max_dupes.dense_rank==\"2\")\n",
    "        max_dupes = max_dupes.drop(\"countD\",\"dense_rank\")\n",
    "        \n",
    "        #Get a list of movie ids ~len(5) for small - which are the ones we want to keep\n",
    "        ids = list(max_dupes.toPandas()['movieId'])\n",
    "        cleaned_dupes = dupes.where(dupes.movieId.isin(ids))\n",
    "        \n",
    "        #Reorder Columns so union works\n",
    "        cleaned_dupes = cleaned_dupes.select('rating', 'userId', 'movieId', 'date', 'title')\n",
    "\n",
    "        \n",
    "        #Get the union of the non_dupes and cleaned_dupes\n",
    "        clean_data = non_dupes.union(cleaned_dupes)\n",
    "\n",
    "        #Subtract 2.5 from each review to create negative reviews\n",
    "        clean_data = clean_data.withColumn(\"rating\",col(\"rating\")-2.5)\n",
    "        \n",
    "        #For testing purposes should be 100,830 for small dataset\n",
    "        #print(f\"The length of the combined and de-deduped joined data-set is: {len(clean_data.collect())}\")\n",
    "\n",
    "        #Repartition for efficiency:\n",
    "        clean_data = clean_data.repartition(10)\n",
    "\n",
    "        #Return clean_data -> Type: Spark RDD Ready for more computation\n",
    "        return clean_data\n",
    "\n",
    "    #Create Train Test Val Splits - .preprocess() calls this function\n",
    "    def create_train_val_test_splits(self, clean_data):\n",
    "        \"\"\"\n",
    "        Procedure: \n",
    "        Create two columns - the first will measure the specific row count for a specific user\n",
    "        the other will be static fixed at the total number of reviews for that user. The row count\n",
    "        is sorted by date ascending, so the first row is the oldest review.\n",
    "        \n",
    "        Then, subset training to be where row_count <= .6 *length, grabbing the oldest 60% of reviews, for\n",
    "        all users.\n",
    "        \n",
    "        We then subset the remaining data into a hold out, with the goal of creating two disjoint validation\n",
    "        and test data sets when looking at userId (meaning they should not have any shared userId values), \n",
    "        but still have roughly the same amount of data, or whatever percentage we want to achieve\n",
    "        \n",
    "        To obtain approximate equality and disjoint userId membership, for the remiaining data\n",
    "        sort userId by user_review_count descending, then alternate values in that list, assigning\n",
    "        half to test and half to validation.\n",
    "        -----\n",
    "        input: RDD created by joining ratings.csv and movies.csv - cleaned of duplicates and formatted accordingly\n",
    "        -----\n",
    "        -----\n",
    "        output: training 60%, val 20%, test 20% splits with colums cast to integer type and na's dropped\n",
    "        -----\n",
    "        \"\"\"\n",
    "        #Type Cast the cols to numeric\n",
    "        ratings = clean_data.withColumn('movieId',col('movieId').cast(IntegerType())).withColumn(\"userId\",col(\"userId\").cast(IntegerType()))\n",
    "        #Drop nulls\n",
    "        ratings = ratings.na.drop(\"any\")\n",
    "    \n",
    "        #strategy, partition by userId, and userId order by date, \n",
    "        #take the first 60% of reviews for all users\n",
    "        w1 = Window.partitionBy(\"userId\")\n",
    "        w2 = Window.partitionBy(\"userId\").orderBy(\"date\")\n",
    "        ratings = (ratings.withColumn(\"row_num\", row_number().over(w2))\n",
    "                       .withColumn('length', count('userId').over(w1))\n",
    "                  )\n",
    "\n",
    "        #store in training RDD by \n",
    "        #selecting all rows where the row_count for that user <= 60% total reviews for that user\n",
    "        \n",
    "        training = ratings.filter(\"row_num <=.6*length\")\n",
    "        #now for validation and test set, we want those to have no users in common, but for them to\n",
    "        #be approximately equal size. \n",
    "        holdout_df = ratings.filter(\"row_num >.6*length\")\n",
    "        \n",
    "        #strategy, of the data not in my train set, group users by number of movies they have seen\n",
    "        #sort descending\n",
    "        holdout_split = holdout_df.groupBy(\"userId\").count().orderBy(\"count\", ascending=False).toPandas()\n",
    "        \n",
    "        #store the list of userIds sorted by descending total movie count\n",
    "        holdout_split = list(holdout_split.userId)\n",
    "        \n",
    "        #partition list of userIds by taking every other index and putting it in the validation set\n",
    "        val_users = holdout_split[::2]\n",
    "        \n",
    "        #create a validation and test set by filtering holdout data based on whether movieId isin val_users\n",
    "        val = holdout_df.filter(holdout_df.userId.isin(val_users))\n",
    "        test = holdout_df.filter(~holdout_df.userId.isin(val_users))\n",
    "\n",
    "        #Repartition for efficiency\n",
    "        training = training.repartition(10)\n",
    "        val = val.repartition(10)\n",
    "        test = test.repartition(10)\n",
    "        #Return train/val/test splits\n",
    "        return training, val, test\n",
    "\n",
    "    #TO DO?? Should we enforce min_review cutoff to make sure no cold-start for any prediction?\n",
    "    def enforce_min_review(self):\n",
    "        pass\n",
    "\n",
    "    #Check to train/val/test splits to make sure approx 60/20/20 split is achieved\n",
    "    def sanity_check(self,train,val,test):\n",
    "        \"\"\"\n",
    "        Method to print out the shape of train/val/test splits, and a check to make sure that\n",
    "        val and test splits are disjoint (no distinct userId appears in both)\n",
    "        input:\n",
    "        -----\n",
    "        train: RDD - Training data split created from .create_train_val_test_splits\n",
    "        val: RDD - Validation data split created from .create_train_val_test_splits\n",
    "        test: RDD - Testing data split created from .create_train_val_test_splits\n",
    "        -----\n",
    "        output:\n",
    "        -----\n",
    "        returnFlag: boolean - True means test and val splits are disjoint on userId\n",
    "        \"\"\"\n",
    "\n",
    "        #Get observatio counts for training, val, and test sets\n",
    "        training_obs = train.count()\n",
    "        val_obs = val.count()\n",
    "        test_obs = test.count()\n",
    "\n",
    "        #Print them out\n",
    "        print(f\"Training Data Len: {training_obs} Val Len: {val_obs}, Test Len: {test_obs}\")\n",
    "        print(f\"Partitions, Train: {train.rdd.getNumPartitions()}, Val: {val.rdd.getNumPartitions()}, Test: {test.rdd.getNumPartitions()}\")\n",
    "        #Check if there are any overlapping_ids in the sets\n",
    "        overllaping_ids = val.join(test, test.userId==val.userId,how='inner').count()\n",
    "        \n",
    "        #Return True if they're disjoint, False if there's overlap\n",
    "        return overllaping_ids == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bb66dfaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:57:16.903398Z",
     "start_time": "2022-04-27T19:57:16.845555Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "class Model():\n",
    "    \"\"\"\n",
    "    Abstract Model class that will contain various methods to deploy collaborative filtering.\n",
    "    Model Parameters that need to be passed thorugh:\n",
    "    ### For ALS Model ###\n",
    "    -----\n",
    "    rank: int - Rank of latent factors used in decomposition\n",
    "    maxIter: int - represents number of iterations to run algorithm\n",
    "    regParam: float - Regularization Parameter\n",
    "    model_save: boolean - Flag to determine if we should save the model progress or not\n",
    "    -----\n",
    "    ### For baseline Model ###\n",
    "    -----\n",
    "    min_ratings: int - Minimum number of reviews to qualify for baseline (Greater Than or Equal to be included)\n",
    "    -----\n",
    "    ### No Input Necessary ###\n",
    "    -----\n",
    "    model_size: str - Either \"large\" or \"small\" used to demarcate which dataset we are running on\n",
    "    model_type: str - Which model type we intent to run, i.e. ALS or baseline\n",
    "    evaluation_data_name: str - Dummy variable used to keep track of which dataset we are making predictions on, either \"Val\" or \"Test\"\n",
    "    time_when_ran: datetime - Time when model was run\n",
    "    time_to_fit: datetime - Time it took to fit the model\n",
    "    time_to_predict: datetime - Time it took to make predictions\n",
    "    metrics: dict - Dictionary used to store the various metrics calculated in self.record_metrics()\n",
    "    -----\n",
    "    ### Misc ###\n",
    "    -----\n",
    "    num_recs: int - Top X number of reccomendations to return - default set to 100\n",
    "    -----\n",
    "    ### Model Methods ###\n",
    "    -----\n",
    "    run_model: Runs the corresponding method that was passed to self.model_type\n",
    "    alternatingLeastSquares: Latent Factor model which uses the Alternating Least Squares Pyspark Class to fit and predict.\n",
    "    baseline: uses a baseline popularity model that returns the top X most popular movies (decided by avg rating per movie)\n",
    "    record_metrics: Calculates metrics for prediction,label pairs\n",
    "    save_model: Used for advanced models like ALS or extensions where we may want to save the model itself\n",
    "    -----\n",
    "    \"\"\"\n",
    "    \n",
    "    #changed default min ratings to 0 from none, otherwise we get an error with None\n",
    "\n",
    "    # Constructor for Model\n",
    "    def __init__(self, model_size=None, model_type=None, rank=None, maxIter=None, regParam=None, seed=10, nonnegative=True,\n",
    "                 model_save=False, num_recs=100, min_ratings=0):\n",
    "        # Model Attributes\n",
    "        # Dictionary to access variable methods\n",
    "        self.methods = {\"als\": self.alternatingLeastSquares,\n",
    "                        \"baseline\": self.baseline}\n",
    "        # Top X number of reccomendations to return - set to 100, probably won't change\n",
    "        self.num_recs = num_recs\n",
    "\n",
    "        # Passed through by user\n",
    "        self.model_size = model_size\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # For ALS\n",
    "        self.rank = rank  # Rank of latent factors used in decomposition\n",
    "        self.maxIter = maxIter  # Number of iterations to run algorithm, recommended 5-20\n",
    "        self.regParam = regParam  # Regularization Parameter\n",
    "        # Flag used to determine whether or not we should save our model somewhere\n",
    "        self.model_save = model_save\n",
    "\n",
    "        # For baseline\n",
    "        # Minimum number of reviews to qualify for baseline (Greater Than or Equal to be included)\n",
    "        self.min_ratings = min_ratings\n",
    "\n",
    "        # Add the attributes we're gonna compute when we fit and predict\n",
    "        self.evaluation_data_name = None\n",
    "        self.time_when_ran = None\n",
    "        self.time_to_fit = None\n",
    "        self.time_to_predict = None\n",
    "        self.metrics = {}\n",
    "\n",
    "    def run_model(self, train, val=None, test=None):\n",
    "        \"\"\"\n",
    "        Run_model is what is called to fit, run, and record the metrics for respective model types.\n",
    "        Function behavior is dependent on the argument passed to self.model_type.\n",
    "        -----\n",
    "        inputs:\n",
    "        -----\n",
    "        train: RDD - Training data set\n",
    "        val: RDD - Validation data set\n",
    "        test: RDD - Test set\n",
    "        -----\n",
    "        outputs:\n",
    "        -----\n",
    "        model_output: Variable Type - Output of whichever model ran -> check self.model_type\n",
    "        -----\n",
    "        \"\"\"\n",
    "        # Get when model was ran\n",
    "        self.time_when_ran = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "        # Identify if we're predicting on the Validation Set or the Test Set\n",
    "        if val:\n",
    "            self.evaluation_data_name = \"Val\"\n",
    "            evaluation_data = val\n",
    "        elif test:\n",
    "            self.evaluation_data_name = \"Test\"\n",
    "            evaluation_data = test\n",
    "\n",
    "        # Grab method for whichever model corresponds to self.model_type\n",
    "        model = self.methods[self.model_type]\n",
    "        # Run model on training / evaluation data\n",
    "        model_output = model(train, evaluation_data)\n",
    "        # Return model output\n",
    "        return model_output\n",
    "\n",
    "    # This method uses the Alternating Least Squares Pyspark Class to fit and run a model\n",
    "    def alternatingLeastSquares(self, training, evaluation_data):\n",
    "        \"\"\"\n",
    "        Builds and fits a PySpark alternatingLeastSquares latent factor model. Calls self.record_metrics(precitions,labels)\n",
    "        to record the results. Some dummy variables are made to record whether or not we are using the validation set\n",
    "        or the testing set. This will help us record our results accurately. Training and predicting are also timed. \n",
    "        -----\n",
    "        Input: \n",
    "        training: RDD - Training data set\n",
    "        evaluation_data: RDD - Either Validation data set, or Training data set\n",
    "        -----\n",
    "        Output: [userRecs, movieRecs] - list containing two lists, each of length == self.numrecs \n",
    "        -----\n",
    "        \"\"\"\n",
    "\n",
    "        # Time the function start to finish\n",
    "        start = time.time()\n",
    "        # Create the model with certain params - coldStartStrategy=\"drop\" means that we'll have no nulls in val / test set\n",
    "        als = ALS(maxIter=self.maxIter, rank=self.rank, regParam=self.regParam,\n",
    "                  nonnegative=False, seed=10, userCol=\"userId\",\n",
    "                  itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "        # Fit the model\n",
    "        model = als.fit(training)\n",
    "        # End time and calculate delta\n",
    "        end = time.time()\n",
    "        self.time_to_fit = end - start\n",
    "\n",
    "        # Time predictions as well\n",
    "        start = time.time()\n",
    "        # Create predictions, matrix with additional column of prediction\n",
    "        predictions = model.transform(evaluation_data)\n",
    "        end = time.time()\n",
    "        self.time_to_predict = end - start\n",
    "        \n",
    "\n",
    "        # Generate top 10 movie recommendations for each user\n",
    "        userRecs = model.recommendForAllUsers(self.num_recs)\n",
    "        # Generate top 10 user recommendations for each movie\n",
    "        movieRecs = model.recommendForAllItems(self.num_recs)\n",
    "        \n",
    "        #Add RMSE and stuff here \n",
    "        \n",
    "        # Use self.record_metrics to evaluate model on RMSE, R^2, Precision at K, Mean Precision, and NDGC\n",
    "        return self.record_metrics(predictions=userRecs, labels=evaluation_data)\n",
    "        \n",
    "        ## FIX THIS IN A BIT ##\n",
    "        ##Evaluate Predictions for Regression Task##\n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "        # Calculate RMSE and r_2 metrics and append to metrics\n",
    "        self.metrics[\"rmse\"] = evaluator.evaluate(\n",
    "            predictions, {evaluator.metricName: \"rmse\"})\n",
    "        self.metrics[\"r2\"] = evaluator.evaluate(\n",
    "            predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "        ##ROC Metric Evaluation##\n",
    "        # For ROC Binary Classification\n",
    "        # Make predictions Binary\n",
    "        binary_predicts = predictions.withColumn(\"prediction\", when(\n",
    "            predictions.rating > 0, 1).otherwise(0).cast(\"double\"))\n",
    "        evaluator = BinaryClassificationEvaluator(\n",
    "            rawPredictionCol='prediction', labelCol='rating', metricName='areaUnderROC')\n",
    "        # Append ROC to our Metrics list\n",
    "        self.metrics[\"ROC\"] = evaluator.evaluate(binary_predicts)\n",
    "\n",
    "        # Save model if we need to\n",
    "        if self.model_save:\n",
    "            self.save_model(model_type=self.model_type, model=als)\n",
    "\n",
    "        # Return top self.num_recs movie recs for each user, top self.num_recs user recs for each movie\n",
    "        return [userRecs, movieRecs]\n",
    "\n",
    "    # Baseline model that returns top X most popular items (highest avg rating)\n",
    "    def baseline(self, training, evaluation_data):\n",
    "        \"\"\"\n",
    "        Baseline model for recommendation system. No personalization, just recommend the Top 100 movies by avg(rating)\n",
    "        A movie must have at least self.min_ratings to be considered\n",
    "        input:\n",
    "        -----\n",
    "        training: RDD - training set data\n",
    "        evaluation_data: RDD - Validation set or Test set data\n",
    "        self.min_ratings: int - how many ratings a movie must have in order to be considered in top 100\n",
    "        -----\n",
    "        output: RDD of Top 100 movieIds by avg(rating)\n",
    "        \"\"\"\n",
    "        # Time model Fit\n",
    "        start = time.time()\n",
    "        # Get Top 100 Most Popular Movies - Avg(rating) becomes prediction\n",
    "        top_100_movies = training.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"prediction\"),\n",
    "                                                         count(\"movieId\").alias(\"movie_count\")).where(f\"movie_count>={self.min_ratings}\")\\\n",
    "            .orderBy(\"prediction\", ascending=False).limit(100)\n",
    "        # Grab Distinct User Ids\n",
    "        ids = evaluation_data.select(\"userId\").distinct()\n",
    "        # Cross Join Distinct userIds with Top 100 Most Popular Movies\n",
    "        predictions = ids.crossJoin(top_100_movies)\n",
    "        # Record end time after RDD operations\n",
    "        end = time.time()\n",
    "        self.time_to_fit = end - start\n",
    "\n",
    "        # Time predictions as well\n",
    "        self.time_to_predict = 0  # Recommends in constant time\n",
    "\n",
    "        # Use self.record_metrics to evaluate model on RMSE, R^2, Precision at K, Mean Precision, and NDGC\n",
    "        self.record_metrics(predictions=predictions, labels=evaluation_data, baseline=True)\n",
    "\n",
    "        # Return The top 100 most popular movies above self.min_ratings threshold\n",
    "        return top_100_movies\n",
    "\n",
    "    def record_metrics(self, predictions, labels, baseline=False):\n",
    "        \"\"\"\n",
    "        Method that will contain all the code to evaluate model on metrics: RMSE, R^2, ROC, Precistion At K, Mean Precision, and NDGC\n",
    "        input:\n",
    "        -----\n",
    "        predictions: RDD - PySpark Dataframe containing the following columns at the minimum: [userId,movieId,prediction] - if not baseline model must include rating column\n",
    "        labels: RDD - PySpark Dataframe containing the following columns at the minimum: [userId,movieId,rating, date]\n",
    "        Baseline - a boolean indicator denoting whether we are using a baseline model or non-baseline model\n",
    "        -----\n",
    "        returns: \n",
    "        None - Writes the results to self.metrics dictionary\n",
    "        \"\"\"\n",
    "        self.review_threshold = 0\n",
    "\n",
    "\n",
    "        \n",
    "        if baseline:\n",
    "           \n",
    "            predictions = predictions.select(\"userId\",\"movieId\")\n",
    "\n",
    "\n",
    "            #jonah and joby's code here\n",
    "            # Join predictions and labels, then filter to the \n",
    "            ##fix this\n",
    "            denominator_p2 = labels.join(predictions, ['userId', 'movieId'], how ='inner').select('userId', 'movieId', \"rating\")\n",
    "\n",
    "            numerator_p2 = denominator_p2.where(f\"rating>{self.review_threshold}\"\\\n",
    "                                               ).groupBy('userId').agg(expr('collect_list(movieId) as movieId'))\n",
    "\n",
    "            denominator_p2 = denominator_p2.groupBy('userId').agg(expr('collect_list(movieId) as movieId'))\n",
    "\n",
    "            ##Evalaute Predictions for Ranking Tests##\n",
    "\n",
    "            predictions = predictions.groupBy('userId').agg(expr('collect_list(movieId) as movieId'))\n",
    "\n",
    "\n",
    "           #Grab ALL (NO FILTERS YET) Validation Data userId, movieIdLists \n",
    "            all_labels = labels.select('userId', 'movieId') \\\n",
    "                .groupBy('userId').agg(expr('collect_list(movieId) as movieId'))\n",
    "\n",
    "            # Only select movies that users have seen in validation data that they rated positively\n",
    "            pos_labels = labels.select('userId', 'movieId', \"rating\").where(f\"rating>{self.review_threshold}\") \\\n",
    "                .groupBy('userId').agg(expr('collect_list(movieId) as movieId'))\n",
    "\n",
    "            #create new denominator for recall at 100 of validation dataset, ordered by rating\n",
    "\n",
    "            \"\"\"\n",
    "            new code\n",
    "            \"\"\"\n",
    "            labels_partition = Window.partitionBy(\"userId\").orderBy(desc(\"rating\"))\n",
    "\n",
    "\n",
    "            top_100_pos_labels = labels.withColumn(\"row\",row_number().over(labels_partition))\\\n",
    "            .filter(col(\"row\") <= self.num_recs).where(f\"rating>{self.review_threshold}\")\\\n",
    "            .groupBy('userId').agg(expr('collect_list(movieId) as movieId'))\n",
    "\n",
    "\n",
    "            #all\n",
    "            labels_all_and_predictions = predictions.join(\n",
    "                all_labels, 'userId').rdd.map(lambda row: (row[1], row[2]))\n",
    "\n",
    "            # Subset of Labels Intersection With Reccomendation\n",
    "            labels_subset_and_predictions = predictions.join(\n",
    "                pos_labels, 'userId').rdd.map(lambda row: (row[1], row[2]))\n",
    "\n",
    "            #new code \n",
    "            labels_subset_and_intersections = numerator_p2.join(\n",
    "                denominator_p2, 'userId').rdd.map(lambda row: (row[2], row[1]))\n",
    "            labels_subset_and_intersections =  denominator_p2.join(\n",
    "                numerator_p2, 'userId').rdd.map(lambda row: (row[1], row[2]))\n",
    "    \n",
    "            \"\"\"\n",
    "            new code\n",
    "            \"\"\"\n",
    "            recall_labels_and_preds = numerator_p2.join(top_100_pos_labels, 'userId').\\\n",
    "                rdd.map(lambda row: (row[1], row[2]))\n",
    "            #Joby and Jonah's code\n",
    "             \n",
    "           #should we change the return statement? Keeping it as is\n",
    "           #return labels_all_and_predictions, predictions\n",
    "            rankingMetrics_all = RankingMetrics(labels_all_and_predictions)\n",
    "            rankingMetrics_subset = RankingMetrics(labels_subset_and_predictions)\n",
    "            \n",
    "\n",
    "            rankingMetrics_intersection = RankingMetrics(labels_subset_and_intersections)\n",
    "            \"\"\"\n",
    "            new code\n",
    "            \"\"\"\n",
    "            recall_rm = RankingMetrics(recall_labels_and_preds)\n",
    "            \n",
    "            self.metrics[\"MAP - All\"] = rankingMetrics_all.meanAveragePrecision\n",
    "            self.metrics[\"MAP - Subset\"] = rankingMetrics_subset.meanAveragePrecision\n",
    "\n",
    "            self.metrics[f\"meanAveragePrecisionAt{self.num_recs}All\"] = rankingMetrics_all.precisionAt(\n",
    "                self.num_recs)\n",
    "            self.metrics[f\"ndcgAt100-All\"] = rankingMetrics_all.ndcgAt(self.num_recs)\n",
    "        \n",
    "            self.metrics[f\"meanAveragePrecisionAt{self.num_recs}subset\"] = rankingMetrics_subset.precisionAt(self.num_recs)\n",
    "            self.metrics[f\"ndcgAt100-subset\"] = rankingMetrics_subset.ndcgAt(self.num_recs)\n",
    "            \n",
    "            self.metrics['MAP - Intersection'] = rankingMetrics_intersection.meanAveragePrecision\n",
    "            \n",
    "            #recallatK() does divide \n",
    "            self.metrics['Precision - Intersection'] = rankingMetrics_intersection.recallAt(self.num_recs)\n",
    "            \"\"\"\n",
    "            new code\n",
    "            \"\"\"\n",
    "            \n",
    "            self.metrics['Recall of liked movies'] = recall_rm.recallAt(self.num_recs)\n",
    "    \n",
    "\n",
    "            \n",
    "        else:\n",
    "            predictions = predictions.select(\"userId\",\"recommendations.movieId\")\n",
    "\n",
    "\n",
    "            \n",
    "    # Method to save model to const.MODEL_SAVE_FILE_PATH\n",
    "    def save_model(self, model_type=None, model=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        -----\n",
    "        model_type: str - string designating what type of model is being saved\n",
    "        model: obj - model object that has .save method\n",
    "        -----\n",
    "        \"\"\"\n",
    "        # Make sure a non-null object was passed\n",
    "        if model and model_type:\n",
    "            model.save(const.MODEL_SAVE_FILE_PATH + model_type)\n",
    "        # Otherwise throw error\n",
    "        else:\n",
    "            raise Exception(\"Model and or Model_type not passed through\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dd4eaa31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:57:19.767366Z",
     "start_time": "2022-04-27T19:57:19.747001Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_path = \"../DS_GA_1004/Capstone/ml-latest-small/\"\n",
    "spark = SparkSession.builder.appName('Spark_Session_Name').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8eab2828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:57:52.134871Z",
     "start_time": "2022-04-27T19:57:23.807312Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, val, test = DataPreprocessor(spark,folder_path).preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a9a35ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:57:54.506721Z",
     "start_time": "2022-04-27T19:57:54.502300Z"
    }
   },
   "outputs": [],
   "source": [
    "m6 = Model(model_type='baseline', min_ratings = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0bf35fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T20:23:57.322633Z",
     "start_time": "2022-04-27T20:22:51.254999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n",
      "|movieId|prediction|movie_count|\n",
      "+-------+----------+-----------+\n",
      "| 136469|       2.5|          1|\n",
      "|   1111|       2.5|          1|\n",
      "|   6442|       2.5|          1|\n",
      "|    496|       2.5|          1|\n",
      "| 124851|       2.5|          1|\n",
      "|  67618|       2.5|          1|\n",
      "|  72692|       2.5|          1|\n",
      "|   8911|       2.5|          1|\n",
      "|   4788|       2.5|          1|\n",
      "| 142444|       2.5|          1|\n",
      "| 171749|       2.5|          1|\n",
      "| 173963|       2.5|          1|\n",
      "|  48698|       2.5|          1|\n",
      "|  26350|       2.5|          1|\n",
      "| 175387|       2.5|          1|\n",
      "| 122092|       2.5|          1|\n",
      "| 100556|       2.5|          1|\n",
      "|  71268|       2.5|          1|\n",
      "| 147300|       2.5|          1|\n",
      "|   6086|       2.5|          1|\n",
      "+-------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m6.run_model(train=train,val=val).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d7ff53ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:56:24.612267Z",
     "start_time": "2022-04-27T19:56:16.232396Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2935:====================================>               (139 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|rating|userId|movieId|      date|               title|row_num|length|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|   2.5|    53|   1256|03-22-2009|    Duck Soup (1933)|     17|    20|\n",
      "|   2.5|    53|    880|03-22-2009|Island of Dr. Mor...|     19|    20|\n",
      "|   2.5|    53|   1982|03-22-2009|    Halloween (1978)|     13|    20|\n",
      "|   2.5|    53|   2686|03-22-2009|Red Violin, The (...|     20|    20|\n",
      "|   2.5|    53|   1298|03-22-2009|Pink Floyd: The W...|     14|    20|\n",
      "|   2.5|    53|    249|03-22-2009|Immortal Beloved ...|     18|    20|\n",
      "|   2.5|    53|   1049|03-22-2009|Ghost and the Dar...|     15|    20|\n",
      "|   2.5|    53|   1441|03-22-2009| Benny & Joon (1993)|     16|    20|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "val.filter('userId = 53').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f5ae92bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:43:09.371021Z",
     "start_time": "2022-04-27T19:43:09.354058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'methods': {'als': <bound method Model.alternatingLeastSquares of <__main__.Model object at 0x7fb1e0d4da60>>,\n",
       "  'baseline': <bound method Model.baseline of <__main__.Model object at 0x7fb1e0d4da60>>},\n",
       " 'num_recs': 100,\n",
       " 'model_size': None,\n",
       " 'model_type': 'baseline',\n",
       " 'rank': None,\n",
       " 'maxIter': None,\n",
       " 'regParam': None,\n",
       " 'model_save': False,\n",
       " 'min_ratings': 0,\n",
       " 'evaluation_data_name': 'Val',\n",
       " 'time_when_ran': '04/27/2022, 15:38:00',\n",
       " 'time_to_fit': 1.5275969505310059,\n",
       " 'time_to_predict': 0,\n",
       " 'metrics': {'MAP - All': 4.0871474687848736e-05,\n",
       "  'MAP - Subset': 9.827364990431464e-05,\n",
       "  'meanAveragePrecisionAt100All': 0.0009508196721311474,\n",
       "  'ndcgAt100-All': 0.0012090734084538412,\n",
       "  'meanAveragePrecisionAt100subset': 0.0010197368421052632,\n",
       "  'ndcgAt100-subset': 0.0014060541410038836,\n",
       "  'MAP - Intersection': 0.8884523809523809,\n",
       "  'Precision - Intersection': 0.8928571428571428,\n",
       "  'Recall of liked movies': 0.025389336369966896},\n",
       " 'review_threshold': 0}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a3e69f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:31:12.120419Z",
     "start_time": "2022-04-27T19:31:12.106237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'methods': {'als': <bound method Model.alternatingLeastSquares of <__main__.Model object at 0x7fb1e0dd7f10>>,\n",
       "  'baseline': <bound method Model.baseline of <__main__.Model object at 0x7fb1e0dd7f10>>},\n",
       " 'num_recs': 100,\n",
       " 'model_size': None,\n",
       " 'model_type': 'baseline',\n",
       " 'rank': None,\n",
       " 'maxIter': None,\n",
       " 'regParam': None,\n",
       " 'model_save': False,\n",
       " 'min_ratings': 5,\n",
       " 'evaluation_data_name': 'Val',\n",
       " 'time_when_ran': '04/27/2022, 15:25:50',\n",
       " 'time_to_fit': 0.554253101348877,\n",
       " 'time_to_predict': 0,\n",
       " 'metrics': {'MAP - All': 0.002697937367692215,\n",
       "  'MAP - Subset': 0.002948111848836309,\n",
       "  'meanAveragePrecisionAt100All': 0.02875409836065574,\n",
       "  'ndcgAt100-All': 0.0401296377781045,\n",
       "  'meanAveragePrecisionAt100subset': 0.02769736842105262,\n",
       "  'ndcgAt100-subset': 0.041868626427465305,\n",
       "  'MAP - Intersection': 0.5351753445126604,\n",
       "  'Precision - Intersection': 0.6166173512275193,\n",
       "  'Recall of liked movies': 0.09726976885636433},\n",
       " 'review_threshold': 0}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "002b3436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T18:26:14.634171Z",
     "start_time": "2022-04-27T18:24:15.489084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "preds = m2_2.baseline(train,val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81b84853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T18:26:15.688285Z",
     "start_time": "2022-04-27T18:26:15.678814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'methods': {'als': <bound method Model.alternatingLeastSquares of <__main__.Model object at 0x7fb1e91a2b20>>,\n",
       "  'baseline': <bound method Model.baseline of <__main__.Model object at 0x7fb1e91a2b20>>},\n",
       " 'num_recs': 100,\n",
       " 'model_size': None,\n",
       " 'model_type': None,\n",
       " 'rank': 5,\n",
       " 'maxIter': 5,\n",
       " 'regParam': 0.01,\n",
       " 'model_save': False,\n",
       " 'min_ratings': 5,\n",
       " 'evaluation_data_name': None,\n",
       " 'time_when_ran': None,\n",
       " 'time_to_fit': 0.15020489692687988,\n",
       " 'time_to_predict': 0,\n",
       " 'metrics': {'MAP - All': 0.0026979373676922144,\n",
       "  'MAP - Subset': 0.002948111848836309,\n",
       "  'meanAveragePrecisionAt100All': 0.028754098360655744,\n",
       "  'ndcgAt100-All': 0.04012963777810449,\n",
       "  'meanAveragePrecisionAt100subset': 0.027697368421052627,\n",
       "  'ndcgAt100-subset': 0.0418686264274653,\n",
       "  'MAP - Intersection': 0.5351753445126604,\n",
       "  'Precision - Intersection': 0.6166173512275186},\n",
       " 'review_threshold': 0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(m2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ae4502a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T17:57:42.813109Z",
     "start_time": "2022-04-27T17:57:42.799554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'methods': {'als': <bound method Model.alternatingLeastSquares of <__main__.Model object at 0x7fb1b8548880>>,\n",
       "  'baseline': <bound method Model.baseline of <__main__.Model object at 0x7fb1b8548880>>},\n",
       " 'num_recs': 100,\n",
       " 'model_size': None,\n",
       " 'model_type': None,\n",
       " 'rank': 5,\n",
       " 'maxIter': 5,\n",
       " 'regParam': 0.01,\n",
       " 'model_save': False,\n",
       " 'min_ratings': 20,\n",
       " 'evaluation_data_name': None,\n",
       " 'time_when_ran': None,\n",
       " 'time_to_fit': 0.44698190689086914,\n",
       " 'time_to_predict': 0,\n",
       " 'metrics': {'MAP - All': 0.023403581324834086,\n",
       "  'MAP - Subset': 0.024464687473561224,\n",
       "  'meanAveragePrecisionAt100All': 0.06285245901639344,\n",
       "  'ndcgAt100-All': 0.12423613626244004,\n",
       "  'meanAveragePrecisionAt100subset': 0.060625000000000005,\n",
       "  'ndcgAt100-subset': 0.1276739864620264,\n",
       "  'MAP - Intersection': 0.4514863282666781,\n",
       "  'Precision - Intersection': 0.5844855123769971},\n",
       " 'review_threshold': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "465d1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T17:59:00.249797Z",
     "start_time": "2022-04-27T17:58:48.885873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_pd = train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fafe50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T18:02:27.365537Z",
     "start_time": "2022-04-27T18:02:27.333006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>userId</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>row_num</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51573</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51666</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51698</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51709</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51925</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129229</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127172</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126430</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating  userId  date  title  row_num  length\n",
       "movieId                                              \n",
       "51573         1       1     1      1        1       1\n",
       "51666         1       1     1      1        1       1\n",
       "51698         1       1     1      1        1       1\n",
       "51709         1       1     1      1        1       1\n",
       "51925         1       1     1      1        1       1\n",
       "...         ...     ...   ...    ...      ...     ...\n",
       "129229        1       1     1      1        1       1\n",
       "2740          1       1     1      1        1       1\n",
       "2977          1       1     1      1        1       1\n",
       "127172        1       1     1      1        1       1\n",
       "126430        1       1     1      1        1       1\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.groupby(\"movieId\").count().sort_values(by = 'userId' , ascending='False').iloc[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "932c37d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T17:45:54.382274Z",
     "start_time": "2022-04-27T17:45:54.360712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'methods': {'als': <bound method Model.alternatingLeastSquares of <__main__.Model object at 0x7fb1b8547280>>,\n",
       "  'baseline': <bound method Model.baseline of <__main__.Model object at 0x7fb1b8547280>>},\n",
       " 'num_recs': 100,\n",
       " 'model_size': None,\n",
       " 'model_type': None,\n",
       " 'rank': 5,\n",
       " 'maxIter': 5,\n",
       " 'regParam': 0.01,\n",
       " 'model_save': False,\n",
       " 'min_ratings': 0,\n",
       " 'evaluation_data_name': None,\n",
       " 'time_when_ran': None,\n",
       " 'time_to_fit': 0.2451331615447998,\n",
       " 'time_to_predict': 0,\n",
       " 'metrics': {'MAP - All': 4.087147468784872e-05,\n",
       "  'MAP - Subset': 9.827364990431462e-05,\n",
       "  'meanAveragePrecisionAt100All': 0.0009508196721311476,\n",
       "  'ndcgAt100-All': 0.0012090734084538406,\n",
       "  'meanAveragePrecisionAt100subset': 0.0010197368421052627,\n",
       "  'ndcgAt100-subset': 0.001406054141003884,\n",
       "  'MAP - Intersection': 0.8884523809523809,\n",
       "  'Precision - Intersection': 0.8928571428571428},\n",
       " 'review_threshold': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe611008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T17:51:46.872922Z",
     "start_time": "2022-04-27T17:51:46.866408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'methods': {'als': <bound method Model.alternatingLeastSquares of <__main__.Model object at 0x7fb1d0805400>>,\n",
       "  'baseline': <bound method Model.baseline of <__main__.Model object at 0x7fb1d0805400>>},\n",
       " 'num_recs': 100,\n",
       " 'model_size': None,\n",
       " 'model_type': None,\n",
       " 'rank': 5,\n",
       " 'maxIter': 5,\n",
       " 'regParam': 0.01,\n",
       " 'model_save': False,\n",
       " 'min_ratings': 5,\n",
       " 'evaluation_data_name': None,\n",
       " 'time_when_ran': None,\n",
       " 'time_to_fit': 0.39426708221435547,\n",
       " 'time_to_predict': 0,\n",
       " 'metrics': {'MAP - All': 0.002697937367692216,\n",
       "  'MAP - Subset': 0.0029481118488363088,\n",
       "  'meanAveragePrecisionAt100All': 0.028754098360655747,\n",
       "  'ndcgAt100-All': 0.040129637778104486,\n",
       "  'meanAveragePrecisionAt100subset': 0.027697368421052634,\n",
       "  'ndcgAt100-subset': 0.04186862642746532,\n",
       "  'MAP - Intersection': 0.5351753445126604,\n",
       "  'Precision - Intersection': 0.616617351227519},\n",
       " 'review_threshold': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "466337e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T21:14:12.916868Z",
     "start_time": "2022-04-26T21:14:12.907025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'methods': {'als': <bound method Model.alternatingLeastSquares of <__main__.Model object at 0x7f9df21304c0>>,\n",
       "  'baseline': <bound method Model.baseline of <__main__.Model object at 0x7f9df21304c0>>},\n",
       " 'num_recs': 100,\n",
       " 'model_size': None,\n",
       " 'model_type': None,\n",
       " 'rank': 5,\n",
       " 'maxIter': 5,\n",
       " 'regParam': 0.01,\n",
       " 'model_save': False,\n",
       " 'min_ratings': 5,\n",
       " 'evaluation_data_name': None,\n",
       " 'time_when_ran': None,\n",
       " 'time_to_fit': 0.429764986038208,\n",
       " 'time_to_predict': 0,\n",
       " 'metrics': {'MAP - All': 0.002697937367692216,\n",
       "  'MAP - Subset': 0.002948111848836309,\n",
       "  'meanAveragePrecisionAt100All': 0.028754098360655744,\n",
       "  'ndcgAt100-All': 0.04012963777810448,\n",
       "  'meanAveragePrecisionAt100subset': 0.027697368421052634,\n",
       "  'ndcgAt100-subset': 0.041868626427465305,\n",
       "  'MAP - Intersection': 0.5351753445126599,\n",
       "  'Precision - Intersection': 0.616617351227519},\n",
       " 'review_threshold': 0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c2f49ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T19:24:46.122324Z",
     "start_time": "2022-04-26T19:24:42.298258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|             movieId|\n",
      "+------+--------------------+\n",
      "|   471|[2879, 118900, 76...|\n",
      "|   463|[322, 6639, 3308,...|\n",
      "|   496|[3404, 2879, 2372...|\n",
      "|   148|[6380, 3676, 4535...|\n",
      "|   540|[1306, 1128, 8633...|\n",
      "|   392|[3308, 2879, 1218...|\n",
      "|   243|[174, 3308, 1689,...|\n",
      "|    31|[1218, 3983, 3385...|\n",
      "|   516|[130634, 37731, 7...|\n",
      "|   580|[3308, 322, 6639,...|\n",
      "|   251|[8633, 1295, 3030...|\n",
      "|   451|[118900, 7649, 11...|\n",
      "|    85|[2290, 27822, 535...|\n",
      "|   137|[118900, 5292, 69...|\n",
      "|    65|[37731, 6380, 899...|\n",
      "|   458|[174, 1866, 3308,...|\n",
      "|   481|[118900, 37731, 5...|\n",
      "|    53|[2879, 1218, 8477...|\n",
      "|   255|[95720, 80693, 48...|\n",
      "|   588|[70994, 89904, 32...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4db38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T19:23:55.233333Z",
     "start_time": "2022-04-26T19:23:55.233320Z"
    }
   },
   "outputs": [],
   "source": [
    "combined.record_metrics(preds, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8bd0cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T18:57:22.809340Z",
     "start_time": "2022-04-27T18:57:15.058090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [([1,2,3],[1,2,3,4])]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "metrics = RankingMetrics(rdd)\n",
    "\n",
    "metrics.recallAt(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6522cd8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:48:26.756592Z",
     "start_time": "2022-04-27T19:48:09.016812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|rating|userId|movieId|      date|               title|row_num|length|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|   1.5|    91|   3508|04-05-2005|Outlaw Josey Wale...|    108|   575|\n",
      "|   1.5|    91|   1407|04-05-2005|       Scream (1996)|    221|   575|\n",
      "|   1.0|    91|   3360|04-05-2005|Hoosiers (a.k.a. ...|     78|   575|\n",
      "|   0.0|    91|   2060|04-05-2005|  BASEketball (1998)|    172|   575|\n",
      "|   0.0|    91|    736|04-05-2005|      Twister (1996)|    274|   575|\n",
      "|   0.5|    91|   1704|04-05-2005|Good Will Hunting...|    145|   575|\n",
      "|   1.5|    91|     21|04-05-2005|   Get Shorty (1995)|    320|   575|\n",
      "|   0.0|    91|   2001|04-05-2005|Lethal Weapon 2 (...|    179|   575|\n",
      "|   1.0|    91|   1580|04-05-2005|Men in Black (a.k...|    227|   575|\n",
      "|   2.5|    91|    541|04-05-2005| Blade Runner (1982)|    257|   575|\n",
      "|   1.5|    91|   2117|04-05-2005|1984 (Nineteen Ei...|    225|   575|\n",
      "|   1.0|    91|   6620|04-05-2005|American Splendor...|    301|   575|\n",
      "|   1.5|    91|    589|04-05-2005|Terminator 2: Jud...|     22|   575|\n",
      "|   1.5|    91|    533|04-05-2005|  Shadow, The (1994)|      9|   575|\n",
      "|   1.0|    91|   2140|04-05-2005|Dark Crystal, The...|    331|   575|\n",
      "|   1.5|    91|   6755|04-05-2005| Bubba Ho-tep (2002)|     32|   575|\n",
      "|  -0.5|    91|   2012|04-05-2005|Back to the Futur...|     70|   575|\n",
      "|   0.5|    91|    223|04-05-2005|       Clerks (1994)|    313|   575|\n",
      "|   1.0|    91|   7561|04-05-2005|   Paperhouse (1988)|    263|   575|\n",
      "|   1.0|    91|   2000|04-05-2005|Lethal Weapon (1987)|    265|   575|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.filter(\"userId = 91\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "955e8ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:51:45.224266Z",
     "start_time": "2022-04-27T19:51:34.606804Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|rating|userId|movieId|      date|               title|row_num|length|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|   2.5|    53|   3100|03-22-2009|River Runs Throug...|      6|    20|\n",
      "|   2.5|    53|    203|03-22-2009|To Wong Foo, Than...|      5|    20|\n",
      "|   2.5|    53|    381|03-22-2009|When a Man Loves ...|      1|    20|\n",
      "|   2.5|    53|    481|03-22-2009|   Kalifornia (1993)|      7|    20|\n",
      "|   2.5|    53|    748|03-22-2009| Arrival, The (1996)|      4|    20|\n",
      "|   2.5|    53|    413|03-22-2009|     Airheads (1994)|      8|    20|\n",
      "|   2.5|    53|   1125|03-22-2009|Return of the Pin...|     12|    20|\n",
      "|   2.5|    53|    916|03-22-2009|Roman Holiday (1953)|     10|    20|\n",
      "|   2.5|    53|   2616|03-22-2009|   Dick Tracy (1990)|      9|    20|\n",
      "|   2.5|    53|    922|03-22-2009|Sunset Blvd. (a.k...|      3|    20|\n",
      "|   2.5|    53|   1100|03-22-2009|Days of Thunder (...|      2|    20|\n",
      "|   2.5|    53|   4019|03-22-2009|Finding Forrester...|     11|    20|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.filter(\"userId = 53\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "46d86b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T19:51:28.171109Z",
     "start_time": "2022-04-27T19:51:15.750569Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|rating|userId|movieId|      date|               title|row_num|length|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "|   2.5|    53|    381|03-22-2009|When a Man Loves ...|     11|    20|\n",
      "|   1.5|   406|    261|02-05-2011| Little Women (1994)|     11|    20|\n",
      "|  -1.5|   442|   2020|03-12-2012|Dangerous Liaison...|      1|    20|\n",
      "|  -0.5|   442|    524|03-12-2012|         Rudy (1993)|      9|    20|\n",
      "|   0.0|   431|    485|02-24-2010|Last Action Hero ...|      3|    20|\n",
      "|   2.0|   257|   1288|03-06-2006|This Is Spinal Ta...|      5|    20|\n",
      "|  -1.5|   257|      7|03-06-2006|      Sabrina (1995)|     10|    20|\n",
      "|   1.5|   189|    318|08-13-2015|Shawshank Redempt...|      9|    20|\n",
      "|   1.5|   207|   2991|11-18-2009|Live and Let Die ...|      2|    20|\n",
      "|   2.5|   278|     50|10-30-2007|Usual Suspects, T...|      1|    20|\n",
      "|   1.0|   194|    168|03-08-2005| First Knight (1995)|      8|    20|\n",
      "|   1.5|   194|   1288|03-08-2005|This Is Spinal Ta...|      5|    20|\n",
      "|   2.5|   569|    377|11-28-1996|        Speed (1994)|      6|    20|\n",
      "|   0.5|   569|    231|11-28-1996|Dumb & Dumber (Du...|      1|    20|\n",
      "|   1.0|   320|   3578|09-11-2010|    Gladiator (2000)|      1|    20|\n",
      "|   1.5|   320|   3534|09-11-2010|      28 Days (2000)|     12|    20|\n",
      "|   1.5|   576|   2723|01-14-2013|  Mystery Men (1999)|      4|    20|\n",
      "|   1.5|   576|   1223|01-14-2013|Grand Day Out wit...|      1|    20|\n",
      "|   1.5|   595|   2078|10-01-1999|Jungle Book, The ...|      6|    20|\n",
      "|   2.0|   147|   2699|02-17-2008|Arachnophobia (1990)|      9|    20|\n",
      "+------+------+-------+----------+--------------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.filter('length = 20').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c957a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
